{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMSR9WLCzBz2HJXagRj6MP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VitalyGladyshev/NeuroN/blob/master/HW_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPSMhCGJuP2d",
        "colab_type": "text"
      },
      "source": [
        "# ДЗ_05\n",
        "\n",
        "## Задание 1\n",
        "\n",
        "Попробуйте обучить нейронную сеть LSTM на любом другом датасете. Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfhdkC3XuMIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SimpleRNN, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from typing import List, Tuple"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQx1ew4l7zmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_confidence_interval(scores: list, conf_interval: float = 0.95) -> Tuple[float]:\n",
        "    \"\"\"\n",
        "    Вычисление доверительного интервала.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    scores: List[float / int]\n",
        "        Список с оценками изучаемой величины.\n",
        "\n",
        "    conf_interval: float, optional, default = 0.95\n",
        "        Уровень доверия для построения интервала.\n",
        "        Опциональный параметр, по умолчанию, равен 0.95.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    conf_interval: Tuple[float]\n",
        "        Кортеж с границами доверительного интервала.\n",
        "\n",
        "    \"\"\"\n",
        "    left_bound = np.percentile(\n",
        "        scores, ((1 - conf_interval) / 2) * 100\n",
        "    )\n",
        "    right_bound = np.percentile(\n",
        "        scores, (conf_interval + ((1 - conf_interval) / 2)) * 100\n",
        "    )\n",
        "\n",
        "    return left_bound, right_bound"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4mU1vzdi407",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "2d21df9c-cbbe-4d1f-b5c6-2a7680d07ee2"
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d8ca1b2b-3aeb-44e4-ab99-15d36face55b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d8ca1b2b-3aeb-44e4-ab99-15d36face55b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving shakespear.txt to shakespear.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'shakespear.txt': b\"That, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their defeated queen,\\nHer flesh broke me and puttance of expedition house,\\nAnd in that same that ever I lament this stomach,\\nAnd he, nor Butly and my fury, knowing everything\\nGrew daily ever, his great strength and thought\\nThe bright buds of mine own.\\n\\nBIONDELLO:\\nMarry, that it may not pray their patience.'\\n\\nKING LEAR:\\nThe instant common maid, as we may less be\\na brave gentleman and joiner: he that finds us with wax\\nAnd owe so full of presence and our fooder at our\\nstaves. It is remorsed the bridal's man his grace\\nfor every business in my tongue, but I was thinking\\nthat he contends, he hath respected thee.\\n\\nBIRON:\\nShe left thee on, I'll die to blessed and most reasonable\\nNature in this honour, and her bosom is safe, some\\nothers from his speedy-birth, a bill and as\\nForestem with Richard in your heart\\nBe question'd on, nor that I was enough:\\nWhich of a partier forth the obsers d'punish'd the hate\\nTo my restraints would not then be got as I partly.\\n\\nAUTOLYCUS:\\nHath sat her love within this man, that was foul prayers\\nWhich are much thus from them with thee; I am not ever thought\\nTo make that with a wise exclaim, as I am sure;\\nTo say well like a dotage on the fixed cease,\\nAnd let mine eyes may straight sole sword conveyard,\\nThat dust-confounded by a land to their command\\nThen puissant with a grief's: it should be so and dead,\\nTill he shall fail his sister; and in true and good,\\nTo see me for the other, hath not heard a midwife\\nLoud from my service and thy sweetly daughter got\\nThe single strange words pent is all his steed:\\nStay from us in, as he hath we brought me into the Milthiness.\\n\\nGUIDERIUS:\\nWhy, my lord,\\nShall not part well: but it shall have my hands;\\nLet us be taken that, thou weights return,\\nTo mine ring ere I should be dangerous with a good way\\nTo swear it: for the bears now he was kin to him,\\nBut then his own island's sister's all speech would deny\\nAnd force I grant it.\\n\\nABHORSON:\\nThou art a very earth. Villain, reserves my keeping.\\n\\nTALBOT:\\n\\nKING HENRY VIII:\\nNow\\nTo have lead for me;--\\n\\nKENT:\\nBut yet ease yourself truly in the numbers,\\nIf he be no talk at my death in the name of hot\\nyears. Then that had not so far good general\\nTo make with buried vacus arrest them.\\n\\nTIMON:\\nStay, and the sere hath dangerous,\\nToo grace: a sail, the breath of knees, broke deeds\\nWould do thy husband and alack to speak,\\nAnd pluck their men at thy abroad doth go.\\n\\nCASSIUS:\\nAnd in desire,\\nAnd call'd me ballant Cassius.\\n\\nBARDOLPH:\\nTost in it, what then take your madder?\\n\\nDUKE OF YORK:\\nShe would be ready, this advice, say you a chaste.\\n\\nSecond Neris:\\nNow, blessed France, and with thy speech can know?\\n\\nANTIPHOLUS OF EPHESUS:\\nNot I: outlive, lady! Philosophy, gentlemen;\\nUpon our wrongs despised, I will not sit to thee\\nFrom Duke of Gloucester's name:\\nLook thee, and thee but wear my noblemans.\\n'Bandible pardon of a thousand embure hath contemn'd\\nIn uneven day and bend unkind of post.\\n\\nAJAX:\\nO Mattle! these strict cuttances cut\\nit down; for I had slept my fellow, to said 'I\\nHave found your lordship hope.\\n\\nMARK ANTONY:\\nHow! I have done them to you; I would\\nto sose a minute that way is, the ingrateful brook.\\n\\nMARCELLUS:\\nTell us to break it, for I hear not every shore.\\nDeny thee, sir; I must bear him to run; I'll put him\\nfrom dangerous and happy judgments for't, they wretch\\nTheir qualities with death together, and take not his sword.\\n\\nDEMETRIUS:\\nCome, go thy wreck; for now, Aumerle, do I know but\\nthe valiant world before I fear; and make no cause\\nIn bristing sorry and behold your friend;\\nBut, with my deed, I do not threw you boy.\\n\\nKING IDAR WILLOUGHBY:\\nO, then wouldst thou speak with me.\\n\\nGOWER:\\nPoor queen, leave them along at.\\n\\nCAPULET:\\nNo, good sir,\\nAnd make a pen and meeding down, trouble me\\nHere provide it: your breast of any other use\\nWearing behind the old looks of a man begin:\\nBut, for your city, as the cases of the art\\nI have held to go to you.\\n\\nVALENTINE:\\nSir, in state, then tell me which are never fall'n.\\n\\nCAESAR:\\n\\nSecond Murderer:\\nMy lord, if virtuous Scotland! is good chamber to\\nthe shepherd.\\n\\nSecond Murderer:\\nHe is like to let her go.\\n\\nNORFOLK:\\nWe'll come by thy sweet brother.\\n\\nHELENA:\\nDid you be more: he, that is entreatable,\\nFrom Rutland's island,' to him, it is not of beauty:\\nIf they your heart's a fixed, sea-good wife,\\nSince one had robb'd himself here in this ready bull\\nFrom which be done to-night: there shall his wife\\nGuards conistering on their faction.\\n\\nBASSANIO:\\nHe did not come to hope a friar, and is the farewell: then\\nI knew we are bloody:\\nThereof must not undo a wrong of thee, they pluck\\nTheir sighs and down an empty love.\\n\\nSecond Gentleman:\\nFor France and Thersitania would kill our solemnity\\nThat know not, I beseech you, but, as I will fear to\\nplease two will I cannot hear, sir.\\n\\nCOUNTESS:\\nWhat, my master may be potent; and in this ancient face I\\nowed! here came not what I should love myself so and\\nset, to a brook-gast that thyself will eat an excellent time.\\n\\nMALVOLIO:\\nSay, good Sir John, when I was fair again,--\\n\\nEMILIA:\\nBut here is dead.\\n\\nMALVOLIO:\\nNow she was still e'er 'good, if no bloody difference:\\nAnd be it like to have thee fought: and line my master,\\nThou bear'st an honour there by ten, you do go, sweet God,\\nThis we do not be proud to us; wemper I do not.\\n\\nGLENDOWER:\\nI'll see thine eyes: my best, though not construct\\nThe saying with a friend, I will have my\\nAbout my heart with thee, Lord Talbot, and so reason\\nI would not bear their swiftness for the world.\\n\\nKING JOHN:\\nTrue mother, and all and a woman's house,\\nIf my true amen is full of gold, I shall be peerled\\nfrom my husband.\\n\\nLUCIUS:\\n'Tis the loyal soul but a free demural youth,\\nIt is the chain that slew that vision.\\n\\nCLEOPATRA:\\nO the fight of hope, madam!\\n\\nFALSTAFF:\\nThen she's but fall of no dullar's fault,\\nLet them ask from the guilty place: Master Ford.\\nO, confess you I would not chid myself out of fame in the deliberate,\\nNo more men, it is all together.\\n\\nAJAX:\\nI know not what.\\n\\nHost:\\nPer me, 'tis babble. Here be not as he was bed:\\nUnrunish and had stormed upon me here by that:\\nSo I should so slire their own, or two youth's seems,\\nFor England would rest in Paris!\\n\\nCASSIO:\\n\\nCELIA:\\n'Tis an effect since you must find you, there is some\\nmaster alike.\\n\\nGLOUCESTER:\\nOpen them for thy heart, back again. Are you\\nhope to use? Prithee, let him go, Brutus. So much it is;\\ncount was of day, and I must raise a layful peace.\\n\\nCORIOLANUS:\\nLet us would see our ripe.\\n\\nMARK ANTONY:\\nI might have stay'd, and I met your triumphant cousin;\\nAnd in the world,\\nWhose parties, that it were two bodies, to discharge\\nA glumber to a perfect tower.\\n\\nOCTAVIUS CAESAR:\\nA part to the tame.\\n\\nBALTHASAR:\\nAs I have all the very line, that gave me your highness,\\nWhere you will hear the single spirit of my business,\\nPlant down flives on your son, and even\\nAnd open with their own conusteries; and thinking\\nyour grave ship should ne'er break Humphrey's eyes,\\nI am poor dear party to make his chamber\\nAnd hospish shameless frozen pride. Here name,\\nAnd light in plot legely in whom I said,\\nGlimmed by an argument of it sweet fears your other mouth,\\nSuch a great estimation would be run as this,\\n'Tis fit for them, 'tis talk before yourselves.\\n\\nKING HENRY V:\\nAnd I must not see her, but go'st with unhappy woe.\\n\\nKING JOHN:\\nBy Paris, it was fair.\\n\\nDUCHESS:\\nNay, I'll ever throw his honour.\\n\\nOTHELLO:\\nGood morrow, amen.\\n\\nCRESSIDA:\\nTo stop the great streets follow your grace which I shall not,\\nAnd scruple him thou art a sit and still so straight.\\n\\nKING lEAR:\\nOne loyal of my love, the wedding-body touchest thee: I pray,\\nHenceforwards, and submiss the truth! though my throne\\nLives as mock'd my pardon with some untold\\nAttore sack lop and shrum' them up:\\nBut be preserved with spirits, so brimfibed again!\\nMy voices were so early, I was enough.\\n\\nMACBETH:\\nThen let him withdraw them debour to branch ere any any\\nday, but to prevail'd be penny of a merry tongue\\nWhich the exploits of fools look with their veins.\\n\\nKING LEAR:\\nThat bell beseems my wife at all, and I canst thou\\nsee. How! lend me your part.\\n\\nBANQUO:\\nDoth a wonderful maletration with mine torch,\\nThe bloody kingdom rogue as his condemn'd\\nMay dares respect it not? let you have bound quite straight;\\nWhich is a child should buy, whose noble occasion\\nHath so impudent, or without his life,\\nAre then she will be married with my words,\\nAnd he that keeps some suitors in the cases,\\nAnd in their heart shall break thy bloody load with over\\nAs every man is much between a sign\\nShould show angerous brother o' the Tower; whilst we present\\nBy our continual gates, the justice' enemy.\\n\\nSICINIUS:\\nO, be no more;\\nThen French all letters indeed be bound from our fair:\\nIf not, she says the fair and left highest branch, a gentleman,\\nSo praised are unvile without their eye by other\\nBut on that ground I know the likey.\\n\\nQUEEN ELIZABETH:\\nWhy, each man shall, as I am choked to see it,\\nWhiles nature between fanny they are a little of the fault\\nAnd lightly read them thus; and her well-met,\\nWhiles we have been upon his son blind damned on thy bonds.\\n\\nSecond Citizen:\\nNo.\\n\\nFirst Lord:\\nRemoved, I cannot speak no host;\\nBut to be loath amazed,--Go parted hands, I know not,\\nMarshally, Are vile haste in the noble hand,\\nSo used as one and naked husband, the stars, not one Cassius,\\nAnd in thy offer'd bone in silent trifles,\\nWho should unquiet, to me shall do me commanded\\nFor his rise in the present peace. But know'st thou\\nBring me my love? were all these sleep command.\\nWhat had thy hand your wit and conceit's bed?\\nDoth it not find, so they are too dangerous,\\nWhen diomed had he choked low nor with your daughter than Hecuba?\\n\\nSecond Lord:\\nIs all his oaths? he that seems children that take\\ntheir defendant; she his name,\\nThou shouldst eat theirs to speak to day, indeed:\\nBy my troth, lord, I know, have drunk my welcome,\\nUnless that seemeth of the table, cousin Hubert,\\nWith present obedience claughter'd with her,\\nOf when the time upon their safety,\\nWhom my reporter's faces, he hath told me,\\nWhen he had come not distracting all Prospero.\\n\\nAENEAS:\\nO thou man, a traitor for myself and Hector's hatch!\\nFit happy life in civil stone, and were a wonder'd point,\\nTo make news et this dolour here, my liege, and wise\\nHigh run with princes: some content, run cold to be\\nThe measure of the manage of a glorious charge.\\nSir, I'll asleep.\\n\\nMERCUTIO:\\nI would not regreet but your tribunes suffoce you?\\n\\nThird Gentleman:\\nYour cousin Sir John Northumberland that loves me well,\\nAnd it comes royal and most inservant.\\n\\nEMILIA:\\nPardon me, madam;\\nIn best take it not to the sun as soonman's stomach\\nThat why he says this as he famished the nurse\\nTo beat purpose to make the passage of their sweetest knights:\\nPluck not a huntred feebleness, subdued his title,\\nI'll walk my hands, my heart thou swear'st from thee.\\n\\nDUKE VINCENTIO:\\nRust not all trazards, that thou hast he not glad me\\nAnd so shine and two of our book, sir: I saw 'em\\nIn what is pipe of smock, perform'd,\\nMust join'd to the unto the noving bear;\\nFrom bringing winged one is recreantly,\\nIn peace, to make a field\\nThat I have wounded what shall have with other head.\\nGive me thy recompense are out of death,\\nOr else resolved too ground.\\n\\nLieutenant:\\nHow fares thee to command? Do it so.\\n\\nPISTOL:\\nStrumpet from my name, unto't, Philomelius, lay the garland.\\nGood Caesar, sweet good stocks, and marry with her words in thy\\nproud people, if he want all unhappy sights. Revenge\\nTo be so means to you and honour for my flesh.\\n\\nAJAX:\\nFair woes, I'll never play the further kind;\\nBut who's the prey by thy shoulder, and only\\nmore such gold of such a point of soldier and the got?\\n\\nKING PHILIP:\\nPeace, you had not the subjects laughman's\\ncrown! O, sir, I am the house. Be here in the king.\\n\\nSecond Lord:\\nThat thinkest a showledge, you will make him a\\nmonth better than you are causes again he's a dog\\nThat I may say: you are every beached and courtier.\\n\\nBRUTUS:\\nNo, cail so privy of her face, and that he may\\nCharge any thing i' the leader here about his offers;\\nTo stoop upon good hands waits and dispite out of our mother\\nAnd now, had offer of themselves; indifferents,\\nWhy dost thou sting the noble house in thy household shepherd,\\nBolder'd in coveration, whose offence were peer\\nBeseeming in that packet speakings more than are to-night;\\nOr gives the minds with lawyers, greatness, O,\\nMy son, I would have flatter'd and with thee\\nAnd mine enemy, but now the noblest hope will never read,\\nSend daggers of their government,\\nThat ever passion doth on portent should have feed\\nTo try the soldiers, lords, quick lets, we wish their palms:\\nAnd leave thee, and another than should have her cause\\nThree taking circumstance.\\n\\nIMOGEN:\\nBy Romeo, I will strike without state.\\n\\nIAGO:\\nSir, you are put a mirror.\\n\\nOCTAVIUS CAESAR:\\nCome, fellow, call my lips before the law of arms,\\nWhilst we may wherefore take thy judgment out of nature,\\nand do you, ere come in heaven, Lord Bohemia!\\n\\nANTONIO:\\nHere comes Capulet.\\n\\nCASSIUS:\\nI cannot please you to my lord; you lose earthly days,\\nbut in thine own rather, be not but a army of my reputation.\\n\\nSIR TOBY BELCH:\\nTrue, sirrah; let us blame it too: I can do as good a paish\\nthat adders as you are, comes hither.\\n\\nDUKE OF YORK:\\nMost prince, I do not dare my lord;\\nNor not by leave again, sir.\\n\\nLADY MACBETH:\\nIt is not to be so.\\n\\nROSALIND:\\nStay, gentle judgment, he was seen a state\\nIs worthy patience: execute them,\\nAnd I prithee, gentlemen: it came command by Mercutio,\\nBut on that king hath heard us fall, who knows to pieces\\nBy angry vassals.\\n\\nCRIMON:\\nO ho! he's a four and terrible breath, good night,\\nOn my man went a father as his language,\\nShe hath thy noble plucking and believed powers\\nUnder the mind thereof, unless thou means to-night,\\nAnd feed it by a man's reports, the debtor shall\\nBe stroked from the hunter with this frower,\\nWhen one is not yesterday as falsely disclose in feast\\nOf this no more than as thou shouldst not bear.\\nIf she were never made by thee,\\nThe ensue of London of lusty island even to the point,\\nThe rax of Rome?\\n\\nWARWICK:\\nYare greatness of good man; for for expension that\\nis right that he beguiled this toil,\\nContempt before your sacred soul, it is more devil to\\nbe possess'd, this like himself I full before\\nA moderate ship's fantasy to be unto your honour.\\n\\nTRANIO:\\nTherefore thou art wife no mean nor convenient kingdom\\nAnd did himself said now:\\nBut father Night hath been a joy with gallant by--\\nO, with my infant, by the seas of kind of Norfolk,\\nHath firm and wind that physic pantion us\\nRanged the realm of each dug of the earth!\\nBe therefore you repent me with the state subdue\\nAnd swears my sister up again! thy heart\\nThey will bear thanks; for now abated with thee,\\nThe malice and tongue Guilty of it owed by thee\\nThat teaching the heavier fates of death of grace\\nAs I chid the poor hair to kiss our field;\\nMeat thou before the seven blow with letters out,\\nAnd turn my virtues are ourselves; there are ye dead;\\nAnd melted legs contracted in such ones,\\nSome beer whose aim must be supposed both blind.\\n\\nISABELLA:\\nA warrior of the sway.\\n\\nSIR HUGH EVANS:\\nFor all things thou hast eating, as Helen is call'd Hermia and\\nlife for well-discretion o'er the way.\\n\\nSICINIUS:\\nNever to discredit you, my master.\\n\\nDON ADRIANO DE ARMADO:\\nNow, what's the matter?\\n\\nTHESEUS:\\nWe may washed her: there's\\nmost conduct but a spirit of horns.\\n\\nHostess:\\nCome, my lord;\\nHow does the proper country's wife? to the cure of saint,\\nby beggars again with this, I have a fellow too, that which\\noaths besides himself in kissed body too.\\n\\nTRANIO:\\nI hear excellent words for you that the 'gainst\\nThe whoreson fair Saturnine. Bark, I fear,\\nI am a tew, he throws me what to you;\\nThis is not Titus.\\n\\nQUEEN MARGARET:\\n\\nCORIOLANUS:\\nThat we'll not look you now,\\nLeave none rough things to do.\\n\\nSecond Pirator:\\nThe thoughts are true, sir, a most rich above, that's your picture;\\nthe mood or policy we please yourself,\\nwhere thou wert glad hale deadly to too business; and\\nas Shallow I am deliver'd in a king\\nThat I lamented this before they said the moon:\\nLet not my noble fortune to him with an arm?'\\n\\nMessenger:\\nAy, sir, the seas of this same true\\nThe base circumstance and a swain doth give his daughter\\nTo wedded me to be as he.\\n\\nEMILIA:\\nWhere is the loss, is his guilty eyes?\\n\\nDUKE VINCENTIO:\\nWhat is this?\\n\\nSecond Murderer:\\nThe realm of Norfolk calls up votenarding man,\\nAnd bear you throne at lodging nor your wit?\\n\\nClown:\\nAn one, sir, cannot have my title in a little clock:\\nYet, patch to hope, you will unlove,\\nTo our two schoolmaster and your brutish and\\nThat evers hast thou: hear me, poor a hope,\\nWe nought conceive, I cannot have thee custom go.\\n\\nTITUS ANDRONICUS:\\nShe doth to Coriolanus.\\nPoor French 'gainst my foot fear me no enough,\\nBut yet it peers from strings in dip in Antony:\\nThe secret son, I would they know my buds,\\nSame murmured over?\\n\\nVALENTINE:\\nThe duke she will put on with their fallings;\\nHe's that the canopy of discovery and faithful friend.\\nDidst thou be your dear lord, with grief? O, let us call\\nMy wife, the earth! good morrow: for I'll see\\nRemuneration.\\n\\nHAMLET:\\nHe's the man.\\n\\nCAESAR:\\nI am sure in every strange-day we meant a pound: I have\\nnot follow'd a tomb to this: why doth the face see?\\n\\nThird Servant:\\nWhen a hurt loved I cannot show him hence,\\nEven in any thing of crimes, though invitely it was a bare\\nspecial husband, who were\\nas fresh, such sweetly towers and night, which you\\ndo came in the right fable-service, will I lock.\\n\\nAUFIDIUS:\\nThose that he's not, but I love him well best:\\nOnce great, my lord! I ever lay downlight\\nThe inct of all my fellow, but the mother shall\\nConfused your peace to you and to the help.\\nWill, when I should have any words your child awhile\\nTo men of heaven and thee, you let my scorn is fled:\\nI must not be again, but and as good as be her chin!\\n\\nATHALLOT:\\nI will vessel it, 'tis unwell kept with you;\\nIf I do better see, I would not begin, and in enfraction 's captive\\nand to die, villain, an exhibition not so habit. But I will\\nwear you are jealously, and I am a mouth of Christenders.\\n\\nBAPTISTA:\\nIt was a great bond broke, and brought him to excuse.\\n\\nKING PHILIP:\\nAnd, if you know it, if he remember.\\n\\nCASSIUS:\\nMistress A fellow, here in natural commoner, will you title?\\n\\nANNE PAGE:\\nThou need'st no supplied with me to rest him by him,\\nFor fortune not from loves too son to see the time\\nThat all the queen's commission into thy better;\\nThere's good that fought upon my sweet queen as the king\\nMay let her, wished with my bounties, soothsay\\nAs I will Ferring you. Come, you are fear.\\n\\nHORATIO:\\nSir, his shippus yuts so fast as vile as est.\\n\\nClown:\\nThou fortune twice will put for blood my father\\nBut Talbot's death, an excellent and fepher's letter\\nLose panish and a second, and all to blood,\\nAnd blessing his pretty cause.\\n\\nBRUTUS:\\nWe do not take it, nurse.\\n\\nBEDFORD:\\nMarry, see my porrest, good Grecian; I'll not do\\nFrom my song till service you must outsage my daughter\\nOf Titanus' goodly interpretation together,\\nBehind them not.\\n\\nLEONATO:\\nWould you, for keeping they are even to control my comfort,\\nYours see the like an audient varlet.\\n\\nCRESSIDA:\\nFarewell, Brutus, disordered neither by the world;\\nWould she was senting of his bosom, purpose!\\n\\nBRUTUS:\\nEven to my sharp and goodly care by me that Milan would\\nnot know some lists on our masters, and eat\\nThat glory should I show them cost my slave,\\nTo raffed one another, they have weep'd by death.\\n\\nEXETER:\\nNow Macbeth shows a most high-dangerous monster;\\nThere I women should be might 'scaped them.\\n\\nCELIA:\\nIf you will mine, though I may presently be so.\\n\\nSeepant:\\nI know our worthy care,\\nUnless the boys themselves I hold upon it should\\nFrom Varrons, sights cry them for Sir John,\\nCould her son dare renish.\\nThou hast not made a several judgment and part,\\nAnd, call upon my mother's shadow's affairs;\\nThe which I'll write some lordship in my sword,\\nAnd drive be but my present cost,\\nOr, by his eyeless toes, he's more proclamated for the world,\\nAs I will take this rest between no man that himself\\nIs trup some field.\\n\\nTALBOT:\\nThou shalt put Heavens bring out his patroclus.\\n\\nSPEED:\\nI know not whither; for the most confidence\\nWas gaming merry weeping red, it and I am as dignity,\\nTo slip it. Good as familiars their remembrance\\nIs crept, whose ancestor hath seen to my sword,'\\nWould queen is now, Perdita's joy is a wall;\\nAnd, As a true, or break no friends a virtuous Fortune,\\nOur majesty's contemptuous armies.\\nWhat news?\\n\\nVALENTINE:\\nYes, my lord,\\n'Twill not go word by banquet and as spirit, and altwinged\\nLike a strange princely honour, he should lose it here,\\nAnd tell them to to the clamorous tedious profession,\\nMany will not speak what the dissembling horrible assumes\\nDo golding well.\\n\\nMENENIUS:\\nA very threatening mortal man,\\nAnd such a woman's loyal deserving name shoes it!\\n\\nSecond Servant:\\nSailors Hector; you shall not give life to make the delicate\\nmen here of my profit of motion.\\n\\nCLAUDIO:\\nMadam, this same can kind my boy, Master Slender,\\nThat any man who has all dinner if every man\\nhad not expeditions from the audit, and\\nI will relieve your torchers; with the earth\\nHave with a speed that would not gain with took upon\\nThat maiden bondmaid mould, or I will pay\\nMamber of itself and fit, I was conspirable,\\nTo ope a coward.\\n\\nOCTAVIUS CAESAR:\\nIf you cast those that rape and breathe me, sir.\\n\\nCLEOPATRA:\\nHast thou vouchsafed with Signior Benedick, who comes,\\nSweet coronation, or our ships are up the carriage of\\nmodesty shall not be proud to sleep, and say 'though\\non him sir; and then, who's so amazed of a bosom which\\nyou in our noble kingdom is such wholesome father\\nIn her conjecture of the gracious stars and self-behelding.\\n\\nKING LEAR:\\nO France, in evil so befall'n your worse,\\nThey chid her for our horsemen: honour to thy reason\\nWill soft us reverend in the right portial opinion;\\nOr earl of wicked chimney had her love,\\nThan tell me, then, I'll light my way; whose seat\\nIs but in me, that I of it was smoke could in my charge\\nSteeps in some time of him.\\n\\nMALVOLIO:\\nAs I love them, since your hands we break some goodness.\\n\\nHENRY BOLINGBROKE:\\nHow should I, goodman wife? is there no cause?'\\nOne estate more than her distrust was sovereign;\\nAnd so was base as, to be banish'd by the day.\\n\\nWILLIAMS:\\nMy lord, it were the motive Lord Antonio,\\nAnd for my mind did in a mountain grave\\nThat time may life your children to the ring of justice\\nAnd put them through their chairs and with thee, bully I\\nOf twained orb and practises of melody\\nWas gilting compound things at me.\\n\\nThird Gentleman:\\nShe hath done the door that desires for she\\nto-night. O, not to\\npray, and therefore I have been hang'd without many things.\\n\\nOPHELIA:\\nThere's mine.\\n\\nTIMON:\\nThat's my mischief, hath no pity yet bought to the good\\ndesire and ground to what thou wert in the sea,\\nso fair I do hear: I drunk for thee, and hear me not?\\n\\nANGELO:\\nYet you'll come in?\\n\\nFirst Senator:\\nNor after, I will, these are entertainment: the fellow of it seem'd\\nHe should be spoke with so he shall bring off.\\n\\nTROILUS:\\nSo peasantly, for the king's tent.\\n\\nDUKE VINCENTIO:\\nSir, thy wife shall hurry my better; he is\\nmay with a ruudership as a napkin to them.\\n\\nOBERON:\\nThou know not. Twenty loves, Sir John,\\nAnd Richard's head shall shake his head\\nAnd give a drop of jealousy, with vein importunite;\\nWhy, something but a king is angry. This blush'd monour\\nMakes us fair, et perdution; for I hear my daughter,\\nWhen we have drop on moon and fement to his extempority;\\nAnd shall he seem not in the sight of Norfolk live such ground\\nMy tongue shall be brief in the fire that tend\\nParticular a due? mercy was your remain!\\n\\nJOHN OF GAUNT:\\nThen, if they shall not rid of thee! if he be hail'd,\\nThat would dispose two miles before thou hast not to make me\\nAs he to worth compexaration with the city,\\nIf I do now, that I shall, and fly, out of Ferminence,\\nLies honourable, and the higher same chain;\\nAnd the king your request to charge me whither\\nDo you appear under goodly voice-his daughters,\\nThat Shylock was Anne Page, or to leave\\nTo bisk which drink with Rome and purposes,\\nBeing in the king's nhambly fancy nor dimm'd,\\nAs maidens the phoetician in the king,\\nAnd all in person, many ambassadors will in\\nFrance on, if there, as thou shalt break a rage\\nTill is the fire of world to sigh thee to their life,\\nThat I have granted her darest end to earnest:\\nNay, being aid there are as obsurding and open\\nAs duty in the kingdom will propose\\nTheir milkings to their strength without discourse!\\nFie! I should ever bear the sleepy hand,\\nA goodly master, wanton not for air:\\nBut I am like a walrer, Bianca;\\nThough him be great in thine own opinion\\nWith embolling wars, while we found birth,\\nThat blinds was kind and hand, according in the advantage,\\nHoist and the gun's that drame every knave's offer\\nHath pray'd his husband drift and thinking.\\n\\nSEBASTIAN:\\nWith days,\\nMost talk of care it should be patient.\\n\\nMENENIUS:\\nO, so. Say you well, indeed,\\nHe shall not mine own peace gave you a stand.\\n\\nVarro's Murderer:\\nWell, they say 'Didst thou so? I must eat thee,\\nCome, queen; nor even as if you came to run,\\nCould not report for his chaft, as I shall bear against\\nthe grounds. I do beseech you, if you would hang.\\n\\nProvost:\\nA proparable pitch could point prefer with you?\\n\\nDUKE OF YORK:\\nI saw those tale of heaven that fled for thee.\\n\\nSurveyor:\\nBefore the deed, together, your face bear my throat\\nBut soldiers long is outscorn himself\\nTo fashions of her icfortune up, the worst,\\nWho did consume me with the adversary-day,\\nWho cannot change, and be the writing, but not moulded\\nSome boy and oaks of wooer and bag that you would\\nher. But what's he like a misery? I swear behind\\nyou, because most ready I am so fraught up.\\n\\nBEATRICE:\\nNow, by my troth, 'tis guilty on the ground.\\n\\nDOSTARD CAIUS:\\nO man!\\n\\nANNE PAGE:\\nShall I cure me to give on our friends without great arm?\\n\\nNORFOLK:\\nAnd tell me, provide her, good Talbot.\\n I speak ere you will bring me thou go with me;\\nGo there with this.\\n\\nBoatswain:\\nA thousand penalty, madam, a extreme hip\\nWith your awoment of a coloured ingenious England,\\nRivers thus knowing this but horses, ceases riding,\\nTo give my sad lady, and we show the fearful scourge\\nOf do as singular in gageous poor suffer;\\nYour gentle Lord Hastings your strength\\nShall seethe it.\\n\\nTALBOT:\\nAlas, with him.\\n\\nMALVOLIO:\\nBack not of all the practise of these trees\\nTo three horrible city hungs in shrift,\\nThat Romeo wear you in the court shall could restore\\nTo come anon: therefore on this, we stand from any man's\\nA future's dependance: of your own side\\nIn Ireland, I will prove between thee;\\nHath stolen for it on the poor innocent Roman\\nThat turns the sisters to the soul of Cretes under\\nAnd say it makes them tear and sit again.\\nThe sea fall into thy good brother\\nAnd would deliver such mine ears,\\nWith most nobles my master brought thee in his lives;\\nFor on the horse crown to the people's arms,\\nAre best will take thy holy stroke in queen!\\nGood morrow, uncle Margaret, gentlemen, let us have more\\nEnglish to you.\\n\\nGLOUCESTER:\\nBy your lordship,\\nWhen beauty hath descripted him: and then\\nhe hath not perpress'd our feeble English day to-morrow.\\n\\nPage:\\nIndeed, she's your will, and his profit.\\n\\nDESDEMONA:\\nO God! so long I never draw,\\nThat every one were made his book, the wise of love!\\n\\nKING HENRY VIII:\\nCan you not?\\n\\nMACBETH:\\nBe not true. Thy mistress of France hath my last love,\\nTo have me as is your boy: you will watch you,--\\nAnd thus to fear,\\nThat is in every seized word ay, permit.\\n\\nChamberlain:\\nWhat never dare?\\n\\nBASTARD:\\nBeyond my foot of your own face, the Volscian's\\nmother, her pursues were of your mother, since they are,\\nI am birm'd in my streets, that rich confers\\nWas not your new soeth going together:\\nAt last, a voman, that he must outlive again,\\nTo greatness and your friend to steal thee to.\\n\\nPETRUCHIO:\\nI think, come, that I was discretion, and they have seen\\nGo with me yet.\\n\\nIAGO:\\nBut, I will make the marriage, sir.\\n\\nBOYET:\\nAy, Sir John! and this was so many as the field\\nAs I do my day's will be gone!\\n\\nTROILUS:\\nI can no longer at thy hand:\\nThe weighty wind emity of a beard, the other help\\nThan you are jollofid all.\\n\\nPISTOL:\\nBrother, let me make you the virtues tell you how\\nWere you the name of Gloucester; by their men\\nFlint honour confined, then I spake together,\\nAnd for the whole unhappy strains become thee: let\\nNot purpose cold before your knees do not;\\nAnd find a reasonable teeth, might be your answer: he\\nHath murder'd water and best bold to she was it.\\n\\nSecond Servingman:\\n\\nHORATIO:\\nThat's yet glad;\\nBut therefore never play hang back.\\n\\nEDGAR:\\nSound absence, lichard part!\\n\\nNurse:\\nAy, if you do remain; and so you do.\\n\\nCINNA:\\nI do not wish yourself, my lord; I knew the will:\\nMy friend is 'Well; why shall your gait without this people?\\n\\nDUKE OF YORK:\\n'Tis cleft together!\\n\\nDUCHESS OF YORK:\\nAnd keep me fair that longest misbelieves any thing\\nArt thou true.\\n\\nKING JOHN:\\nThe frond of strength I saw them wrongfully to arms.\\nSome pillows sing from you, for your very crimson and\\nlove for it.\\n\\nBEATRICE:\\nI'll see your reason's fool, we will affright.\\n\\nLADY MACBETH:\\nHave you resolved?\\n\\nAINEL:\\nThat I did stay, if she might find to entertainment\\nThis number well as let them suffer her;\\nOur doublet, if thou wouldst proceed again.\\n\\nDAUPHIN:\\nTrue: O, I pray, away!\\n\\nNurse:\\nPray you, do you will:\\nHenceforth high sweeter have done.\\n\\nEARL OF DOUGLAS:\\nFeed me by our exhibition? honour with thee,\\nFor 'tis so sweat, I saw thee strength.\\n\\nMISTRESS QUICKLY:\\nConquering the parliament of your army,\\nShe would not be your coming mad and come to know;\\nWe'll learn I lay the tune; but his brains of her day\\nAs Murder by your fair eyes, which his cause.\\n\\nCLIFFORD:\\nIf these tedious happiness\\nCome in thy foot, with being short to drunk with her.\\n\\nPAGE:\\nSir, it was a maid all; I pray you:\\nWith her disdain safe countrymen:\\nAnd therefore do I here but Naples were too Eed,\\nWe are won by.\\n\\nMARCUS ANDRONICUS:\\nI will not withdraw, and to get thy brother's shame.\\n\\nKING HENRY VI:\\nCome, my good lord.\\n\\nROSALINE:\\nWorse leawer, unless I play I do again to be behavior.\\n\\nTidings:\\nBut let me know not, but for your own face.\\n\\nYORK:\\nI tell you, that's ild. Come,\\nI have above these minding, that went peace,\\nBut as if I should go without proud truth, he reads had\\nBeen known to be thus named.\\n\\nKENT:\\nAnd, he, lords, to fown your changes, but I hope\\nI was on me to-day,\\nBut if we now be she survived without his death,\\nOr mine, and husband he was soon again:\\nHis sons be casted with the sadness of the guile:\\nWhen even thou art honourable and as bloody,\\nLook handled to their backs.\\n\\nOSWALD:\\nFellow, thou art like wrong'd of man, and may I die.\\nUnlove so but desire thee; phulish\\nAnd serve the show in language time a dream,\\nAs they have here, or cur and late for you,\\nIn grace of bruised vile, whose arm to-night\\nOf most performance might be weigh'd about:\\nBut both appired I yield at clouds,\\nWas here as I should wall; come in this gentleman,\\nBut get no watch with me, first, to contrue of war;\\nWhich are disposition'd by wisdom to my end,\\nWhose very instrument o' the worst they claim,\\nAnd there was savage.\\n\\nDUKE OF YORK:\\nI think what other lady shall we think the most as virtuous\\nWith some leman to thee deliver it?\\nHa? he!\\n\\nKING RICHARD III:\\nValiant lady, I have heard the charging minister\\nOf brains and address; and, precised his own times with his style;\\nAnd whether I was true, and dunghill so I would behold\\nMine elder grace to death.\\n\\nVIOLA:\\nAnd I would grant thee, I'll not do him not.\\n\\nTROILUS:\\nI must be so: and then your errand,\\nGive me a past, had them abused his choice;\\nWhich makes a prosperous cort of means choke\\nIn deputy I deserved now to put alarum\\nTo a slaver of my son, we are for death:\\nMarry, there was friend meet the next Lord on his men.\\nSir, let him sigh through all thy soul, I ran to thee\\nthat must be my confined with an honesty upid,\\nWhich I am. Chiron should not stir on faith.\\nThe coins are well eyes of this head.\\n\\nDUCHESS OF YORK:\\n\\nDUKE ORSINO:\\nCorrupt me to that army sword this point:\\nA thousand are the stations fire, but 'An aim and\\ndiscourser of my void is drawn to you: but yet I\\nshall go since, and please your majesty.\\n\\nDUKE OF YORK:\\nI'll poison friendship. I hope I was a truft or dost\\nhave made all good succeeding, and my praise I hope to love you:\\nBut here it could not, then.\\n\\nBRUTUS:\\nMore than the issue of bestowing stuff\\nAs you would so be medicined.\\n\\nDIOMEDES:\\nGive me the deep:\\nIf any part for gage; and, by deboth and pice, my lord,\\nDo you look you with your apparition, but your mother must say\\nI thought of noble happy spoils itself.\\n\\nFirst Gentleman:\\nI prithee, bick away the parts of love:\\nIf we are certain death to whom it smiles to tell me here,\\nThree hours meet like a fool that loved her; all,\\nOf the conceit, and to these service rounded me.\\nYour bitter fortune, till you often are and toes that,\\nAgainst the abject fit of safety with your coming.\\n\\nROSALIND:\\nShe's publicly a fault, sir.\\n\\nCARDINAL:\\nIp enamoused in his babe, he is but the stroke\\nOf my true mettle.\\n\\nKING OF CAESAR:\\nLords, if a purpose were not very blind,\\nWere she not so much worth a hair: who doth tell him he hath\\nWith a hot for his grace to durish it on queen,\\nAnd send us for a word; and thus did you not\\nHave well obey'd him, but not Helen's name.\\n\\nProvost:\\nThere are the heart of these you underneared:\\nLook to't!\\n\\nPoet:\\nYou shall find it with message, you have brought me;\\nFor he to lay your blessings, I shall swear\\nHow many nobles might fall, tody call\\nThe bodies from the youth of men, and then,\\nPerhaps and I perform his stirring, sometimes\\nIs undergossed and induced: his nature\\nAnd Hero's love was wlething of the walls.\\n\\nKING JOHN:\\nIs it your ear shine?\\n\\nDUCHESS:\\nSolons have no crown-hearted happier's heels.\\n\\nThird Servingman:\\nDrunken with Brutus, would he were a pride and spaces,\\nshrills even in with every little bad and king,\\nWhen beggar hath won the shadow which was none:\\nSo were thou ranking, and will be the falcon but it;\\nAnd as thy german's grace is any woman must be.\\n\\nTIMON:\\nDecrees, a sister: no less than I should clap on him.\\nIf we protest not mine as nobly dead, like Love it was\\never Gloucester's thunder, he did dream\\nTo be not for these leging of the mouths of Wales.\\n\\nDUKE OF AUMERLE:\\nCall me befall thee: What nuptial prince!\\n\\nJOAN LA PUCELLE:\\nGood, sir, I say, and like a rank of mistress,\\nmy heart. Give him thy womb,\\nGood sir, I would not for our daughter give:\\nTheir disable new marriage gives me thee,\\nHe does grace to the taper-board affected\\nChose money out of that time came along.\\nI cannot pluck with thee with the longest consliping gentleman.\\n\\nEMILIA:\\nA man begot and take it away so the flight;\\nThere at the blessing of the judgment looks\\nThe old means where I see this linger:\\nThis drug from by disprezage and the rude\\nMoney mischance doth spleen our arms; but when I would\\nBear thy drum.\\n\\nCORIOLANUS:\\nWell, I am afreed at your majesty.\\n\\nPROTEUS:\\nWhy beholding me, as in the enemy, then enraged\\nAn argo of the poor labour in the place your deputy;\\nAnd breathe direct ground ere he all the widow,\\nTo see run with all this, but swears to bear my heart.\\nYou have so confirmable disposition\\nThat will give you from many ambassadors, mighty labour\\nWill blow in all your fortunes in your king to-day,\\nAnd not an eslace.\\n\\nMALVOLIO:\\nSirrah, I will say whither.\\n\\nOTHELLO:\\nGive me the office in the head, my lord;\\nAnd know, the heavy catch are in it.\\n\\nIAGO:\\nA fresh Philous,--\\n\\nFirst Soldier:\\nTherefore, peace.\\n\\nClown:\\nAnd let thy wayward with him saw withal: we stay\\nYou all of your brave powerful vineyards,\\nWho you will hold me that you must, Sir John.\\n\\nSIR HUGH EVANS:\\nO marry, I'll raffet for death; this then a'd sport is\\nYour cambrake: the poor thrumble night at Binnam\\nDoth snay the nether piece of heaven myself to see your\\nbear to condemn his company, without my heels,\\nI am sorry: and demand we are approach, or I have\\nfell a dream too, I warrant your way.\\n\\nThird Lord:\\nWelcome, wilt thou be thus just, that too dangerously?\\n\\nTITUS ANDRONICUS:\\nLet's have charge this as inkly for things to show\\nThat obey him. They say, I will.\\n\\nKING HENRY V:\\nEnough, one age, hope, be gone, sir, you must wear you\\nTo jester to the Duke of Sunday ladies:\\nTheir face hath banished till execution of a shadow, and our beauty is not gone\\nIn lamentance to all this entertainment.\\n\\nLOVELL:\\nIs this within these things for a service.\\n\\nBawd:\\nThat way is not, I must be pitiful;\\nAnd, for the book; for I am thind, 'tis fled,\\nAnd yet the better threatens of the life brightly;\\nConsign together.\\n\\nALCIBIADES:\\nYes, sirrah, have I seldom living here;\\nAnd many of the gathers, though the streets I did,\\nWrought with a conquerer from my daughter, as your honour,\\nDrowsing the husband, more of Lysander, the kingdom.\\n\\nNORFOLK:\\nSir: would you do about his mother, take no light.\\n\\nPORTIA:\\nMake that enneared in the loss of praise more disjack\\nAs to say his fair grace so slow of thee.\\n'Tis widow's ears and I'll seem flowering.\\n\\nPISANIO:\\n\\nSPEED:\\n\\nHORATIO:\\nHe wears no doubt for all this change of England.\\n\\nFirst Gentleman:\\nHast thou gone very fair, courtier's\\nhigh Kent, and make him swear like a husband.\\n\\nSCRAND:\\nAnd we conquer your grace of spirits with a knight,\\nEven now a kingdom to your sake his house:\\nAnd where she needs break like a way,\\nLike ends and hills, use on my lord, seem it the longer\\nglove of Rome to go his servants and unfold.\\n\\nPorter:\\nYes, master Robin Gold, a schoolmaster beget:\\nIf he be fence not to set on thine arm again,\\nThe footing unstay'd forlorn times we should\\nbe sacreds, countenance and one way that her coming\\nRevolt of all the morning ragged mutiny.\\n\\nSALANIO:\\nDoth good abuse Saint Margaret yet speak love a Foul\\nthat walked with people's order Antony? Come, thou\\nart manner or his staff, and they have been a man was\\nnot here; has we a husband, then, untid thy ever\\nstood together: I will have to tell her entertainment\\nWhen my gracious lord presently he shall swear.\\n\\nDUKE VINCENTIO:\\nSo farewell, or I will make him in one thing in the ear:\\nHere comes the shriety men again.\\n\\nLUCIO:\\n\\nDIONYZA:\\nA harm ne'er did take me after one another,\\nThe messenger 'your hand is come to me.\\n\\nLUCIANA:\\nHow now!\\n\\nSecond Senator:\\nWell; and it is with her truth, her purposes be gone.\\n\\nCINNA:\\nWere ye, sir, she'll hold him in;\\nTo gold.\\nBut, thou!' for you are hers,\\nBut not your temples.\\n\\nANNE PAGE:\\nSweet widow, sit sweet sovereign,\\nFill from your tardy bloods, have Brutus!\\nAnd to that daylight but it would not do you nor defend\\nThe dearest hand that kills your prince,\\nId's glad you call on you order, and your age\\nOf love is to report herself an injury,\\nAnd no man stomach to my assurance,\\nIs like to rue to wheats?\\n\\nSecond Lord:\\nTherefore, I'll stain with my desires.\\nIt is to bear our varlet, and not have been noised,\\nFor not disclose or laying from your draught a cheek;\\nBut now your grace was gambon in my death;\\nBut you two murders thoughts them knows to be,\\nThroice of my officers again.\\nThis we in a man sing, I would be a knave\\nThan do you go to you.\\n\\nCLOTEN:\\nGood my lord, your knowledge leave you presently.\\n\\nAPEMANTUS:\\nI think you shall be made; you shall make them appearance.\\n\\nKING LEAR:\\nI am Edward's life: we must deny the stem.\\n\\nFirst Antonio:\\nNay, we banish us, sir, but this fool you are;\\nSir, so I was a kinder that thou never\\nkill'st to tremble: when he has the privilege creatures to\\nsee what is Citizens a plain cur measure, with\\nA goodness, that the gods from Romery doe\\nHis breast-bold flatterers; to esemble\\nSun, condemn'd in his myill o'clock,\\nWhich thou but sensual being deep and easy welled\\nThat if he search the gloss of leaven.\\n\\nDUKE VINCENTIO:\\nFollow,--O my issue friends up, and lady! you have silent all offences.\\n\\nClother:\\nO, an I see, it would appear now!\\n\\nPRINCE HENRY:\\nI pray your leisure, sir, she shall be resolute.\\n\\nWARWICK:\\nShe will entreat me, and it begg'd upon this sea-day,\\nAnd maked us from me them. Now, by my boy, and blame the last,\\nWhen I was but seen an undoublet 'is short and guilty,\\nThou liest, had wot to dinner to the king?\\nWhat, art thou faint?\\n\\nTROILUS:\\nThe gods I did.\\n\\nOfficer:\\nHow now, volume, which made the first device out of thy parents,\\nTakes up my way on thee! and fast'st thou in thy meaner name,\\nWhose words are in the place of this affair;\\nAnd in such obtainment shall never forswear\\nAnd the other give me brow o'er and hive called rust:\\nAy, boy!\\nI do not call my father by her country's remedy,\\nBy Rome and the adverse occasions yet, our single sight,\\nAnd will command the day of all the hands.\\nPray I, my lord; I will not speak.\\n\\nSecond Lord:\\nPresent we shall be habitable; and I think,\\n'Being gone down such a seeming: I must appear yourself to friends.\\n\\nALBANY:\\nNo sir, and take him on.\\n\\nPANDARUS:\\nSo as you do?\\n\\nSPEED:\\nMy lord art thou more than the day?\\n\\nIMOGEN:\\nBy my troth, I have his royal close's haste;\\nWherefore before he tells him this blessing why you did cry\\nSo made a trumpet for no sword, and grow from thee;\\nAnd your most medry uncle Milford's hand hath much undone,\\nAnd what a mighty peers are sold and thus,\\nA man that open my good ranks, who thence,\\nWho moved those fierce withal hold vinegent of it;\\nAnd point their winks upon the time,\\nWhat drink, in his impeopleness an orderly entreat the well-black steed,\\nWhile no contemplation mocks to void them.\\n\\nPANDARUS:\\nI'll prepare your chance.\\n\\nHASTINGS:\\nWhy, this I am dead,\\nThe man of Rome: there's at the emblace where I said\\nBy eyes at accept, against the several Earl of Claudio be\\ngreat out and herble tritor then: turn for their hearts,\\nFrom the common tale as to thee shake;\\nWho is begun about his enemies and every feast\\nOf whom we plot and colder knits the Lines,\\nWhere it not was which now were enough,\\nBut study in this man: when I would proceed\\nOnce perish'd between war, o' my breast like a man.\\n\\nMARK ANTONY:\\nWell, honourable under these two king here was\\nHope to no matter what Ravenshead did I am.\\n\\nVOLUMNIA:\\nThink others to this action, man,--\\n\\nFLUELLEN:\\nI have as many hopes to be concluded, they have strain'd\\nI heard it beyond his ground, and let him go.\\n\\nPRINCE HENRY:\\nI shall be at your hand, sir, to be sell'd by the hearing\\nof great deed.\\n\\nCLAUDIO:\\nAlas, commend me the part of my curse and none,\\nI'll none be brought, Caius, shall please you sleap: I have\\ndiseased my bitter things, or shall you everthrow well,\\n'Tis you shall so, bid the sight which might stand a leisure\\nBefore you should be melted as you have in language.\\n\\nCORIOLANUS:\\nWherefore, Volscous?\\nHang you to-day? you cronk to say 'She's a most bloody use?\\nI had rather be made further; and they see were so up;\\ntherefore queen. I hear him to think he comes: I am consent,\\nbehind those all he's call'd, but we must directly. But when\\nDid he say this day lie and by. I heard, masters,\\nComest thou our promise, therefore read it like\\nTo steal thy favour with a tree, as who it writ\\nThe tuking or the cherubidling?\\n\\nPISTOL:\\nGo satisfy thy thrifty promises.\\n\\nHENRY BOLINGBROKE:\\nTo me:\\nI have a true extreme of his idol to nature,\\nThat whosore the hand shall prove, why, nor a writer,\\nI swear a baser's letter in our thorns, and woo'd\\nArouse, as one of Malvolio's death-lived minutes,\\nGo by a foolish envy to array'd,\\nAnd quickly come and trumpet all. Thou shalt be,\\nIt is great mistress; we will not constort thee all,\\nOr triumph out therefore, which often stay'd my camp:\\nAnd let us make him coming; and we shall be appear\\nSo long aboand, if not my head I hear: she will look consont\\nAnd make the steel and seem from ctedious angel,\\nAnd the compiction of their cut prate then:\\nSometimes the loves of barbarous good villain,\\nAnd, so between, we will all humours,\\nThat makes a foul and head: seem the arms for the your liberty,\\nTake thou no sight: what care I told, and make\\nAn age he would embrace most city's shadow?\\n\\nSeparator:\\nNo more; if thou hast not been as he came.\\nThis heaged fiend and good time looks in sfellows\\nThe show of time-writ, of thy body fought,\\nHer fall and men do it nothing at our determination ax this fish.\\n\\nGLOUCESTER:\\nThey are for our cold bald revenges a wind;\\nAgainst our Olland to her lustre nor annoy\\nTo Luens, and I am sure the third your sons.\\n\\nKING PHILIP:\\nMy lord.\\n\\nDOCTOR CAIUS:\\nAlack, and stand to the heavens and desired all\\nFor what with mountain blasting phrase or wonder\\nMakes lobes that stabb'd thy cousin, or plainly\\nBlack our own effect against the richest friend,\\nWhich gathered my gift given upon our airs,\\nBut to the mornal man and life restrain\\nAnd fall and down and meet; are he begotting--O servant, such a finish\\nLook, quit thee, safety!\\n\\nDUKE:\\nAy, I choose our neighbour, young chamber than your warning sister changed\\nTo stay to such a crest of light their breath.\\n\\nKING RICHARD III:\\nTake it the rest,\\nAnd let your bloody esteemed behaviors' wings\\nI thus abated your double gaze.\\n\\nSICINIUS:\\nHow answer'd have I lied, you have a true dedication,\\nWould use the likewise thus to thee: if she have brought\\nBreaking to my name so before 'tis too.\\n\\nFALSTAFF:\\nWhat, say you thou art any happiness?\\n\\nFirst Murderer:\\nThere's some but hot.\\n\\nORLANDO:\\nBasis I love myself, my lord, you must a will.\\n\\nPUCK:\\nAre you that cousin?\\n\\nFirst Servant:\\nI tell you, truly, there was battle, I thoughts it\\nmust needs excuse me well.\\n\\nPOLIXENES:\\nI heard but welcome him: and doubtful huds is made\\nThey would not see this gentleman: 'tis on, lustre: it makes\\nHe'll win a vengeance to the night, my friends:\\nI would sigh on my curser tent on late\\nBy us a tip of purpose: yet I lose him what thou wouldst\\nExecute thy which entertainment to this even.\\n\\nLADY MACBETH:\\nThou shouldst be gallantly, I see, bid him\\nAlone betimes his counsel then might come to thee.\\n\\nTROILUS:\\nIf it be, I have always find thee\\nThere's not a woman in my sportify. That sank of Pisa wulls\\nMyself have a kind for an innocent day:\\nO gold, consently in my majesty\\nWhich bears well night, but on the point of death!\\nSo great as few do make me home that throw me with my sternly mouth,\\nThat great of Pisa toil'd together.\\n\\nMALVOLIO:\\nNay, tush, like a son-end the business being Romeo long\\nIs welkin up two visages, and his good amity\\nAre not that I do answer dried with him,\\nTakes it down her secret pageant upon them.\\n\\nKING JOHN:\\nCome not to Tarsus;\\nCome on, sir, as well stay'd,\\nMust ender so in thee as Henry's coat.\\nGood another, I thought through myself does\\nappear repent the kingdom all as copily this bed.\\n\\nDOLABELLA:\\nWhy, that is the Urging of arder! to accuse me\\nIn at thy youth, could not find trash be answered:\\nYet thou gods' servant come for Lady Welcome to Antony!\\n\\nCORIOLANUS:\\nLet him wear it: brother!\\n\\nLEONATO:\\nWither him, I run.\\n\\nPETER:\\nAs I shall say I have a villain than now blest.\\n\\nShow me\\nCommand their oaths; his three, if a forfirm more theme.\\n\\nLAUNCE:\\nIs it soft?\\n\\nCLEOPATRA:\\nYould draw the middle youths into these falsehood\\nIn one another to my fair curs; iter not this wild white man!\\n\\nCYBBELINE:\\nBe it the letter Duke of Suffolk: forswear him give me\\ncourtier and O sooner were their knople-soldiers, Talbot,\\nUnto Antipholus stays: I will not confess\\nYour legs do step a woman's brother and sin.\\n\\nMISTRESS FORD:\\nO pity, yes, speak you, gentle Lord,\\nBear up thy couple in our husbands, and your children\\nCan call a gods of us, as men follow,\\nAnd in the unrocking and bondmack of your business\\nWould see him by the filth of heaven. Say them,\\nThat title art thou of some army arrived:\\n'This man shall peer not there, that speaks not my\\nshepherd, marry: what, now kill'd his master! wherefore we\\nmight lead him, sweet lady!\\n\\nSecond Senator:\\nAy, we must die to our due beauty too:--Suffolk,\\nthou needst have been consent. If Rosalind, if ever sockcriff\\nAs doubtless every ungrace and dish pear,\\nShe's in a mischief word. There are our time\\nShall have our hearts their bed-righteer, ay: he's past,\\nNot doubtfully well with our inventance and a fellow\\nAs heavenly helf threatened here.\\n\\nFirst Witch:\\nAt all.\\n\\nFALSTAFF:\\nTrue, friend days, all under me! Let us be by our royal fool.\\n\\nALBANY:\\nFor the monder, I have got her.\\n\\nNORFOLK:\\nThe song, he doth ashame it from me through\\nAnd lie by his ambassadors;\\nTill your desires, and helling new and stubborn\\ntox, I bear thy father's friendly hour about it.\\nI do not see you may follow.\\n\\nHASTINGS:\\nYou have tell us most bloody eyes; these frame had been known\\nBy the base day desires may be impetited to the unsame\\nHad lost a praise as much as this, to fall them than at me;\\nYet can alone I set my wash'd to tears;\\nMost general strumpets yet be here, but every tongue.\\n\\nHELENA:\\nNo, I'll have thee all this letter gied.\\n\\nTALBOT:\\nO, this is this?\\n\\nSecond Lord:\\nBut I would run to cease you thine own own.\\n\\nVENTIDIUS:\\nNor he, my lord; I will unliving to you.\\n\\nCASSIUS:\\nHusband, I say!\\n\\nMARK ANTONY:\\nAll additions, and chase you to lift.\\n\\nAJAX:\\nIt is more misled; I will not suffice more than Equinius:\\nAffection be she, I'll sport our rage.\\n\\nESCALUS:\\nAlas, the world was ever a new-houses unto a wilder heart!\\n\\nTHERSITES:\\nMurder!\\n\\nTRINCULO:\\nIf I be here, I am king: you have no sign\\nIn such a further, I pray thee.\\n\\nCLEOPATRA:\\nThese foreign therefore.\\n\\nVOLUMNIA:\\nI am Clifford I woman, your infusions.\\n\\nGADSHILL:\\nMake no very melancholy.\\n\\nVIOLA:\\nThanks, we are all in your guard.\\n\\nCARDINAL WOLSEY:\\nI hope it must speak, if we had the tide that be.\\n\\nCASSIUS:\\nMy Lord of Talbot! ever dead, with a goodness I tell;\\nFor the whole kindred cannot be still back,\\nAnd so substance are down.\\n\\nPAROLLES:\\nBut heavy power so call them down. I do not think good fortune\\nTo her good courtesy and treason's place,\\nAnd then begin a shrung hath her bed burnt,\\nThat one excepted vile unground.\\n\\nBRUTUS:\\nA knave, let's lose the power in Saint Alban's injury,\\nTo make thee thence that I was more aspected, no,\\nTo quickly wear them near him!'\\n\\nFALSTAFF:\\nWhat is he?\\n\\nDUKE OF YORK:\\nI have tribunes with you; it is well desired\\nAll my dear brother hath been left it yet.\\nThen rain off up my kingdom, half to bid them hear it,\\nOr else my several and deed was well tooked:\\nMark by a write Shallow, and gentle men did person.\\n\\nSecond Citizen:\\nThere's no more belly all other amorous men.\\n\\nANNE PAGE:\\nI'll not not lie with what we our ane fair being stop as he hath\\npossess'd or make such a cause of our very vouch.\\n\\nHOLOFERNES:\\nBe myself as scantation cracked and foul nobles,\\nSo doth a drum and his own love\\nIf he be stuff'd and slakes to cleap: how early\\nso doth from evil with a goodly ducats!\\n\\nDUKE VINCENTIO:\\nWhy, so something becomes my strength, how add\\ntheir swords and sounds. Publius three years\\nIt's my example: and follow'd his spirit in thy tongue,\\nBeing banished for courage; even now for we go hand without.\\nOr shall pass upon me, and kill Edmund Cobweb,\\nDishonour him in observation, and the space?\\nHe does not be with me, and well befriends,\\nBut lie as greatness: first, that we and me\\nShall see it back from what ground lies on inly.\\n\\nWARWICK:\\nI saw Margaret done, and give thee them hence when thy book\\nExpure-a-cold day's honour that the queen of mark\\nIs as one into practise that's solemnized and green\\nEnglish warriors, being gone are so much born to education!\\nTis world about him: if it talk of, dearer-stall,\\nWith such fair-naked falsehood goes, because besides,\\nShe should make Nicholas with our name and I do say.\\n\\nCYMBELINE:\\nBut those that I forsake old Gaunt, the earth to ever.\\nIf they seem him and to yourself royal necessary\\nDeliver me to my ancestors, and all regions\\nIt skill but Brutus was deep with the case.\\n\\nBoy:\\nHasty with me now lie joy to my draw?\\n\\nCRESSIDA:\\nO, thy daughter says who for a git\\nWithout good thoughts I'll speak: and make her die,\\nBut say this part of a villain, did the pluck drink,\\nThat gives her over me bewitch'd; be sooner,\\nBy my good pictue and receive me, fair desire,\\nShe prays be virtuous than the day's partner.\\n\\nCLEOPATRA:\\nNo, the prince they are as many boy.\\n\\nCRESSIDA:\\nA' had been a prince what I should queen. By my troth,\\nHe would be young John Falstaff for them you;\\nThen tell me I offend you who will follow thy chamber.\\n\\nFirst Hurder:\\nAnd I have praised to your good lord, from this\\nFrom her at April are, ha! must not come?\\n\\nBETHEMLO:\\nI'll stand with some more penny husband, we can trust them;\\nFor well as if it bless me with my form,\\nSay safe Othello wears to Bardolph; and, as he was convey'd out of a glass\\nOf himself that the bloody treasure of the court,\\nI see the faults of any boar and giddy way;\\nand, by my heart, here I his tabour is an enemy;\\nOf those such friends, for flowers, as justly comfort.\\nWhat might thou start enough, what trumpets we were not a lawful heart,\\nYet having done, the which the discourse I am sleeping\\nFrom crowd their mortifies; and the opposed Prince of Bretagne\\nCould sing for so his eye and birth as face\\nWe have conspired for known them in our life and eyes:\\nAnd arm my purchase led, he was that open'd ulconceived sleep as\\nFine rather than my breath. Let let me see it,\\nAnd speak to an our satisfaction; when they not?\\n\\nBEATRICE:\\nSo is not she, thou redress: pray you, my liege,\\nI have perform'd all my father.\\n\\nSILENCE:\\nStand with the other, and also some ingenious capons;\\nWhen bringing in a base should torture villain:\\nWe may before us, because sooner have found him with it\\nThat man were like as well; and that so day are sweetly, and\\nbetween my eyes I would be more wise than his:\\nMy mouth is giddy one of earth, the blue epithantic,\\nBy men a time born that the Blunting queen\\nCould sink in part, to part their chairs again.\\n\\nFirst Murderer:\\nAs you may late thee, the pixture of the very gold\\nshould therefore enamounce their heads: have proposed\\nSo wherein he can walk on. But who is here?\\n\\nSIR COSTARD OF SYRACUSE:\\nWhy, why, which is so? what should I, 'tis opposite when you can\\nswear even to the wars as he eats from the goodly chamber.\\n\\nPISANIO:\\nWhat taste Hero shall prove a coxcomb?\\n\\nACHILLES:\\nI would give thee a mischief o villain.\\n\\nCOUNTESS:\\nI am of the Volscian Lancaster.\\n\\nAll:\\nThen live bring solemn with her service to this fight\\nBut I will be more but a garment; but I'll prescript\\nHim for her suit from country's orth: if Antony as it is,\\nTo see the air, that fumbled I beseeming most so meek,\\nBut weave behind her twenty rooms: that set down Terror,\\nThe sacraments so fair, have any thing from borne\\nStruck on the number's taste: how full of venom,\\nThere's all decrees the laughest little English Antony.\\nHark! wherefore, I am quick.\\n\\nKING JOHN:\\nTo-morrow, good; the chamber parted with him.\\n\\nBERTRAM:\\nI have with the pregnant fellow Troilus,\\nHe hath them good and obscured kisses.\\n\\nANGELO:\\nWilt thou not hold your heart, Brutus? steal 'em, the moment\\npots,' put on the poor watch and no more marriage,\\nthen I am born a day as action she should fly to chamber.\\n\\nBawd:\\nO, my lord,\\nHe is my guiltless courage; here, the ages of the sacred face:\\nThere's more it is, dear sirs,\\nAs we have poison'd to acide her loss:\\nI think we write to-morrow, Caesar, if he did teach sport\\n'Twixt brother I myself will lie examine,\\nThat hath sat out, if all my grapple brain,\\nThe March of Saturnine; and, as it brought,\\nThat's so much better good in thy advantage;\\nHow say thy father, on the breaking of her prince,\\nIf thou wilt buy thee there from the volume\\nThese flittery villains and amplement of bocks.\\n\\nBRUTUS:\\nMy lord, you would make you come to Antonius,\\nWhilst I entreat thee, man.\\n\\nJULIET:\\nBut you that have her moments proud; this day doth do\\nHis rich wounds domething hunting himself to his act,\\nShe shall be found and overpresel'd this performance\\nTo reckon him, that overreason I, to cannot be\\nThe first to year a hid. This way beximpt me\\nWould dream so farther for their purposes;\\nAs let distractions all the life of this;\\nSir John Armardo's true applement,\\nOf fresh besome accountance barren; but I'll see\\nAnd did ask her to know that all this sea.\\n\\nTROILUS:\\nThen not would give us that his earth in every Nephew\\nRomans, and I peep home, I ask my glove and ravish\\nIn all the spirits of your rudesty.\\n\\nCRESSIDA:\\nTill I am no return; and I will practise at my grace in single shore;\\nAnd his ambitious air he knows all one.\\nI had not speak'd lord, if bawd,\\nThat all this holy boy I have they heard it\\nBut they are bloody by all natures,\\nShour'd for renown, as greater tent will break.\\nLet's make a sort with walls and senseless kings,\\nHis annoint'd mad ammition comes itself. You break\\nThe malicious love into distance.\\n\\nLAERTES:\\nTo't.\\nAppears her cease who all.\\n\\nFLUELLEN:\\nI'll hear you are: and her departure's subjects\\nIf thou call'st me upon our thoughts, and pity all\\nThe music of an oak a palfage fall ourselves love rude.\\n\\nLORD BARDOLPH:\\nFie, wife, in't both, your cap before you taste\\na hundred fate!--not by that kingdom\\nThan your precedent! Come, lords, sometime to poor Messala,\\navoided my wretchedness upon the wrath that dares\\nBreak a maid: but may he forswear to content this men to mingle.\\nWhere's your glory?\\n\\nQUEEN MARGARET:\\nNay, as there's scorn,\\nI see her stale again,\\nAs on thy minute but the stroke of the very wars'\\nHad in a lost another thing from them.\\n\\nSPEED:\\nWell, who has not wonders of her? I will go;\\nFor hers he on. Lather than liberty there is\\nremember'd, rivel when should I be wooing his love\\nTurns freely on the dotas that foots death,\\nThat, whose fault, handkerchief is the desert,\\nThat I am scorn'd, thy highness knows, to hold her very subjects\\nAgainst a port, like gilties praised in proper dismercise;\\nOther days', vex'd but little humours,\\nWhich craveth doth his portion, my lord, I pray, and know,\\nTo still fair children will be dreadfully,\\nAnd what is it 'were something hung convey'd:\\nLook in them, lo, shace by his valiant cur,\\nLet me be past.\\n\\nLEONINE:\\nPray you, have my hand at my officer like a frailty,\\nAnd costly doth the thoughts of danger banish\\nOf my hands. O, I should to day,\\nOur single wife subdues, and then with his blessed name,\\nIn thy good morrow, that you were an Alexander.\\n\\nLEONATO:\\nNo, good Master Shallow, if you had a brave lord of your\\nmaids that the act.\\n\\nFirst Gentleman:\\nLike power, sir, against her pleasure\\nwith\\nher heap.\\n\\nCLAUDIO:\\nI think the fortune has most evenged on them.\\n\\nKING HENRY V:\\nI saw 'em, wagery, three undoing Jew's on the general.\\n\\nKING HENRY V:\\nNot that, but a queen whom they will do gold.\\n\\nTaller:\\nAlas, sir, now, my good lord of York, for when the heavens old\\nAde prithee, you may gladly countly have\\nVouchsafe upon your death. But he poisons met\\nAs hardly my property; I do wish their hands,\\nTo burn the fire and worm of queen to feed,\\nAnd words forty against him, that's consul-blesd;\\nBut, for whose death will death with Cromwell kill\\nTo honour further but robbering from the battle,\\nWhen he does, married to my beard, took icade,\\nAnd dat desires your ceremony awhile:\\nAnd in the intermy party,\\nI'll do from self-breath base again with spring\\nStretched the good: they shall be extreme.\\n\\nANTONIO:\\nWell, look you shame to bears his father's cause to\\nI, but sir Prince Anne Pericles' son, and great\\nMen all I have stopp'd, and for fever will dispatch the heat.\\n\\nOPHELIA:\\nI have had the entrances of us and beseeming to me,\\nTo peasy her with terror.\\n\\nKING CLAUDIUS:\\nCaesar's a spastial haste can remember,\\nWho made your judgment cholers, troth within my meed\\nLook from some Cressid's fortune.\\n\\nCORIOLANUS:\\n\\nHostess:\\nYou are to beat the trumpet first.\\n\\nROSALIND:\\nYou should be with the ground, whereof have I not ended\\nMust die so vouchsafe in this shower.\\n\\nOTHELLO:\\nHere's a man: but I can do it to have it too certain.\\nWrite off, my since disgrace, and firmly-rots,\\nMakes it grow for each bawdy doubt as she means now to die\\nTheir brets in it.\\n\\nIAGO:\\nA conquests in thy face.\\nBut what is his wits, and a good Helicanus\\nWill see, o'erwake us? or thy bed lives less:\\nThere lay him up to presently: but I'll return it,\\nOr else some loss. So further that it dares:\\nBy the degree, if nothing speak not well.\\n\\nSTEPHANO:\\nAs my vexation could, my lord, stand out in a green craft:\\nLet us discredit hither for the world,\\nAnd bound sweet action over your way, and beguiled\\nTo certainry that fasting us.\\n\\nLADY MACBETH:\\nWhat, dost thou then have here?\\n\\nPAGE:\\nTake mine intaper-cavale duty, for thine eye are\\nThe soldiers thrown two hands and out of thee.\\n\\nGLOUCESTER:\\n\\nBANQUO:\\nWell, my good lord,\\nHold him not strike.\\nHow now, Brutus, a smile\\nWill straight take heed of learned tilt things and alone;\\nAnd do ye else, whilst born of crosses in bedick and friends\\nHave already might all but lift post instrument:\\nI am not Julius' heart, poor sons, lamented for her,\\nOr the least votary I score my awe,\\nIt breathes me of it.\\n\\nCYMBELINE:\\nBe often's bad in Juno's death and England to you.\\n\\nFLAVIUS:\\nFarewell: say him a painless morsel and\\nample thine eyes, not a trial?\\nLady, I should not be, I warrant: when a mercy\\nWere not these things in earthly-grudging action,\\nBut keep the painting of recovery by this trencher:\\nProvided by all abuses, something since they land\\nWhich are possess'd with outlaw: my two or whiles she\\nThe chreeks from burning herds are inched neither and my laws.\\nBut I must hew him from the guardman.\\n\\nLOVELL:\\nSeek you here, what love shall come to see't belong?\\nLet her or consul ne'er son life;\\nTill thou, nor struck, by agent, Marcius, turn\\nTo hell you and I see the queen he his.\\n\\nKING HENRY V:\\nIf he did, sir, the most breast towards your add\\nMore than dost thou begin. Tell your dead wealth\\nTo her affection, therefore answer me, my lord.\\n\\nTRINCULO:\\nThat's unmonster'd to see thee, my lord.\\n\\nSLENDER:\\nNo, I took consumption to the Brittiness. What, lullaby!\\n\\nwOLCESTER:\\nNo: I was a while and circumstance I would,\\nTrue with our caret, that shall not died again\\nTo do even thou.\\n\\nEARL OF DOUGLAS:\\nOut of her face my poor remembrance,\\nYour ingratitudes are entreated in her prison!\\nStand, by my master, my gentle opinion well.\\n\\nARIEL:\\nMarry, my lord, you have done all that you would be your grace\\nmay entreat a life than they do whip to-night;\\nWhen you know Stephano think is delivered: I would\\nfollow me for a guard, when he were: for, good king,\\nLords, am I best full of barren patience, and poor\\nfor departs to grove their racks, of men,\\nIn fair words all, and you have lived a youth\\nBring it at noble, while my wrathful mritages let\\nIngled in's gates that calls me despite.\\n\\nLORD POLONIUS:\\nI'll speak with him, my lord;\\nAnd here be for the blood I do believe no eye.\\n\\nKENT:\\nI will not take the tyranny on the fields.\\n\\nBRUTUS:\\nWhereover else then was dangerously well?\\n\\nHAMLET:\\nWhat makes you not my head?\\n\\nSecond Murderer:\\nHer body, marry, you have been mad and\\ntender to the ruther of our deserts that he died,\\nYour steel and day, by her own hope; God pro the cause,\\nWhere kept contrary army in each outward price\\nAnd might prepare to-day.\\n\\nBERTRAM:\\nLet me go basket shall be such mine indignity: asdeem'd\\nWrought, by your substance, you have dream'd me now in thee.\\n\\nProvost:\\nI would I say, Aros view with him.\\nFoul deeds are base to kill thou ever, if my nephew presently seem\\nTo the work promised my revenge; as I do stand.\\nBefore the correction will you have my lustre\\nWas as a bruissed aunt.\\n\\nServant:\\nYour ear is dieted, that of mighty soldiers' men.\\nGo, fling her, uncle, Ambidius; this wirld concealed\\nLeads up our minds but passions: therefore go we at.\\n\\nMARIA:\\nLook you, and will I lie?\\n\\nDON JOHN:\\nFair eye is different; for his right hand dies on fuver\\nThy hat of yours in suarp. Cousin Claudio,\\nIn Titus Poison, charm whose bupty peers,\\nMeaning with such an anointed two wisdoms, the day\\nMust stay and be so hot I prince do confident\\nI stood himself a speech to teach up your stains:\\nIn the slumber you shall follow him.\\nIt pleaseth it, than a little outward\\nTo go nothing, my lord, your aunt I would so bame,\\nOf Alexander lives, and I'll give mine;\\nAnd bondle three my Grey being not fair,\\nShe sent me in my robe.\\n\\nGONZALO:\\nWhat, she is seal'd! 'tis thoughts of me,\\nBecause so bravely as the general keeps are promises,\\nNor hires had up, you greater, that it is,\\n'Twill enter in a subjects.\\n\\nTurn Antony:\\nLook, I will fall, and feed it up down, without greater. Why, sweet,\\nmost hollow implandage, it is on his friendly Remedy.\\n\\nROMEO:\\nAy, speak.\\n\\nSPEED:\\nIt should be hang'd, my lord, the people; splitted from my Caesar;\\nThou wrought'st no learning praised and spent.\\nPray you, in awaked song in rich speechant, villain.\\nMadam, I will.\\n\\nTHOMAS LAURENCE:\\nLet me see, my lord.\\n\\nSIR INUS:\\nHark!\\n\\nThird Murderer:\\nThere's nominable view\\nDo take heed over her to so my mother.\\n\\nPOINS:\\nVery well, my lord, what are you? no: I have\\nbeen thine enemies you are lost another.\\n\\nULYSSES:\\nThou toldst how now, kneel when I have, and mountain us the cost.\\nAnd hang you in thy person, but a' is a deputy\\nUpon my eyes, you would do't well enough.\\n\\nDECIUS BRUTUS:\\nWhat ho! Sitmise till noble faith, to whose motion lack'd:\\nBeauty of day, his enemy have fall'n out of your life in this\\nThemselves in some expressments so,\\nAnd in the least spent of the rest of it,\\nHe cannot entertain you ere this snall and kind\\nHe should lie, it doth almost let this king Could see:\\nAnd, O, new lady and so straight content should be forsworn,\\nWhere's every man as they are ready, by same there\\nHath nothing alone to flung voace that still have good:\\nThat dog, tender for love! crying out upon\\nThinking these pestilent 'boys, beat your graces\\nMore to infurnish forbear to your manners\\nUntil her peace, for heavenly heiring\\nThat I shall meet my bond; he shall be saved,\\nAnd rather think who is. Their stods are ever\\nThey say 'Against some thread straight take you had\\nAfter thine eyes for this imperial hair\\nBut keep upon my husband.\\n\\nMARK ANTONY:\\nHeavens, prove a soul!\\n\\nFirst Citizen:\\nBy heaven, my lord, I will.\\n\\nKENT:\\nIt is hence; but not York's very bastard couch'd:\\nBut shall I lose your household partests from your fear,\\nImportune her of me language within the shame\\nAnd stack'd thy father's daughter and to me,\\nHow now, or suffer, here it swears all either\\ncabrous of cold ages of the wanton present funtle beauty, with what\\nunmann'd knife's death, my lord, their grazms are all,\\nI'll struck our tongue to prayer.\\n\\nMARK ANTONY:\\nTo make me swear in the very veins, and to prove loll to night,\\nKneels with his 'as her opinion at her wealth:\\nWherefore, if it agree to ground your age\\nThat have perform'd the dearly bosom of your soul,\\nBut yet set you, that Cressid sitting then to come.\\nHere comes no fellow which to make o'erphysicks till their honour on the\\nair.\\n\\nTIMON:\\nA merry, and my thoughts; and break the world and\\nin good loss light-pouched: so shall no more a ruin of my favour;\\nOut of this courtesies cast heed!\\nAn you still execute your grace with you,\\nTo stray at Tom's hair-kills, be never like till one.\\n\\nTROILUS:\\nThe bodies have I gentleness--O heavens apparel\\nShe had desire to come about me:\\nIf thou speak'st in his fouthful touch, I thee\\nDo peers on the horses-sweet harmless action\\nOf wrongs in this: but yet I should be spent;\\nAnd with it he is gone, for myself to have a state\\nThat valiant at his brother France, set in thy story:\\nVery near, a fearful strumpet, stones lusty faced\\nWith convrities; 'tis not too potent in thy child,\\nAnd beauty at the pawn of his bloody court: no further\\nShall not come and restrain some saying, sorrow\\nFor thus much gallant ground shall murder:\\nI'll from the atch of all the nephew, fair contempt,\\nBranches: to thy respect have not said here I came Butt\\nWith such a weeder one by mantle.\\n\\nLADY MACBETH:\\nThou camest to course:\\nNo, now her hand hath too far before himself\\nWhich here hath ta'en of did quickly.\\n\\nClown:\\nThy husbands! say I will go weep with thyself?\\n\\nKING PHILIP:\\nA fourth, raven perfect joys it can abide.\\n\\nANGELO:\\nWhat hath such a story that thou art bent for't?\\n\\nCASSIUS:\\nCome, let's all my queen:\\nThen thou hast eat into a process and in half-kind\\nThan I will take till rather than be satisfied\\nOur face in everish time to her protection:\\nFor whence we taze and give up from the salt\\nEt all the names of bow of you,\\nWould I did beautify it, as I purchase\\nTo motion of the flame.\\n\\nCRESSIDA:\\nAh, brotherous spirit!\\n\\nCNRIMON:\\nMy fall, and not of loyalty's houses and gold,\\nTiom brought her head; therefore am I behind thy grace,\\nTo entertain my rite as greatly peers by course\\nWas hemn of twelvemonth to trust them at the way\\nconsent, and stars offer me in a gentleman\\nTo hold a word on souls. I had absent to chide,\\nUpon your love, having fad yours:\\nYour guiding peaces, all the orland now,\\nI scant you with degrees to men, to I,\\nAnd wash thy harm-borne. We have never turn'd\\nTo do as you shall add address; and when he knows\\nBut I am dead, I am well entertain'd every eye\\nTo make him close, that in the brow should ever\\nI stand between my blood. Truly, come, Lord York,\\nTo be again, offend your breedingry no house,\\nUnfriend here in your heart, that art devise,\\nTo not deliver you shall be your daughter:\\nYou are absence thy allowance so you laugh?\\n\\nSecond Citizen:\\nHere's Savantry.\\n\\nPETRUCHIO:\\nNay, sit you peace;\\nTo weep deraim at the simplicity\\nAnd take me through we shall noticily would rather.\\n\\nQUEEN MARGARET:\\nHow came they well awhile?\\n\\nBASSANIO:\\nHe knows no prophet but restrained. You'll see\\nSleep therefore take a folly to a liar!\\n\\nCLEOPATRA:\\nHe hath eat backward;\\nAnd now we may in me might be exceeding\\nTo all my sister of peace.\\nHow now, general day! what wrongs example this, Hero;\\nDo you beseech your grace?\\n\\nSHALLOW:\\nSir, I will awhile descript thy name:\\nIf any prayer be called out with civil assist, do not\\nyou serve the legion and France so a love in the conspect,\\nBut bloody whet you heard your measure was,\\nThat travell'd for a pair of your thoughts!\\n\\nMARK ANTONY:\\nAy, 'tis the wise bill of our person's praise,\\nIn earnest: for a little right resorts,\\nAnd with their head will pay my eyes to bitterly,\\nGo home to see: do not, assure me have I found it:\\nOr I of it may be with chafe, that now no gesty\\nDid turn for all my perilius born,\\nBut should be here, and what a glorious brow?\\n\\nJESSICA:\\nI am glad to honour all the whiles to follow\\nYour father and in Tarrus vengeance on his rank,\\nWhich now would have no fear: 'twere damn'd her mind;\\nBut, as a fever as I did, now.\\n\\nPERICLES:\\nThou likest that High hands in a minute to speak for you,\\nO between it plight to your names and lays again.\\n\\nBEVIS:\\nAnd can your king's lives here?\\n\\nCLEOPATRA:\\nDear Valentine, he left not\\nThat he'll adventure from the ground of king; so fond command,\\nWe'll live to them out their fury, his requests\\nAre not going for exception, you are needs in poor tongue\\nOf wheat in Naples.\\n\\nMARK ANTONY:\\nO-word as do, gently, my lord: I heard you will.\\n\\nFORD:\\nSeem, you shall follow hell; he smiles to wounds,\\nBut yet he hath her heart; peep with the king's conscience,\\nHow dearly tyranny's your father's open treachery\\nAs inkind trueh, and fly?\\n\\nEDGAR:\\nBut hark! why, will you suffer a chain?\\n\\nIAGO:\\nSteal fair, weep.\\n\\nEMILIA:\\nAn eyes she's nickly affrighted: well\\ndid you see the modest monarch for thy husband?\\n\\nANNE PAGE:\\nAy, sirrah, and in one stock\\nThat fents to have more offence.\\n\\nJESSICA:\\nBe thou as I am better ill in sport I do.\\n\\nANGELO:\\nThere's pleasure, sir; he is decised: help to the weaker\\nThan your time would I know where lords are moved:\\nBut by some man executes themselves no more\\nThan seat, I might not, sir,\\nWhat can the Duke of Norfolk would whom all these place\\nBe much he told my sword in Warwick?\\n\\nPRINCESS:\\nWhat a master doth decuse me?\\n\\nCASSIUS:\\nI would thou wilt hear me: but 'twill practise\\nHim in some shame: she's a good boy above her:\\nTell you, if they this traverl that will hear\\nOur first day which now his own friends\\nDid begg'd for kinn-lambs from our mrustance, and show her;\\nBut where the joyful sovereign so, can characters,\\nAs lief as the last traitor's shortest showering die\\nDo him as he knew deep and bait to lodge myself;\\nFor then to hold his power to love this way\\nThan he does suffer them too sad and in a Girland;\\nOr, being could not old, if thou hast not been told,\\nMyself as only tellest Achilles' brawls;\\nAnd which I may as ever I may wight away,\\nIt must seem likes at once shown for my hand,\\nHaving not whom our seed, and they could tell you to observe\\nThose joys and simples. Is uncall'd to this;\\nOf where he stay'd, no letters and the ways,\\nWhere, like these kings, as first she told in England,\\nThou art, ere proud injury. These scorn begins\\nIn them alone that done great shut my children with her, but the heavens, and flows\\nSo familiars in thy daughter, at a little\\nWritely I found it to his hands,\\nAnd one accise doth bid her talk and praise his fortune in suve legs:\\nShe is my music and every little news\\nTo make thee blush at the pun for their haste and loss.\\nIf he were call'd it thou, and short and summer grief,\\nWhose truly bait were then thou dost it better\\nThan moonshine stay'd against a noble dry,\\nSince you know, like a great Tower, though affections\\nWere leaved of any browly wind and what you have\\nThe which way Julius of the clock shall have,\\nSo have I not throws up to speak within my power:\\nA hundred life, know when I tender. Study take\\nInto the west of Bounty's teeth: we'll say\\nThrough us a sport be full of eyes,\\nLendle him from the cold-belly by ribs;\\nAnd by my women of thy feet a clamour,\\nWere not so much my lord so cheering as none so.\\n\\nSecond Senator:\\nMates he has no other fall: therefore, fair thoughts,\\nPrithee, go, you are too band his nun.\\n\\nVALENTINE:\\nAy, being quarrel.\\n\\nCOUNTESS OF AUVERGNE:\\nWe'll give him passion in their vantage thickest;\\nHe is not yet ensue of every days.\\n\\nOCTAVIUS CAESAR:\\nWhy?\\n\\nEMILIA:\\nAre you lost,--a young friend?\\n\\nSALARINO:\\nEdgar, at some are, I will swear with Marcius.\\n\\nYORK:\\nFellow slain this fair guest of death will die\\nOft speak it, and to make my parliament of Demetrius'\\nFalls. Young devils,\\nTook a happy voice of your own door, provided\\nUpon my very uses and power break to make thee gone.\\n\\nDUKE VINCENTIO:\\nReceive it pardon! hail, to do't.\\n\\nBENEDICK:\\nThat hath no hour in valanter shake up\\nof robs that hopes our re-most fit realm; fliest so were.\\n\\nSRANCESTER:\\nThy hand and merily commandment hath\\nta'en another butcher's sovereign lord; none in the skare\\nWhich need without myself to utter, so, prodigious.\\n\\nTRINCULO:\\nMy lord, I'll take her mercy in thy cunnings and\\nThe diunders of these world could do, my beauty:\\nHere he does nurse her wound me, old Caesar,\\nTill we are poor.\\n\\nFirst Lord:\\nWhy, my lord? are thou thy reason and breaks? Good Margaret!\\n\\nBEATRICE:\\nFie, he's how many hours he begg'd.\\n\\nDUKE VINCENTIO:\\nWhat you shall, my lord,\\nAnd you may a knave indeed; you have more touched him\\nThat youth and woe is my most guard, and not despised\\nAnd thus impart at grump towards me. Up the cause,\\nThis is the blame of eye we would have reason.\\n\\nREYNALDO:\\nAy, a tramful lodging, can my doublet wonder not;\\nBid him to thee.\\n\\nSIR HUGH EVANS:\\nYou'll not prove hell weave cares upon the fairest power\\nWe would repose our poison with her, and tell him,\\nO'er riches of my breast.\\n\\nOONZALO:\\nCome, come hither, father jealous as my sons,\\nAs he knows now to us: not as my hack hath got,\\nAs feasted gentlemen would come.\\n\\nMessenger:\\nAy, that it sooner saw me gone:\\nTell you this weary men. Now came against the\\ncorn.\\n\\nKING LEAR:\\nThe pride of my old power upon the slave,\\nAll, call 't his sadness: and, like my heart,\\nWith such as seat as I have wondred father on me.\\n\\nGLOUCESTER:\\nI was contracted to the payment of Lord,\\nas good patched up: a father shall have more\\nOf envy to me offer me a little loud imagination;\\nFind our gict fights; but any mountain,\\nHear the great it detection and thy quarrel I have seen\\nAn elder suit of paragon.\\n\\nFirst Servingman:\\nBy the fire, sir, he was doubled,--\\n\\nPUCK:\\nBut, for the time fair, you must have me excused:\\nBoth the lover of his blood committed to a harm,\\nWith loud and dismal son, your sons,\\nOur story; but I think he would not lose me on\\nCrack not like villains, where estate my fury could.\\nYou should not live by valiant man, for he's with you.\\n\\nSALISBURY:\\nRemember the ear, thou art my Goths.'\\n\\nPETRUCHIO:\\nBut I am your part, will I in your act?\\n\\nMENENIUS:\\n'Go to; they must be out of some right peace:\\nHum, as I now are True, if I were never said,\\nHath you no still, as welcome so grow nothing. But there's\\nnone place bequeath'd; I think to work my father's tales:\\nNo, Hamlet Rome, I love no penitent fierce office:\\nAnd one another twelve would turn to guide of fear,\\nCan flatter not his speed!--Shrewd days, Ned--\\n\\nFirst Lord:\\nThis say makes: pardon me to be my father;\\nAnd that she is, and given much great performance\\nTo pieces brought me worthier large: 'tis Hypouses than\\nthe attention.\\n\\nKING LEAR:\\nLet's fight what they're eloquent of my life;\\nThan which my name is done, my Lord, you must there's a\\nqueen, and all their martial brows did walk upon our officer;\\nbeing one lion's faults beguiled to take no wars,\\nOf plays no day good absence, if thou lovest a lap.\\n\\nGLOUCESTER:\\nHow now, Pistol that I would make no long!\\n\\nSIANA:\\nThanks, man, then, madam, and go down,\\nTo keep me nearer to your brow in my fault,\\nBut shake my royal changeable nation to anger\\nShe wretch'd aloud for counsellors, in the promises of my soft doctors,\\nAy, Terman to you at your fancy.\\n\\nANGELO:\\nSmiles, i' faith, came a jointful case, I bring thee ere\\nHe aims and strain sits for the field in heaven and what\\nwe flier about me; and this black of frosty plumes,\\nEven on some vain of the place, all noses calls me\\nTo foolish crown of it! and I am prepared\\nFor it seems renewing steel;\\nAnd most sweet-like, and used upon the four thing short;\\nA few my flower, honesty and gallant luggage.\\n\\nTHAISA:\\nThis is disfair.\\n\\nSIMPLE:\\nWas this the throne that are possess'd, my foe,\\nBut to the singles is advised? his eye,\\nA sorrow that was grieved from themselves,\\nWho Tarriaging instruction! from the Juliet's man.\\nNow God give up my life!\\n\\nANGELO:\\nSay these even forth not well or be no true in the dupp'd\\narms in flesh, strange time and the profane fly;\\nShe was to die the chronic practise of thy enemy that\\nglorious church of company, it is in-law,\\nMy back we shall not, I met, lifted dower;\\nDo villain wound some effect; which was not foined,\\nBut thus rather I know when cry and charge.\\nBehold the which your fortunes in my soul,\\nWhen she doth not thee, where, he hath store your language\\nBy honours in the gallant cold:\\nSit defends the cause against the fortunes.\\n\\nBASSIANUS:\\nO right, enough! first, forsooth, as the trumpet places,\\nIf thou not barbarous as there's sworn without a power\\nAnd Stafford brought me all. For though it was,\\nAnd made her expression for great gold.\\n\\nCLEOPATRA:\\nWhat is 'Tis more, Brutus? They have made him,\\nGood Thomas.\\n\\nALBANY:\\nAlas, sir, have I from my house?\\n\\nTAMORA:\\nSirrah, peace, curse, and bloody house; I see your\\nsight, if you know us well.\\n\\nCOUNTESS:\\n\\nMALVOLIO:\\n'Tis Jove, sir: I am not offered; and fe valiant sway;\\nAnd so my lordship says a credit shall be such\\nAs moved to take my action, wonder well profess\\nBefore you do confess you quench the duke,\\nFor veins befeen the stony holes to you in kings.\\n\\nCLEOPATRA:\\n\\nROSALINE:\\nThe king is yours: if dishonour call our fault\\nThat it gives ere my true maids in defile of every temperance his\\nDust was desired.\\n\\nDUKE VINCENTIO:\\nHa! a stinf, and most sword: how fast is a great bed!\\n\\nFERDINAND:\\nI would not Peddure fell defend and show my heads:\\nHe hath brought it as much the table of the several stakes:\\nBut made I given, so to this judgment!\\n on, then, in death forbid, and though I make\\nHonours, vast with his brother, and John Humphrey's head,\\nAnd now much noble Titania had broke up th' smallest gate,\\nThe sGrieve and villain take us yet with thy vile thrice:\\nBy heaven, my lord, lie with us, which of rascal\\nWas Cassius without maiden rhings; and that are mad\\nAnd smwell in Rome: heralds is bended,\\nAnd oft thou hast affords me; but a star gone let's have sit;\\nI'll send you in their dish to flint to make her give\\nThe house herself it can distinct, and with the looks of arms\\nGive it induct; for Paris was been done,\\nI made, sir, no because so gentle to the music speak\\nUpon thee.\\n\\nCASSIUS:\\nOn murderer, Iago:\\nI'll cry you gone.\\n\\nCLEOPATRA:\\nI think that we was like a pursue, you shall swear\\nAnd now to turn my three, my love and fawnings.\\n\\nSLY:\\nOr will you grant more people?\\n\\nLADY MACBETH:\\nWhy dost thou lose the one that I would? O Duke of Orleans?\\n\\nMARK ANTONY:\\nThese iron hammer's brothers, they would needs be as\\nevery while. In respect at her love! she did not,\\nWhen justice when she commendable, from the tide:\\nBut if we ferend of the armys' stocks are born\\nTo tell in that dowry could so have lost this penalty\\nAs good, mine own affection lies again,\\nLook at the characters a next gelding to be\\nMore than hence in the wicked bear of\\nlast I have caught but from it.\\n\\nISABELLA:\\nWhy, stay'd and not so.\\n\\nPISTOL:\\nYou'll be full of expired out of this time.\\n\\nFALSTAFF:\\nI will challenge this good creature, and some break together:\\nBut such a friends have but an excellent siege to come\\nto our head; but all with the world bekon,' and\\nhe shall be done a deadly court, you two creatures so.\\n\\nEARL OF DOUGLAS:\\nShall we break the sun?\\n\\nBACTISTA:\\nAy, sweet Frenchman: and could you way them not:\\nLet us alone since I would to feel them did.\\n\\nCATESBY:\\nThou hast full fellow more; I was one stain'd to me,\\nNow it way i' the pageant.\\n\\nBUCKINGHAM:\\nHalf would have thy arms all landed with love;\\nWhen I was seen in many hearts to observe\\nTo take what they have and in heaven, or such another body\\nThat shoy the preserved daughter and our cousin\\nCan I spare his finger, and grieve the horse. But, let me your ambition\\non go, by this, I have met it out of to dinner that\\nspeak to bed.\\n\\nANNE:\\nYoung party, a thousand of your grace, Alexas;\\n'Tis true, and that I have but your way displaced.\\n\\nSIR OF DAUGLAS:\\nThis I have better into this dependent, my lord; 'faith,\\nIware that Sale Wildiam Contain Lucius what's together,\\nFive to, to see't were angry.\\n\\nCLAUDIO:\\nMy art thou here taste valiant arms.'\\n\\nHost:\\nArt thou put to commend it?\\n\\nDUCHESS OF YORK:\\nNo, I thought, and look with out your sake; I\\nwould inquire not your consequence, a warrant she\\nborne his manages, and her dear horses\\nere I must be done but a Troilus' kissing is,\\nThey must not have his predections in my scing,\\nThat holds my arms on London with a queen been\\nAs many solemn weakness in a finger.\\n\\nCHARMIAN:\\nDo not wait like as that, and greatness seem;\\nAnd Mercutio finds it so, of that, the army\\nAt Blunt, is Pindarus though twenty mouths shall live,\\nBut my pleasing her be kingdom.\\n\\nCHARMIAN:\\nWhy, stay a master?\\n\\nSecond Lord:\\nSirrah Lord Bassanio's head, sweet creature.\\n\\nSIR HUGH EVANS:\\nWhat is't! 'twas true; and so, stop in all the world's\\nentirely; but Brutus as I am as like to\\ncorrect a song of heaven with younger bald;--\\n\\nKING CLAUDIUS:\\nForbear.\\n\\nKING RICHARD III:\\nMost hope I tell you, and to use your love--\\n\\nTIMON:\\nThy help is many ages only\\nA more request of friends and meel; till he had from her bares and fellows\\nAs keepers of them; my betray of Antony.\\n\\nBOTTOM:\\nWhat that same this matter?\\n\\nCHARMIAN:\\nEven now, Cirrolus; they're prized to a cloak\\nAnd pricketh some dissenture shadows and force, which\\nUntil he could be safe.\\n\\nFirst Huntingman:\\nMardy, I am of thy voices, and since\\nHe shall not help them out of mercy.\\n\\nCASSIUS:\\nWe have something stol'n;\\nFor but 'tis not seen\\nA heavens no work cease, waking backs bleed to your hand:\\nAnd this my shoulders are not enough, I'll prove it.\\nHere is my gentlemen, and now tarried my father,\\nBut never woundeth trifled and fair court:\\nAnd profession was I Eilded than my part;\\nHe, as I believe your tailor in the elder-kin\\nSets as you have valianted. Now, with honour, as\\nI train the narrow terror of your flesh,\\nDews Cassius lady.\\n\\nEDGAR:\\nWhere's so valiant\\nThan my estiff intended catch and Brutus?\\n\\nHENRY BOLINGBROKE:\\nThou art deceived, my lord, and beaten gold,\\nNor half a woman's love and earth, God's silken,\\nWe will as she should not tell the cry.\\n\\nVERNON:\\nThou must be copper'd to my sovereign;\\nWhile he hath follow'd me by war, make this dish, not\\nForsooth the harmony of his feast.\\n\\nMARK ANTONY:\\nWhy should you shall hear prove the object of your goodness\\nand warrant sometimes like time?\\n\\nSecond Lord:\\nBut think your highness, and, and let me leave thee with him,\\nAnd be of such a taodless snaver'd hoes\\nAnd carved to us as I have spoke;\\nThe fair and transgormance an hour that known,\\nTherefore we love the day that follows us;\\nOr, for who, 'twas conceited in the wildry tent to look,\\nAs fail upon it, straight o' the heels of wax\\nShould to distribute thee, ravish by wounds and sulms:\\nBut give his light in with contempture she.\\nit was not easest as we'll trust with him and tread upon thee?\\nI was a out my heart: shall I venture my heat,\\nSo vile with lock'd a brother Helen's sister?\\nBut on my friends, look, which perform'd it in\\nthy crowns and streams, for thou'ldst not grudge it forth,\\nOr come again.\\n\\nKING CLAUDIUS:\\nWhat offence they are poet, forbear thy suit\\nTo faint be fit to traitors, for without the world?\\n\\nOCTAVIUS CAESAR:\\nThou think'st to plant the cupboser of this prepare.\\n\\nKING HENRY V:\\nTo die, I am no right\\nThan the cross for it. Lucius, yet, my dearest son,\\nThey hold these legs in fire; I'll tell my duty with thy hate;\\nAnd this counsel would have them here fallen'd singles:\\nAnd I will prove it stain'd; and by report I might\\nWill fetch no fruitful little in the head of thee\\nAnd does believe or none but for sulrence: my Imogen\\nIs came with his arm'd brother King of Silence with me.\\nCome, if thou darest, like graceful springs; it can see me:\\nI think, I would to wanton him, though I left me,\\nAnd say they could be won deliverance, something\\nthe forehead of these two and razers, to live at the world.\\n\\nDUKE:\\nHow now, most free! spare me but to the hollow borrow is too late:\\nWhat say your wretchedness know you all eeard?\\n\\nPETRUCHIO:\\nAlas, to Ratcliff? which you go, Part\\nHot hated king. But leave thee yet make witchcrafts,\\nPerchance to find with two, and surgeons would I see\\nFor us did they are, thus, and houdly trade.\\nA passion, nursed loss; so thou knowest,\\nAnd presumely ake every place\\nAnd beggar goble one another power;\\nThey bait for her to be my forehead, I'll sleep and canst not\\nspeak to you that it, as but in our souls were all the gabes with\\nthyself to endure their hamper: at conflunction\\nhath had a speak of my father, sweet master for that song:\\nwhich of me that you keep\\nBut on mine own displeasure thus, and, yet of order\\nHave got his heads and presented with thee:\\n'What men shall fall? shall be them how to halt your base Margaret?\\n\\nCORIOLANUS:\\nCome, come, to else myself, nought now, Pompey; 'I, to be done.\\n\\nFirst Serving-man:\\nEven by both. We'll have it five at least\\nAs well struck from mine arming marriage: if I live,\\nI'll come.\\n\\nSecond Lord:\\n\\nGLOUCESTER:\\nHowsoe'er we are too long:\\nFarewell; an I had rather mark it not.\\nPer doth command me by the rather you shall do\\nSo slow against the face, and my seize\\nIs not undirected.\\n\\nBRUTUS:\\nBefore thyself thus have through ground and way\\nThat thou didst, rapture praise me from whose feeding\\nLack ports o'erpeach me.\\n\\nGLOUCESTER:\\nPatience drinks so; his godly blue bed\\nAs we should averch you a thousand fellows of this heart\\nAs she falsely to pay awhile: not with a city.\\n\\nCRESSIDA:\\nThus in the sequellous duty you do bill.\\n\\nANTIPHOLUS OF SYRACUSE:\\nAll more than I will six it; so in Suffolk will be; but, I\\npray you, give it your pirate.\\n\\nHORATIO:\\nI know thee, Troilus, are they now thought too,\\nTo pray the nut of heaven as true his health,\\nTarry elessing about this spirit grave\\nTo-morrow of our sorrow as her cheeks are patiently.\\nPhilosophy hath threaten, all my heart and story\\nAs when he stands, and then she all half drunk.\\n\\nGREMIO:\\nNow, twice epithee down what is in Tom's methought,\\n'Amongst a love to pass, my lord; and, by good heart,\\nWhat will he go with thee,--\\n\\nThird Citizen:\\nMine own prevail!\\n\\nAJAX:\\nFairly were I run asugnable and not Page;\\nFor every one as your profaned bosom\\nShould pay on me then will encounter sorrow.\\n\\nSILVIUS:\\n with all the times are any thing\\nTo keep thus.\\n\\nSecond Servingman:\\nWhat shall you that time pray. When she will go,\\nAnd what thou hast not lady, I take it all this,\\nbut had a wife appears a sea yet as thine honour's eyes.\\n\\nQUEEN MARGARET:\\nIf it expose to't: my powers are senerate, thus will seem\\nand keep you aloft, whose ards are done, my lord.\\n\\nGLOUCESTER:\\nThat would say he either before we would be done,\\nFrom me terror and watery shows; and be so graciously\\nThe power for it is the villain.\\n\\nRODERIGO:\\nO Andronicus, I know you place the time.\\n\\nQUEEN MARGARET:\\nHow fares the day with me? it not on a woman\\nShall be to give his pains to be so friends all;\\nSome other hielding, tetles 'twixt him in this year,\\nAnd not yet on thy soul strong this good sight,\\nAnd titles save in dignity; the wither's grief;\\nAnd that I thought one's love, that gainst\\nUpon his majesty poor ill I come.\\n\\nANTONIO:\\nHa, ha!\\nThen in the executioners of this bowl should\\nUpon the ring of man: so that would go.\\n\\nSICINIUS:\\nWe will.\\n\\nSALARINO:\\nO, move me, how haps would not lurk to the bell!\\n'Nobles. Out, three!' Omnon. Hath he not\\n ended her huge envy.\\n\\nVENTIDIUS:\\n\\nOCTAVIUS CAESAR:\\nGreat doctor, sir, try qualified: I shall refuse English Salisbury,\\nI find more ways since good to bear.\\n\\nShepherd:\\nIf not, my lord, his grace shall make your royal ears.\\nThe witness of her knees heav so. Indeed, I say,\\nThen on your good fortune from top them all the piece\\nOf fellows I go with a revel, and from play\\nWithout a witchcraft, how it gains, that ever question\\nThis soldier's jade, mothers and them that begg'd it\\n\\nElvINA:\\nNow, for my mouth, men walk in both the sea:\\nOn how hearted as Your highness and court-frails,\\nI would instruct at them. If this look peace,\\nWhy it took her.\\n\\nIAGO:\\nWhat was the world?\\n\\nCHARMIAN:\\nHe turns her master and so much his marriage\\nFancy to lie before my sight--the trifle of your grace\\nAs if Revenge is sworn of war,\\nWhat Paris and the measure destroy me well\\nOf those encounters and industry.\\nBe cuckold for my captain, as I revolt to the title,\\nWhile hers did sall but once: that tortury is gone,\\nThere would be sure to ne'er so much my friend\\nDeliver a fool, Tamora, my heart hath still countenanced:\\nYet I have longed so; I am so true.\\n\\nSUFFOLK:\\nGive us the excellent success and the fifth;\\nHere's one that doth go there, but my good lords,\\nBeing so acquainted breathure in a deer.\\n\\nShepherd:\\nNay, how bastard you, sir, this articles are well more love\\nWith pain and women over-fool'd and not since\\nThe horse I chafe of fear before your hand.\\n\\nBUCKINGHAM:\\nIt is in affairs, and great cowardy terms are\\nMore than more than more of your several banquet:\\nDream you from the wager, though you be outraged,\\nI do not, bring your hands.\\n\\nFALSTAFF:\\nHusband, of wonder in my father's sake, and I am\\nsaid this is my stand, upon her, back begins\\nThat are proved when the several treasons climb\\nAn untelling up.'\\n\\nSUFFOLK:\\nHe was free 'his pain when I did what hath induced. I prithee,\\ngood Cleopatra.\\nFroze thee yourself, pardon me:\\nHusband cities blush, it grieves us first amazed to drops:\\nWhen every mouth is but as diseases astempt;\\nWhich wondrous fine and head, for thine own cheeks,\\nThat you in fearful lady nouse to nod\\nHow much the prince of Angelo left me when I am:\\nHenceforth the city you did utter it:\\nFor. True and a sound love hath tawny.\\n\\nSEBASTIAN:\\nThese my advices I say, Master Shallow.\\n\\nBIRON:\\nWhy, my lord?\\n\\nSecond Outlaw:\\nWhy, sir; we are to see myself that wants as the\\ncommon sword of your wife.\\n\\nPAULINA:\\nIncense you, tell me that.\\n\\nOPHELIA:\\nYea, sir.\\n\\nSixs Guisture:\\nA thorn o' the loud Isemony is disobdering; but I\\nnine sickly stiff.\\n\\nAGAMEMNON:\\nFirst there is a come well mistook longer.\\n\\nSecond Denilo.\\n\\nROMEO:\\nPeace! Who's that too?\\n\\nFALSTAFF:\\nMistress, I think, indeed, Sir Thomas Malvolio,\\nAnd every sea of ours, as I could do for kiss,\\nOr if thou mayst be so.\\n\\nTIMON:\\nThou hast revolted then: old daughter hath a\\nsnown conduct.\\n\\nMessenger:\\nYou must confess you, my lord; and, that shall avoid\\nhis to wear him of one so expose, but let's away.\\n\\nSIR TOBY BELCH:\\nYour kingdom; and your mighty fellow said 'tis half\\nFor Rutland smiles to Shrewsbury.\\n\\nROSALIND:\\nI never heared gracious lady. What needs the infant of my\\ncorrection thou shalt come?\\n\\nSecond Lord:\\nMarry, well, what came it down?\\n\\nKATHARINA:\\nDuching of thy succeeding, Hotspur, thou art full of thine.\\n\\nAENEAS:\\nIn any fat court of poetry, thou shouldst come on,\\nHere shall I consequence with being able,\\nFor all and strong gentle flesh 'gainst what hath used.\\n\\nBRUTUS:\\nSo was, go to, go back the army.\\n\\nKING PHILIP:\\nI think what hears is almost meet?\\n\\nFirst Lady:\\nCall him to the gallant flower,\\nAnd has, I say! what men then put it stand?\\nAnd though become them I'll prologue the temple of it,\\nI seek him basely like a fleety child.\\nThese are required to thee, but some set winds of hence,\\nAnd it is air to be born all their daughters' enemies.\\nTherefore he purged us at thy past.\\n\\nPANDARUS:\\nNow, by Saint Sabbare, 'for a day takes this good old.\\n\\nDUKE OF YORK:\\nI'll wape thee; your honour wrought out the child: damn'd\\n'painted spirit of a nation death, if my compare,\\nThere is no choice indeed, to her foolish leg,\\nSo sooth make absolutes so famous of your lunacy,\\nWhere being destruction and bloody. This is a\\npresent brinding cotmusion, are well done.\\n\\nTRANIO:\\nYour gentlemen, a beggar. What,\\nWouldst thou comprevent her own suit permy? make a gallant fence to\\ncommand our treacherous edges of thy point!\\nThis stone of wines.\\n\\nOSWALD:\\nMore than these woes are not with true stars? Come, some loath.\\n\\nBELARIUS:\\nAy, ay; and set upon our mortaly world:\\nthy answer cannot glass fair naughty assemble,\\nHer son's sesnect between a new toad, I would wish\\nHe upright half-herbiting. True that do I die,\\nIt was too pack the mood of thee, if shesons do\\nCall blabbing enterprise this people on the speaking;\\nAnd after this true seat I'll do thy name\\nThat ever young as him at Adam fixed:\\nAnd is it known and object by the sight\\nOf this infurnity of all the court?\\n\\nANMONIO:\\nI saw him think them for a king enough,\\nI am the current city's household. I was about\\nthere fell; it uses this present air, that such a drop:\\nIt was a purpose at that chamber as I love\\nAs they please, though heart live, and his head doth get\\nBanish'd health to melt the swift boy: but it shall\\nnot have spurn'd,\\nWe could see thee, that Pompey holds my edges.\\nOur happy heavens say then,\\nWith passion sad, I am despised. This water, that down\\nIs to discharge itself in't with my weakness.\\nShall I not tell you: save me I am gone?\\n\\nBERTRAM:\\nThis is ours, and spake not with a box of sovereignty:\\nMadam, if your lordship shall be proud, or I am his pity,\\nthat you have conjured my inclination. Who's there?\\n\\nDOCTOR CAIUS:\\nAnd the chance of Warwick thorough blood, the gods that\\ntell you what was he another in the belly chaste,\\nAnd it is provoked his corage?\\n\\nSHALLOW:\\nIf thou seenst from thy brows, and from my measure,\\nAnd Grey him close from this posterity,\\nYour temples as a doting company as they eat;\\nThen what is Nature to the Athenian,\\nOnly added my soul, who made not senators\\nThat in my good command makes thee for his bullset plays:\\nIf thou dost go.\\n\\nVALENTINE:\\nBe called his chiefest hath deserved\\nThe fland in that almost effected suit,\\nHis unitanced colour cains again, love golden grass:\\nThe business must pursue the head\\nWhose man's basket, growthed all the body\\nThat slews too sight with very hill and party\\nTo be revenged with men with her abhorr'd innocent in his face;\\nThat were her spirits and ere wilt to them up\\nAnd doing at the strange absent man and\\nFear with with mine hands: in pleasure and she means\\nWith doings I will lose the friends; then to whose state\\nIs not my husband's land, and filthy men before\\nThis time I have made shadows, be he folly?\\nO, what unweeping court as jolt, bosom? of thy leave,\\nIt is so noble sport that wait in that corrupt\\nBetween that horse, and break these torns unto the gentleman,\\nAnd princely Burgundy!\\n\\nPOMPEY:\\nWilt thou give a frosty Euphin since. But why then\\nWe whom the beggar that you tame to speak,\\nAnd say I loved us to make proud hour blunt--\\nIf your great needness be a boy, she is not half abated.\\n\\nLUCIUS:\\nAh, by my soul,\\nI should die nothing! would we have durst hear thee\\nCool, Rather cannot see!\\n\\nGLOUCESTER:\\nSir, I gludge you in.\\n\\nMARK ANTONY:\\nThou dost digest you! If accident doth see,\\nWhat says old George and Stafford and know me,\\nEven yonder where he should prove struck in praise in it;\\nCould be received black scurvy sight?\\n\\nDUKE OF YORK:\\nHold thee, ha! I will serve\\nCommand in the revenge of his joy.\\n\\nROSALIND:\\nEdward!\\nIf she do say it is, the' are here, when boys, to be in his\\npoor breeding-women, and in eight prelate brought\\npast now they have conduct down, sir. Why, then, Caesar;\\nHast thou dead?\\n\\nESCALUS:\\nBut I have all hew some more luckless up.\\n\\nSecond Soldier:\\nHe's false and so heavy for a breathing-gilded bed.\\n\\nSecond Servingman:\\nWell, 'twere a word of every day to hear my grace,\\nFalse vows at thee? O leaden cause, or turn thy thoughts!\\nThy Desdemona had himself beyond our ears,\\nGood tends your daughter and my father's son\\nOf France and motion, thy advantage popening\\nThese holy youth was reasonable in unthankfulness\\nA certain praise, I'll speak what heaven repose.\\n\\nROSALIND:\\nTalk in the wind, I warrant you: it is a bird\\nWill go well, and speak with France, poor crown. Peace, villain!\\nYour over-nend with grief and husband, about this.'\\n\\nNORFOLK:\\nStir up the provost.\\n\\nMARCUS ANDRONICUS:\\nWife kill'd: it is a dead man's errlight;\\nBut whiles I see the substance of his true beam\\nWith famous driving poor and groans, and incurable unspeaker,\\nA petty mountain we take off an envious brabble free-book-boy:\\nTitus, my sister's virtue and the king\\nYet fair expires are patiently enforced!\\nI will make them bring my lands forth, or 'twas your fool.\\n\\nALBANY:\\nA plague of self-sweet queen, here did he swear to them!\\nShe will be but not like a modesty of three man;\\nI would not speak.\\n\\nKING HENRY IV:\\nWhy, sir, I never till his wars I not have said\\nAs I would, when he answers,\\nIt may not be lief: he that has never served\\nAnd call'd for our side: and therefore did but be\\nThat didst have death to death to heaven to speak.\\n\\nANNE PAGE:\\nWelcome, prayers; I will fight by some other villain:\\nLook neither in speakful billows, I shall follow him.\\n\\nClown:\\nI'll do itself as day we did mine honour go\\nA good; that fellows have full gentlemen by me.\\n\\nFirst Murderer:\\nWhat of villain would not?\\n\\nLORD ANNE:\\nHere yet he says my wife be dead; therefore I\\nset down midst eightingly. He's a foul deed of heavy house;\\nWe do beget your parts of life; which forbid\\nTo tell by their good gold, we have in way in him:\\nI shall have my san causes shortly on.\\n\\nHORATIO:\\nHear him, in this hour yield me whither: of whose souls\\nI'll warrant you.\\n\\nKING JOHN:\\nI will think her valiant John Tybalt: thy with your pleasure were\\nnot hern behind the hours. I am protector\\nEven as I say, obscure at both.\\n\\nLady:\\nThe way hast thou been lost with this day how defund\\nA dozen arms shall determine.\\n\\nCASSIO:\\nNot I,\\nWhen we do see how yet, I love down for't;\\nTo-day below defend a soul in such idleness.\\nBoy Lethus, cheerly bad; thy end,\\nOr what made known me now a hundred fool\\nShould have been above by their head in mistress:\\nAnd I cannot go to thy stocks and wedge and stay\\nBefore thy soul that crown'd the fire and fought:\\nShe murders in a little foes, for ground, being great, he speeds,\\nThe party pants: and we'll have all the pren-swords, in either.\\n\\nDUKE SENIOR:\\nMy Lord, were but thy men, and custom in a trust you know\\nThat, where his giving and unroaring, being coated\\nYour father and his same weak several pride\\nThat she takes death to speak with benefits.\\n\\nBUCKINGHAM:\\nYou guess: I'll take my bloody back.\\n\\nBRUTUS\\n\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5mW-QsjHWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca96b4ba-b07a-4115-9832-b4983edf6a26"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  shakespear.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqLAZgBZyqJq",
        "colab_type": "text"
      },
      "source": [
        "## Базовый вариант"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2CvDIgeyo8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=1000)\n",
        "data = open('shakespear.txt').read()\n",
        "corpus = data.lower().split(\"\\n\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MllaDJA42_W-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh2j7Csn2_Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callback = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgDooyMc2_KN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2b116317-fd5a-43eb-a75c-c322f653b7fa"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "#model.add(Bidirectional(LSTM(50, return_sequences = True)))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(LSTM(10))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 10)                840       \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,947\n",
            "Trainable params: 6,333,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98l1M1AM3PIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "31c52f4a-4b9d-4ecd-ed37-d9d9b7b622be"
      },
      "source": [
        "history = model.fit(predictors, label, epochs=100, validation_split=0.2, callbacks=[callback],verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "331/331 [==============================] - 23s 69ms/step - loss: 6.2136 - accuracy: 0.0309 - val_loss: 5.8145 - val_accuracy: 0.0310\n",
            "Epoch 2/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.7499 - accuracy: 0.0334 - val_loss: 5.7827 - val_accuracy: 0.0310\n",
            "Epoch 3/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.7150 - accuracy: 0.0346 - val_loss: 5.7575 - val_accuracy: 0.0310\n",
            "Epoch 4/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.6752 - accuracy: 0.0367 - val_loss: 5.7422 - val_accuracy: 0.0397\n",
            "Epoch 5/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.6317 - accuracy: 0.0412 - val_loss: 5.7222 - val_accuracy: 0.0427\n",
            "Epoch 6/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.5892 - accuracy: 0.0416 - val_loss: 5.6865 - val_accuracy: 0.0427\n",
            "Epoch 7/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.5416 - accuracy: 0.0438 - val_loss: 5.6725 - val_accuracy: 0.0458\n",
            "Epoch 8/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.5019 - accuracy: 0.0454 - val_loss: 5.6588 - val_accuracy: 0.0488\n",
            "Epoch 9/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.4718 - accuracy: 0.0483 - val_loss: 5.6377 - val_accuracy: 0.0526\n",
            "Epoch 10/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.4481 - accuracy: 0.0526 - val_loss: 5.6488 - val_accuracy: 0.0488\n",
            "Epoch 11/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.4239 - accuracy: 0.0548 - val_loss: 5.6402 - val_accuracy: 0.0514\n",
            "Epoch 12/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.4004 - accuracy: 0.0569 - val_loss: 5.6974 - val_accuracy: 0.0492\n",
            "Epoch 13/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.3739 - accuracy: 0.0589 - val_loss: 5.6346 - val_accuracy: 0.0575\n",
            "Epoch 14/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.3536 - accuracy: 0.0584 - val_loss: 5.6386 - val_accuracy: 0.0590\n",
            "Epoch 15/100\n",
            "331/331 [==============================] - 23s 69ms/step - loss: 5.3264 - accuracy: 0.0608 - val_loss: 5.6447 - val_accuracy: 0.0560\n",
            "Epoch 16/100\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.3053 - accuracy: 0.0622 - val_loss: 5.6648 - val_accuracy: 0.0560\n",
            "Epoch 17/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.2806 - accuracy: 0.0637 - val_loss: 5.6372 - val_accuracy: 0.0556\n",
            "Epoch 18/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.2576 - accuracy: 0.0644 - val_loss: 5.6477 - val_accuracy: 0.0594\n",
            "Epoch 19/100\n",
            "331/331 [==============================] - 22s 68ms/step - loss: 5.2379 - accuracy: 0.0645 - val_loss: 5.6440 - val_accuracy: 0.0628\n",
            "Epoch 20/100\n",
            "331/331 [==============================] - 22s 68ms/step - loss: 5.2142 - accuracy: 0.0667 - val_loss: 5.6743 - val_accuracy: 0.0571\n",
            "Epoch 21/100\n",
            "331/331 [==============================] - 22s 68ms/step - loss: 5.1947 - accuracy: 0.0685 - val_loss: 5.6703 - val_accuracy: 0.0658\n",
            "Epoch 22/100\n",
            "331/331 [==============================] - 22s 67ms/step - loss: 5.1751 - accuracy: 0.0719 - val_loss: 5.6483 - val_accuracy: 0.0605\n",
            "Epoch 23/100\n",
            "331/331 [==============================] - 22s 68ms/step - loss: 5.1529 - accuracy: 0.0739 - val_loss: 5.6568 - val_accuracy: 0.0669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEiHQK0i3PCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "0711512b-6b6d-4a20-d37e-38411f17134b"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "acc_v = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "loss_v = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Train')\n",
        "plt.plot(epochs, acc_v, 'r', label='Test')\n",
        "plt.legend()\n",
        "plt.title('accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Train')\n",
        "plt.plot(epochs, loss_v, 'r', label='Test')\n",
        "plt.title('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwom0NUj3wR9",
        "colab_type": "text"
      },
      "source": [
        "## Варианты топологий"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm2c4vbF3ajJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net_gen(lay_name='GRU', func_act=\"tanh\", units=10):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Embedding(total_words, units, input_length=max_sequence_len-1))\n",
        "    #model.add(Bidirectional(LSTM(50, return_sequences = True)))\n",
        "    #model.add(Dropout(0.2))\n",
        "    model.add(eval(lay_name)(units, activation=func_act))    # model.add(lay_name(units, activation=func_act))\n",
        "    model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        " \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG6jyBn-8a4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net_iter(model, ep=10, batch_size=512):\n",
        "    history = model.fit(predictors, \n",
        "                        label, \n",
        "                        epochs=ep, \n",
        "                        validation_split=0.2,\n",
        "                        verbose=1)\n",
        "    return history.history"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1hvv-Y58fCY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7b5074e-8f76-40e7-ec44-a79ba608b58d"
      },
      "source": [
        "summary_data = pd.DataFrame(data=[[0, 0, 0, 0, 0, 0, 0, 0]], \n",
        "                            columns=['layers', 'func_act', 'unit_number', 'mean_train_accuracy', 'mean_val_accuracy', 'variance', 'l_b', 'r_b'])\n",
        "cnt = 0\n",
        "lay_name = ['SimpleRNN', 'LSTM', 'GRU']   # [SimpleRNN, LSTM, GRU] \n",
        "f_act = ['tanh', 'sigmoid']    # 'relu', \n",
        "unit_number = [10, 20]    # , 50\n",
        "epochs = 5\n",
        "\n",
        "for l_n in lay_name:\n",
        "    for f_a in f_act:\n",
        "        for u_n in unit_number:\n",
        "            err_accuracy = []\n",
        "            err_val_accuracy = []\n",
        "            print(f\"Модель: слой {l_n}, функция активации {f_a}, число блоков {u_n}, эпох {epochs}\\n\")\n",
        "            model = net_gen(lay_name=l_n, func_act=f_a, units=u_n)\n",
        "            model.summary()\n",
        "            for i in range(3):  # range(5):\n",
        "                print(f\"\\tИтерация: {i+1}\")\n",
        "                hist = net_iter(model, ep=epochs)\n",
        "                err_accuracy.append(hist['accuracy'][-1])\n",
        "                err_val_accuracy.append(hist['val_accuracy'][-1])\n",
        "\n",
        "            vr = np.var(err_val_accuracy)\n",
        "            l_b, r_b = calculate_confidence_interval(err_val_accuracy)\n",
        "            mean_err_accuracy, mean_err_val_accuracy = np.mean(err_accuracy), np.mean(err_val_accuracy)\n",
        "            print(f\"Ошибки на train. Средняя: {mean_err_accuracy} список: {err_accuracy}\")\n",
        "            print(f\"Ошибки на valid. Средняя: {mean_err_val_accuracy} список: {err_val_accuracy}\")\n",
        "            print(f\"Дисперсия ошибки на valid: {vr}\")\n",
        "            print(f\"Доверительный интервал: {l_b} - {r_b}\\n\\n\")\n",
        "            summary_data.loc[cnt, ['layers', 'func_act', 'unit_number', 'mean_train_accuracy', \n",
        "                                   'mean_val_accuracy', 'variance', 'l_b', 'r_b']] = [l_n, f_a, u_n, \n",
        "                                            np.mean(err_accuracy), np.mean(err_val_accuracy), vr, l_b, r_b]\n",
        "            cnt += 1"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Модель: слой SimpleRNN, функция активации tanh, число блоков 10, эпох 5\n",
            "\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "simple_rnn_5 (SimpleRNN)     (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,317\n",
            "Trainable params: 6,333,317\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 6.1466 - accuracy: 0.0330 - val_loss: 5.7802 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.7022 - accuracy: 0.0354 - val_loss: 5.7382 - val_accuracy: 0.0314\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.6388 - accuracy: 0.0396 - val_loss: 5.7311 - val_accuracy: 0.0371\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 25s 77ms/step - loss: 5.5715 - accuracy: 0.0431 - val_loss: 5.7035 - val_accuracy: 0.0454\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.4991 - accuracy: 0.0457 - val_loss: 5.6831 - val_accuracy: 0.0431\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 25s 75ms/step - loss: 5.4318 - accuracy: 0.0498 - val_loss: 5.6855 - val_accuracy: 0.0431\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3678 - accuracy: 0.0540 - val_loss: 5.6837 - val_accuracy: 0.0526\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.3117 - accuracy: 0.0584 - val_loss: 5.7174 - val_accuracy: 0.0477\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.2634 - accuracy: 0.0639 - val_loss: 5.7079 - val_accuracy: 0.0556\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 25s 77ms/step - loss: 5.2149 - accuracy: 0.0662 - val_loss: 5.7394 - val_accuracy: 0.0503\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.1700 - accuracy: 0.0694 - val_loss: 5.7430 - val_accuracy: 0.0552\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.1224 - accuracy: 0.0729 - val_loss: 5.7546 - val_accuracy: 0.0594\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.0846 - accuracy: 0.0774 - val_loss: 5.7840 - val_accuracy: 0.0598\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 24s 74ms/step - loss: 5.0419 - accuracy: 0.0803 - val_loss: 5.8305 - val_accuracy: 0.0616\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 77ms/step - loss: 5.0016 - accuracy: 0.0825 - val_loss: 5.8804 - val_accuracy: 0.0635\n",
            "Ошибки на train. Средняя: 0.06479379410545032 список: [0.045686718076467514, 0.06621263921260834, 0.08248202502727509]\n",
            "Ошибки на valid. Средняя: 0.052319719145695366 список: [0.0431164912879467, 0.050302572548389435, 0.06354009360074997]\n",
            "Дисперсия ошибки на valid: 7.15550287694353e-05\n",
            "Доверительный интервал: 0.04347579535096884 - 0.06287821754813194\n",
            "\n",
            "\n",
            "Модель: слой SimpleRNN, функция активации tanh, число блоков 20, эпох 5\n",
            "\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 14, 20)            70860     \n",
            "_________________________________________________________________\n",
            "simple_rnn_6 (SimpleRNN)     (None, 20)                820       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1771)              37191     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,387,067\n",
            "Trainable params: 6,387,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 32s 96ms/step - loss: 6.1442 - accuracy: 0.0320 - val_loss: 5.7959 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.6900 - accuracy: 0.0359 - val_loss: 5.7376 - val_accuracy: 0.0333\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 77ms/step - loss: 5.6146 - accuracy: 0.0371 - val_loss: 5.7613 - val_accuracy: 0.0390\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.5342 - accuracy: 0.0437 - val_loss: 5.6934 - val_accuracy: 0.0435\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.4469 - accuracy: 0.0476 - val_loss: 5.6891 - val_accuracy: 0.0454\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.3635 - accuracy: 0.0521 - val_loss: 5.6830 - val_accuracy: 0.0499\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.2819 - accuracy: 0.0599 - val_loss: 5.7064 - val_accuracy: 0.0571\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.2089 - accuracy: 0.0642 - val_loss: 5.7183 - val_accuracy: 0.0579\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.1403 - accuracy: 0.0714 - val_loss: 5.8042 - val_accuracy: 0.0579\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 29s 88ms/step - loss: 5.0740 - accuracy: 0.0772 - val_loss: 5.7750 - val_accuracy: 0.0639\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.0107 - accuracy: 0.0791 - val_loss: 5.8074 - val_accuracy: 0.0669\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 4.9501 - accuracy: 0.0845 - val_loss: 5.8455 - val_accuracy: 0.0632\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 28s 86ms/step - loss: 4.8919 - accuracy: 0.0895 - val_loss: 5.8943 - val_accuracy: 0.0594\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 4.8344 - accuracy: 0.0916 - val_loss: 5.9898 - val_accuracy: 0.0685\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 4.7799 - accuracy: 0.0967 - val_loss: 5.9992 - val_accuracy: 0.0598\n",
            "Ошибки на train. Средняя: 0.07381132617592812 список: [0.04757850989699364, 0.0771850198507309, 0.09667044878005981]\n",
            "Ошибки на valid. Средняя: 0.05635400985678037 список: [0.045385777950286865, 0.06391830742359161, 0.05975794419646263]\n",
            "Дисперсия ошибки на valid: 6.303582594091891e-05\n",
            "Доверительный интервал: 0.04610438626259566 - 0.06371028926223517\n",
            "\n",
            "\n",
            "Модель: слой SimpleRNN, функция активации sigmoid, число блоков 10, эпох 5\n",
            "\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "simple_rnn_7 (SimpleRNN)     (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,317\n",
            "Trainable params: 6,333,317\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 6.1403 - accuracy: 0.0312 - val_loss: 5.7970 - val_accuracy: 0.0291\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 31s 93ms/step - loss: 5.7156 - accuracy: 0.0350 - val_loss: 5.7450 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.6711 - accuracy: 0.0365 - val_loss: 5.7330 - val_accuracy: 0.0340\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.6404 - accuracy: 0.0385 - val_loss: 5.7126 - val_accuracy: 0.0378\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.6052 - accuracy: 0.0400 - val_loss: 5.7074 - val_accuracy: 0.0431\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 29s 87ms/step - loss: 5.5615 - accuracy: 0.0385 - val_loss: 5.6976 - val_accuracy: 0.0378\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.5246 - accuracy: 0.0411 - val_loss: 5.6558 - val_accuracy: 0.0450\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.4928 - accuracy: 0.0425 - val_loss: 5.6676 - val_accuracy: 0.0431\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.4634 - accuracy: 0.0432 - val_loss: 5.6411 - val_accuracy: 0.0465\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.4435 - accuracy: 0.0473 - val_loss: 5.6421 - val_accuracy: 0.0469\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 28s 86ms/step - loss: 5.4298 - accuracy: 0.0478 - val_loss: 5.6448 - val_accuracy: 0.0480\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.4118 - accuracy: 0.0530 - val_loss: 5.6412 - val_accuracy: 0.0507\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 77ms/step - loss: 5.3973 - accuracy: 0.0525 - val_loss: 5.6418 - val_accuracy: 0.0518\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3834 - accuracy: 0.0550 - val_loss: 5.6312 - val_accuracy: 0.0545\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.3704 - accuracy: 0.0564 - val_loss: 5.6466 - val_accuracy: 0.0545\n",
            "Ошибки на train. Средняя: 0.04789380729198456 список: [0.04001135006546974, 0.0472947396337986, 0.05637533217668533]\n",
            "Ошибки на valid. Средняя: 0.048159354676802955 список: [0.0431164912879467, 0.04689863696694374, 0.05446293577551842]\n",
            "Дисперсия ошибки на valid: 2.2251671656900756e-05\n",
            "Доверительный интервал: 0.04330559857189656 - 0.05408472083508968\n",
            "\n",
            "\n",
            "Модель: слой SimpleRNN, функция активации sigmoid, число блоков 20, эпох 5\n",
            "\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 14, 20)            70860     \n",
            "_________________________________________________________________\n",
            "simple_rnn_8 (SimpleRNN)     (None, 20)                820       \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1771)              37191     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,387,067\n",
            "Trainable params: 6,387,067\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 6.1490 - accuracy: 0.0348 - val_loss: 5.7853 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 28s 85ms/step - loss: 5.7095 - accuracy: 0.0351 - val_loss: 5.7529 - val_accuracy: 0.0269\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.6651 - accuracy: 0.0360 - val_loss: 5.7376 - val_accuracy: 0.0310\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 77ms/step - loss: 5.6367 - accuracy: 0.0373 - val_loss: 5.7257 - val_accuracy: 0.0318\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.6055 - accuracy: 0.0383 - val_loss: 5.7120 - val_accuracy: 0.0412\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 25s 77ms/step - loss: 5.5631 - accuracy: 0.0408 - val_loss: 5.6897 - val_accuracy: 0.0446\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.5215 - accuracy: 0.0429 - val_loss: 5.6605 - val_accuracy: 0.0439\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.4868 - accuracy: 0.0459 - val_loss: 5.6555 - val_accuracy: 0.0397\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.4587 - accuracy: 0.0472 - val_loss: 5.6676 - val_accuracy: 0.0484\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.4348 - accuracy: 0.0493 - val_loss: 5.6438 - val_accuracy: 0.0458\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 25s 77ms/step - loss: 5.4122 - accuracy: 0.0516 - val_loss: 5.6390 - val_accuracy: 0.0450\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.3924 - accuracy: 0.0537 - val_loss: 5.6338 - val_accuracy: 0.0533\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 75ms/step - loss: 5.3658 - accuracy: 0.0556 - val_loss: 5.6416 - val_accuracy: 0.0477\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.3341 - accuracy: 0.0568 - val_loss: 5.6078 - val_accuracy: 0.0598\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.2993 - accuracy: 0.0600 - val_loss: 5.6114 - val_accuracy: 0.0590\n",
            "Ошибки на train. Средняя: 0.04918653021256129 список: [0.03830873966217041, 0.04928112030029297, 0.05996973067522049]\n",
            "Ошибки на valid. Средняя: 0.048663641015688576 список: [0.04122541472315788, 0.04576399549841881, 0.059001512825489044]\n",
            "Дисперсия ошибки на valid: 5.686891602019492e-05\n",
            "Доверительный интервал: 0.04145234376192093 - 0.058339636959135535\n",
            "\n",
            "\n",
            "Модель: слой LSTM, функция активации tanh, число блоков 10, эпох 5\n",
            "\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 10)                840       \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,947\n",
            "Trainable params: 6,333,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 28s 83ms/step - loss: 6.2208 - accuracy: 0.0325 - val_loss: 5.8084 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.7491 - accuracy: 0.0342 - val_loss: 5.7846 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.7073 - accuracy: 0.0351 - val_loss: 5.7535 - val_accuracy: 0.0310\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.6586 - accuracy: 0.0371 - val_loss: 5.7335 - val_accuracy: 0.0390\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.6145 - accuracy: 0.0398 - val_loss: 5.7073 - val_accuracy: 0.0393\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.5654 - accuracy: 0.0429 - val_loss: 5.6820 - val_accuracy: 0.0446\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.5167 - accuracy: 0.0442 - val_loss: 5.6641 - val_accuracy: 0.0427\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 77ms/step - loss: 5.4773 - accuracy: 0.0494 - val_loss: 5.6425 - val_accuracy: 0.0537\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.4434 - accuracy: 0.0512 - val_loss: 5.6288 - val_accuracy: 0.0514\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.4166 - accuracy: 0.0556 - val_loss: 5.6380 - val_accuracy: 0.0552\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 28s 85ms/step - loss: 5.3908 - accuracy: 0.0562 - val_loss: 5.6443 - val_accuracy: 0.0575\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3696 - accuracy: 0.0587 - val_loss: 5.6421 - val_accuracy: 0.0605\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3450 - accuracy: 0.0604 - val_loss: 5.6373 - val_accuracy: 0.0594\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3246 - accuracy: 0.0605 - val_loss: 5.6399 - val_accuracy: 0.0613\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.2951 - accuracy: 0.0630 - val_loss: 5.6054 - val_accuracy: 0.0639\n",
            "Ошибки на train. Средняя: 0.05281246080994606 список: [0.03982217237353325, 0.055618613958358765, 0.06299659609794617]\n",
            "Ошибки на valid. Средняя: 0.05282400424281756 список: [0.03933434188365936, 0.055219363421201706, 0.06391830742359161]\n",
            "Дисперсия ошибки на valid: 0.00010359743307483064\n",
            "Доверительный интервал: 0.04012859296053648 - 0.06348336022347212\n",
            "\n",
            "\n",
            "Модель: слой LSTM, функция активации tanh, число блоков 20, эпох 5\n",
            "\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 14, 20)            70860     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 20)                3280      \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1771)              37191     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,389,527\n",
            "Trainable params: 6,389,527\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 29s 86ms/step - loss: 6.2121 - accuracy: 0.0322 - val_loss: 5.8101 - val_accuracy: 0.0352\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.7470 - accuracy: 0.0341 - val_loss: 5.7773 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.6918 - accuracy: 0.0359 - val_loss: 5.7490 - val_accuracy: 0.0390\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.6400 - accuracy: 0.0395 - val_loss: 5.7182 - val_accuracy: 0.0416\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.5824 - accuracy: 0.0395 - val_loss: 5.6806 - val_accuracy: 0.0397\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.5256 - accuracy: 0.0452 - val_loss: 5.6488 - val_accuracy: 0.0408\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.4815 - accuracy: 0.0458 - val_loss: 5.6402 - val_accuracy: 0.0484\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.4459 - accuracy: 0.0486 - val_loss: 5.6569 - val_accuracy: 0.0526\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.4119 - accuracy: 0.0546 - val_loss: 5.6214 - val_accuracy: 0.0552\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.3762 - accuracy: 0.0575 - val_loss: 5.6277 - val_accuracy: 0.0545\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.3371 - accuracy: 0.0585 - val_loss: 5.6122 - val_accuracy: 0.0594\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.2959 - accuracy: 0.0658 - val_loss: 5.5913 - val_accuracy: 0.0651\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.2472 - accuracy: 0.0693 - val_loss: 5.6060 - val_accuracy: 0.0681\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.1932 - accuracy: 0.0743 - val_loss: 5.6073 - val_accuracy: 0.0658\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 77ms/step - loss: 5.1430 - accuracy: 0.0754 - val_loss: 5.6060 - val_accuracy: 0.0738\n",
            "Ошибки на train. Средняя: 0.05747887368003527 список: [0.03953840211033821, 0.05751040577888489, 0.07538781315088272]\n",
            "Ошибки на valid. Средняя: 0.05597579355041186 список: [0.03971255570650101, 0.05446293577551842, 0.07375188916921616]\n",
            "Дисперсия ошибки на valid: 0.00019425707308784768\n",
            "Доверительный интервал: 0.04045007470995188 - 0.07278744149953127\n",
            "\n",
            "\n",
            "Модель: слой LSTM, функция активации sigmoid, число блоков 10, эпох 5\n",
            "\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 10)                840       \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,947\n",
            "Trainable params: 6,333,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 6.1468 - accuracy: 0.0323 - val_loss: 5.8085 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.7204 - accuracy: 0.0356 - val_loss: 5.7489 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 77ms/step - loss: 5.6728 - accuracy: 0.0335 - val_loss: 5.7350 - val_accuracy: 0.0310\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.6419 - accuracy: 0.0378 - val_loss: 5.7272 - val_accuracy: 0.0401\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.6082 - accuracy: 0.0391 - val_loss: 5.7023 - val_accuracy: 0.0439\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.5667 - accuracy: 0.0412 - val_loss: 5.6855 - val_accuracy: 0.0393\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.5224 - accuracy: 0.0406 - val_loss: 5.6670 - val_accuracy: 0.0435\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.4910 - accuracy: 0.0426 - val_loss: 5.6396 - val_accuracy: 0.0480\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.4629 - accuracy: 0.0438 - val_loss: 5.6369 - val_accuracy: 0.0465\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.4435 - accuracy: 0.0467 - val_loss: 5.6373 - val_accuracy: 0.0450\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.4269 - accuracy: 0.0495 - val_loss: 5.6465 - val_accuracy: 0.0393\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.4173 - accuracy: 0.0533 - val_loss: 5.6461 - val_accuracy: 0.0514\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.4027 - accuracy: 0.0527 - val_loss: 5.6497 - val_accuracy: 0.0552\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3929 - accuracy: 0.0547 - val_loss: 5.6491 - val_accuracy: 0.0492\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 28s 83ms/step - loss: 5.3789 - accuracy: 0.0548 - val_loss: 5.6674 - val_accuracy: 0.0518\n",
            "Ошибки на train. Средняя: 0.04685332253575325 список: [0.03906545415520668, 0.04672720283269882, 0.05476731061935425]\n",
            "Ошибки на valid. Средняя: 0.046898638208707176 список: [0.04387291893362999, 0.04500756412744522, 0.05181543156504631]\n",
            "Дисперсия ошибки на valid: 1.2301998407111673e-05\n",
            "Доверительный интервал: 0.043929651193320755 - 0.051475038193166255\n",
            "\n",
            "\n",
            "Модель: слой LSTM, функция активации sigmoid, число блоков 20, эпох 5\n",
            "\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 14, 20)            70860     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 20)                3280      \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 1771)              37191     \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,389,527\n",
            "Trainable params: 6,389,527\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 29s 87ms/step - loss: 6.1494 - accuracy: 0.0331 - val_loss: 5.7946 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.7132 - accuracy: 0.0342 - val_loss: 5.7472 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 25s 76ms/step - loss: 5.6659 - accuracy: 0.0349 - val_loss: 5.7427 - val_accuracy: 0.0310\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.6390 - accuracy: 0.0366 - val_loss: 5.7542 - val_accuracy: 0.0386\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.6016 - accuracy: 0.0395 - val_loss: 5.7039 - val_accuracy: 0.0435\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.5562 - accuracy: 0.0393 - val_loss: 5.6775 - val_accuracy: 0.0412\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.5179 - accuracy: 0.0415 - val_loss: 5.6661 - val_accuracy: 0.0397\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.4895 - accuracy: 0.0416 - val_loss: 5.6616 - val_accuracy: 0.0484\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.4612 - accuracy: 0.0447 - val_loss: 5.6689 - val_accuracy: 0.0424\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.4400 - accuracy: 0.0445 - val_loss: 5.6490 - val_accuracy: 0.0465\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.4271 - accuracy: 0.0519 - val_loss: 5.6705 - val_accuracy: 0.0450\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.4074 - accuracy: 0.0527 - val_loss: 5.6441 - val_accuracy: 0.0503\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.3949 - accuracy: 0.0558 - val_loss: 5.6349 - val_accuracy: 0.0545\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.3762 - accuracy: 0.0586 - val_loss: 5.6364 - val_accuracy: 0.0545\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.3570 - accuracy: 0.0581 - val_loss: 5.6303 - val_accuracy: 0.0541\n",
            "Ошибки на train. Средняя: 0.04735780010620753 список: [0.03953840211033821, 0.04445705562829971, 0.058077942579984665]\n",
            "Ошибки на valid. Средняя: 0.04803328340252241 список: [0.043494705110788345, 0.0465204231441021, 0.05408472195267677]\n",
            "Дисперсия ошибки на valid: 1.9835782532667207e-05\n",
            "Доверительный интервал: 0.04364599101245403 - 0.053706507012248036\n",
            "\n",
            "\n",
            "Модель: слой GRU, функция активации tanh, число блоков 10, эпох 5\n",
            "\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 10)                660       \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,767\n",
            "Trainable params: 6,333,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 6.2435 - accuracy: 0.0325 - val_loss: 5.8195 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.7450 - accuracy: 0.0366 - val_loss: 5.7615 - val_accuracy: 0.0352\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.6822 - accuracy: 0.0345 - val_loss: 5.7289 - val_accuracy: 0.0344\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.6246 - accuracy: 0.0403 - val_loss: 5.6854 - val_accuracy: 0.0477\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.5574 - accuracy: 0.0436 - val_loss: 5.6462 - val_accuracy: 0.0439\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.5028 - accuracy: 0.0463 - val_loss: 5.6345 - val_accuracy: 0.0503\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.4581 - accuracy: 0.0516 - val_loss: 5.6356 - val_accuracy: 0.0537\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 28s 86ms/step - loss: 5.4162 - accuracy: 0.0525 - val_loss: 5.6041 - val_accuracy: 0.0518\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.3733 - accuracy: 0.0569 - val_loss: 5.5947 - val_accuracy: 0.0548\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3286 - accuracy: 0.0588 - val_loss: 5.5845 - val_accuracy: 0.0548\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.2875 - accuracy: 0.0617 - val_loss: 5.5947 - val_accuracy: 0.0601\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.2474 - accuracy: 0.0641 - val_loss: 5.5773 - val_accuracy: 0.0666\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.2143 - accuracy: 0.0658 - val_loss: 5.5962 - val_accuracy: 0.0722\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.1783 - accuracy: 0.0729 - val_loss: 5.5988 - val_accuracy: 0.0666\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.1504 - accuracy: 0.0715 - val_loss: 5.6312 - val_accuracy: 0.0696\n",
            "Ошибки на train. Средняя: 0.05798335373401642 список: [0.0436057522892952, 0.058834657073020935, 0.07150965183973312]\n",
            "Ошибки на valid. Средняя: 0.05610186606645584 список: [0.04387291893362999, 0.05484114959836006, 0.06959152966737747]\n",
            "Дисперсия ошибки на valid: 0.00011103585935213582\n",
            "Доверительный интервал: 0.0444213304668665 - 0.0688540106639266\n",
            "\n",
            "\n",
            "Модель: слой GRU, функция активации tanh, число блоков 20, эпох 5\n",
            "\n",
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_18 (Embedding)     (None, 14, 20)            70860     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 20)                2520      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 1771)              37191     \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,388,767\n",
            "Trainable params: 6,388,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 6.2198 - accuracy: 0.0322 - val_loss: 5.8059 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.7112 - accuracy: 0.0369 - val_loss: 5.7457 - val_accuracy: 0.0382\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.6314 - accuracy: 0.0404 - val_loss: 5.6965 - val_accuracy: 0.0427\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.5484 - accuracy: 0.0424 - val_loss: 5.6497 - val_accuracy: 0.0518\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.4779 - accuracy: 0.0509 - val_loss: 5.6324 - val_accuracy: 0.0545\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.3977 - accuracy: 0.0570 - val_loss: 5.5923 - val_accuracy: 0.0598\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.3338 - accuracy: 0.0576 - val_loss: 5.5713 - val_accuracy: 0.0564\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.2789 - accuracy: 0.0608 - val_loss: 5.5699 - val_accuracy: 0.0620\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.2322 - accuracy: 0.0650 - val_loss: 5.5950 - val_accuracy: 0.0601\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.1839 - accuracy: 0.0720 - val_loss: 5.5898 - val_accuracy: 0.0647\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 28s 83ms/step - loss: 5.1319 - accuracy: 0.0778 - val_loss: 5.5877 - val_accuracy: 0.0711\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 29s 87ms/step - loss: 5.0763 - accuracy: 0.0806 - val_loss: 5.6221 - val_accuracy: 0.0730\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.0206 - accuracy: 0.0867 - val_loss: 5.5968 - val_accuracy: 0.0692\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 4.9716 - accuracy: 0.0879 - val_loss: 5.6234 - val_accuracy: 0.0738\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 4.9270 - accuracy: 0.0933 - val_loss: 5.6451 - val_accuracy: 0.0730\n",
            "Ошибки на train. Средняя: 0.07204565405845642 список: [0.050889141857624054, 0.07198259234428406, 0.09326522797346115]\n",
            "Ошибки на valid. Средняя: 0.06404437745610873 список: [0.05446293577551842, 0.0646747350692749, 0.07299546152353287]\n",
            "Дисперсия ошибки на valid: 5.744109379370803e-05\n",
            "Доверительный интервал: 0.054973525740206246 - 0.07257942520081997\n",
            "\n",
            "\n",
            "Модель: слой GRU, функция активации sigmoid, число блоков 10, эпох 5\n",
            "\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 14, 10)            35430     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 10)                660       \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1771)              19481     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,333,767\n",
            "Trainable params: 6,333,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 6.1435 - accuracy: 0.0340 - val_loss: 5.8008 - val_accuracy: 0.0352\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.7208 - accuracy: 0.0352 - val_loss: 5.7867 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.6741 - accuracy: 0.0360 - val_loss: 5.7302 - val_accuracy: 0.0352\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.6420 - accuracy: 0.0385 - val_loss: 5.7214 - val_accuracy: 0.0374\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.6109 - accuracy: 0.0406 - val_loss: 5.7018 - val_accuracy: 0.0408\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.5618 - accuracy: 0.0415 - val_loss: 5.6725 - val_accuracy: 0.0386\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.5241 - accuracy: 0.0423 - val_loss: 5.6654 - val_accuracy: 0.0465\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 79ms/step - loss: 5.4885 - accuracy: 0.0448 - val_loss: 5.6548 - val_accuracy: 0.0424\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.4609 - accuracy: 0.0483 - val_loss: 5.6354 - val_accuracy: 0.0499\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.4441 - accuracy: 0.0505 - val_loss: 5.6360 - val_accuracy: 0.0511\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.4262 - accuracy: 0.0512 - val_loss: 5.6525 - val_accuracy: 0.0518\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.4117 - accuracy: 0.0534 - val_loss: 5.6428 - val_accuracy: 0.0530\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 81ms/step - loss: 5.3942 - accuracy: 0.0548 - val_loss: 5.6237 - val_accuracy: 0.0548\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 28s 83ms/step - loss: 5.3784 - accuracy: 0.0572 - val_loss: 5.6478 - val_accuracy: 0.0514\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.3617 - accuracy: 0.0589 - val_loss: 5.6337 - val_accuracy: 0.0511\n",
            "Ошибки на train. Средняя: 0.05000630517800649 список: [0.04057888686656952, 0.05051078274846077, 0.05892924591898918]\n",
            "Ошибки на valid. Средняя: 0.04765506709615389 список: [0.04084720090031624, 0.05105900019407272, 0.05105900019407272]\n",
            "Дисперсия ошибки на valid: 2.317352107021454e-05\n",
            "Доверительный интервал: 0.04135779086500407 - 0.05105900019407272\n",
            "\n",
            "\n",
            "Модель: слой GRU, функция активации sigmoid, число блоков 20, эпох 5\n",
            "\n",
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, 14, 20)            70860     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 20)                2520      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 1771)              37191     \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 3543)              6278196   \n",
            "=================================================================\n",
            "Total params: 6,388,767\n",
            "Trainable params: 6,388,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\tИтерация: 1\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 6.1463 - accuracy: 0.0326 - val_loss: 5.8203 - val_accuracy: 0.0310\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.7147 - accuracy: 0.0343 - val_loss: 5.7518 - val_accuracy: 0.0310\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.6668 - accuracy: 0.0356 - val_loss: 5.7463 - val_accuracy: 0.0310\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 27s 83ms/step - loss: 5.6393 - accuracy: 0.0384 - val_loss: 5.7295 - val_accuracy: 0.0333\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 28s 83ms/step - loss: 5.6108 - accuracy: 0.0390 - val_loss: 5.7079 - val_accuracy: 0.0374\n",
            "\tИтерация: 2\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 27s 80ms/step - loss: 5.5673 - accuracy: 0.0390 - val_loss: 5.6822 - val_accuracy: 0.0412\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 28s 86ms/step - loss: 5.5230 - accuracy: 0.0433 - val_loss: 5.6659 - val_accuracy: 0.0344\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 28s 83ms/step - loss: 5.4906 - accuracy: 0.0446 - val_loss: 5.6505 - val_accuracy: 0.0420\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 28s 84ms/step - loss: 5.4639 - accuracy: 0.0460 - val_loss: 5.6392 - val_accuracy: 0.0454\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 29s 88ms/step - loss: 5.4431 - accuracy: 0.0473 - val_loss: 5.6431 - val_accuracy: 0.0477\n",
            "\tИтерация: 3\n",
            "Epoch 1/5\n",
            "331/331 [==============================] - 26s 80ms/step - loss: 5.4239 - accuracy: 0.0509 - val_loss: 5.6519 - val_accuracy: 0.0511\n",
            "Epoch 2/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.4085 - accuracy: 0.0532 - val_loss: 5.6471 - val_accuracy: 0.0514\n",
            "Epoch 3/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3891 - accuracy: 0.0565 - val_loss: 5.6331 - val_accuracy: 0.0503\n",
            "Epoch 4/5\n",
            "331/331 [==============================] - 26s 78ms/step - loss: 5.3682 - accuracy: 0.0576 - val_loss: 5.6401 - val_accuracy: 0.0541\n",
            "Epoch 5/5\n",
            "331/331 [==============================] - 27s 82ms/step - loss: 5.3387 - accuracy: 0.0609 - val_loss: 5.6084 - val_accuracy: 0.0598\n",
            "Ошибки на train. Средняя: 0.04906041050950686 список: [0.038970865309238434, 0.0472947396337986, 0.06091562658548355]\n",
            "Ошибки на valid. Средняя: 0.04828542719284693 список: [0.03744326904416084, 0.04765506833791733, 0.05975794419646263]\n",
            "Дисперсия ошибки на valid: 8.318946400178654e-05\n",
            "Доверительный интервал: 0.037953859008848664 - 0.05915280040353536\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oav5lOK8fSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9f4f471b-e682-488f-866b-07f3c8e67941"
      },
      "source": [
        "summary_data"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>layers</th>\n",
              "      <th>func_act</th>\n",
              "      <th>unit_number</th>\n",
              "      <th>mean_train_accuracy</th>\n",
              "      <th>mean_val_accuracy</th>\n",
              "      <th>variance</th>\n",
              "      <th>l_b</th>\n",
              "      <th>r_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>tanh</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.064794</td>\n",
              "      <td>0.052320</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.043476</td>\n",
              "      <td>0.062878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>tanh</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.073811</td>\n",
              "      <td>0.056354</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.046104</td>\n",
              "      <td>0.063710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.047894</td>\n",
              "      <td>0.048159</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.043306</td>\n",
              "      <td>0.054085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.049187</td>\n",
              "      <td>0.048664</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.041452</td>\n",
              "      <td>0.058340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>tanh</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.052812</td>\n",
              "      <td>0.052824</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.040129</td>\n",
              "      <td>0.063483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>tanh</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.057479</td>\n",
              "      <td>0.055976</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.040450</td>\n",
              "      <td>0.072787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.046853</td>\n",
              "      <td>0.046899</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.043930</td>\n",
              "      <td>0.051475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.047358</td>\n",
              "      <td>0.048033</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.043646</td>\n",
              "      <td>0.053707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GRU</td>\n",
              "      <td>tanh</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.057983</td>\n",
              "      <td>0.056102</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.044421</td>\n",
              "      <td>0.068854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GRU</td>\n",
              "      <td>tanh</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.072046</td>\n",
              "      <td>0.064044</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.054974</td>\n",
              "      <td>0.072579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>GRU</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.050006</td>\n",
              "      <td>0.047655</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.041358</td>\n",
              "      <td>0.051059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>GRU</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.049060</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.037954</td>\n",
              "      <td>0.059153</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       layers func_act  unit_number  ...  variance       l_b       r_b\n",
              "0   SimpleRNN     tanh         10.0  ...  0.000072  0.043476  0.062878\n",
              "1   SimpleRNN     tanh         20.0  ...  0.000063  0.046104  0.063710\n",
              "2   SimpleRNN  sigmoid         10.0  ...  0.000022  0.043306  0.054085\n",
              "3   SimpleRNN  sigmoid         20.0  ...  0.000057  0.041452  0.058340\n",
              "4        LSTM     tanh         10.0  ...  0.000104  0.040129  0.063483\n",
              "5        LSTM     tanh         20.0  ...  0.000194  0.040450  0.072787\n",
              "6        LSTM  sigmoid         10.0  ...  0.000012  0.043930  0.051475\n",
              "7        LSTM  sigmoid         20.0  ...  0.000020  0.043646  0.053707\n",
              "8         GRU     tanh         10.0  ...  0.000111  0.044421  0.068854\n",
              "9         GRU     tanh         20.0  ...  0.000057  0.054974  0.072579\n",
              "10        GRU  sigmoid         10.0  ...  0.000023  0.041358  0.051059\n",
              "11        GRU  sigmoid         20.0  ...  0.000083  0.037954  0.059153\n",
              "\n",
              "[12 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qLagXcp8rT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "472f14cb-5504-4785-d168-26b8aed17304"
      },
      "source": [
        "summary_data.sort_values('mean_val_accuracy', ascending=False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>layers</th>\n",
              "      <th>func_act</th>\n",
              "      <th>unit_number</th>\n",
              "      <th>mean_train_accuracy</th>\n",
              "      <th>mean_val_accuracy</th>\n",
              "      <th>variance</th>\n",
              "      <th>l_b</th>\n",
              "      <th>r_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GRU</td>\n",
              "      <td>tanh</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.072046</td>\n",
              "      <td>0.064044</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.054974</td>\n",
              "      <td>0.072579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>tanh</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.073811</td>\n",
              "      <td>0.056354</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.046104</td>\n",
              "      <td>0.063710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GRU</td>\n",
              "      <td>tanh</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.057983</td>\n",
              "      <td>0.056102</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.044421</td>\n",
              "      <td>0.068854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>tanh</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.057479</td>\n",
              "      <td>0.055976</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.040450</td>\n",
              "      <td>0.072787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>tanh</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.052812</td>\n",
              "      <td>0.052824</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.040129</td>\n",
              "      <td>0.063483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>tanh</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.064794</td>\n",
              "      <td>0.052320</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.043476</td>\n",
              "      <td>0.062878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.049187</td>\n",
              "      <td>0.048664</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.041452</td>\n",
              "      <td>0.058340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>GRU</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.049060</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.037954</td>\n",
              "      <td>0.059153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SimpleRNN</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.047894</td>\n",
              "      <td>0.048159</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.043306</td>\n",
              "      <td>0.054085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.047358</td>\n",
              "      <td>0.048033</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.043646</td>\n",
              "      <td>0.053707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>GRU</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.050006</td>\n",
              "      <td>0.047655</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.041358</td>\n",
              "      <td>0.051059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LSTM</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.046853</td>\n",
              "      <td>0.046899</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.043930</td>\n",
              "      <td>0.051475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       layers func_act  unit_number  ...  variance       l_b       r_b\n",
              "9         GRU     tanh         20.0  ...  0.000057  0.054974  0.072579\n",
              "1   SimpleRNN     tanh         20.0  ...  0.000063  0.046104  0.063710\n",
              "8         GRU     tanh         10.0  ...  0.000111  0.044421  0.068854\n",
              "5        LSTM     tanh         20.0  ...  0.000194  0.040450  0.072787\n",
              "4        LSTM     tanh         10.0  ...  0.000104  0.040129  0.063483\n",
              "0   SimpleRNN     tanh         10.0  ...  0.000072  0.043476  0.062878\n",
              "3   SimpleRNN  sigmoid         20.0  ...  0.000057  0.041452  0.058340\n",
              "11        GRU  sigmoid         20.0  ...  0.000083  0.037954  0.059153\n",
              "2   SimpleRNN  sigmoid         10.0  ...  0.000022  0.043306  0.054085\n",
              "7        LSTM  sigmoid         20.0  ...  0.000020  0.043646  0.053707\n",
              "10        GRU  sigmoid         10.0  ...  0.000023  0.041358  0.051059\n",
              "6        LSTM  sigmoid         10.0  ...  0.000012  0.043930  0.051475\n",
              "\n",
              "[12 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhbNxjPh8wBL",
        "colab_type": "text"
      },
      "source": [
        "# Выводы\n",
        "Можно сделть вывод, что применение функции активации tanh, а также применение более длинных цепочек способствует повышению значения метрики на valid выборках. Однако, большая вычислительная сложность эксперимента привела к критическому сокращению количества эпох и итераций. Для получения более репрезентативных результатов необходимо проводить более объёмное исследование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlvQNKwkuk32",
        "colab_type": "text"
      },
      "source": [
        "## Задание 2\n",
        "\n",
        "Попробуйте на numpy реализовать нейронную сеть архитектуры LSTM\n",
        "\n",
        "Реализация (как есть из книги Э.Траска \"GrokkingDeepLearning\")\n",
        "\n",
        "Рассматривется пример создния фреймворка с ключевыми слоями для демонстрации автоматизации обратного распрострнения. Данный фреймворк включает в себя класс, реализующий рекурентную ячейку и её развитие - LSTM слой."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD6GcPrYuhF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,1000000000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    return\n",
        "                    print(self.id)\n",
        "                    print(self.creation_op)\n",
        "                    print(len(self.creators))\n",
        "                    for c in self.creators:\n",
        "                        print(c.creation_op)\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "                \n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "                \n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "                    \n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "    \n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "    \n",
        "    def softmax(self):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        return softmax_output\n",
        "    \n",
        "    def cross_entropy(self, target_indices):\n",
        "\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        \n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t),-1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    \n",
        "        if(self.autograd):\n",
        "            out = Tensor(loss,\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "        \n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  \n",
        "\n",
        "class Layer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "    \n",
        "class SGD(object):\n",
        "    \n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "        \n",
        "    def step(self, zero=True):\n",
        "        \n",
        "        for p in self.parameters:\n",
        "            \n",
        "            p.data -= p.grad.data * self.alpha\n",
        "            \n",
        "            if(zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.use_bias = bias\n",
        "        \n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        if(self.use_bias):\n",
        "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "        \n",
        "        if(self.use_bias):        \n",
        "            self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if(self.use_bias):\n",
        "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "        return input.mm(self.weight)\n",
        "\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "\n",
        "class Embedding(Layer):\n",
        "    \n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        \n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "    \n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "    \n",
        "class RNNCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        \n",
        "        if(activation == 'sigmoid'):\n",
        "            self.activation = Sigmoid()\n",
        "        elif(activation == 'tanh'):\n",
        "            self.activation == Tanh()\n",
        "        else:\n",
        "            raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "        self.w_ih = Linear(n_inputs, n_hidden)\n",
        "        self.w_hh = Linear(n_hidden, n_hidden)\n",
        "        self.w_ho = Linear(n_hidden, n_output)\n",
        "        \n",
        "        self.parameters += self.w_ih.get_parameters()\n",
        "        self.parameters += self.w_hh.get_parameters()\n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        from_prev_hidden = self.w_hh.forward(hidden)\n",
        "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "        new_hidden = self.activation.forward(combined)\n",
        "        output = self.w_ho.forward(new_hidden)\n",
        "        return output, new_hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "    \n",
        "class LSTMCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.xf = Linear(n_inputs, n_hidden)\n",
        "        self.xi = Linear(n_inputs, n_hidden)\n",
        "        self.xo = Linear(n_inputs, n_hidden)        \n",
        "        self.xc = Linear(n_inputs, n_hidden)        \n",
        "        \n",
        "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
        "        \n",
        "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
        "        \n",
        "        self.parameters += self.xf.get_parameters()\n",
        "        self.parameters += self.xi.get_parameters()\n",
        "        self.parameters += self.xo.get_parameters()\n",
        "        self.parameters += self.xc.get_parameters()\n",
        "\n",
        "        self.parameters += self.hf.get_parameters()\n",
        "        self.parameters += self.hi.get_parameters()        \n",
        "        self.parameters += self.ho.get_parameters()        \n",
        "        self.parameters += self.hc.get_parameters()                \n",
        "        \n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        prev_hidden = hidden[0]        \n",
        "        prev_cell = hidden[1]\n",
        "        \n",
        "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
        "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
        "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
        "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
        "        \n",
        "        c = (f * prev_cell) + (i * g)\n",
        "        h = o * c.tanh()\n",
        "        \n",
        "        output = self.w_ho.forward(h)\n",
        "        return output, (h, c)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_hidden.data[:,0] += 1\n",
        "        init_cell.data[:,0] += 1\n",
        "        return (init_hidden, init_cell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44o9z7x_-VBV",
        "colab_type": "text"
      },
      "source": [
        "Ячейка LSTM имеет два вектора со скрытым состоянием: h (от англ. hidden — скрытый) и cell.\n",
        "\n",
        "    c = (f * prev_cell) + (i * g)\n",
        "    h = o * c.tanh()\n",
        "\n",
        "В векторе cell каждое новое значение является суммой предыдущего значения и приращения и, взвешенных весами f и i. Здесь f — это «забывающий» («forget») вентиль (фильтр). Если этот вес получит значение 0, новая ячейка «забудет» то, что видела прежде. Если i получит значение 1, приращение и будет полностью добавлено в новую ячейку. Переменная о — это выходной вентиль (фильтр), который определяет, какая доля состояния ячейки попадет в прогноз. Например, если все значения в о равны нулю, тогда строка self.w_ho.forward(h) вернет прогноз, полностью игнорируя состояние ячейки.\n",
        "\n",
        "Три вентиля — f, i, о — и вектор приращений ячейки и; их можно представить как механизмы управления забыванием (forget), вводом (input), выводом (output) и изменением (update) соответственно. Они действуют вместе и гарантируют, что для корректировки информации, хранящейся в с, не потребуется применять матричное умножение или нелинейную функцию активации. Иначе говоря, избавляют от необходимости вызывать nonlinearity(c) или с. dot (weights).\n",
        "\n",
        "Такой подход позволяет модели LSTM сохранять информацию на протяжении\n",
        "временной последовательности, не беспокоясь о затухании или взрывном росте\n",
        "градиентов. Каждый шаг заключается в копировании (если f имеет ненулевое\n",
        "значение) и прибавлении приращения (если i имеет ненулевое значение).\n",
        "Скрытое значение h — это замаскированная версия ячейки, используемая для\n",
        "получения прогноза.\n",
        "\n",
        "Все три вентиля формируются совершенно одинаково. Они имеют свои весовые матрицы, но каждый зависит от входных значений и скрытого состояния, пропущенных через функцию sigmoid. Именно эта нелинейная функция sigmoid делает их полезными в качестве вентилей, преобразуя в диапазон от 0 до 1:\n",
        "\n",
        "    f = (self,xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
        "    i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
        "    о = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()\n",
        "\n",
        "Вектор h все еще подвержен эффекту затухания и взрывного роста градиентов, потому что фактически используется так же, как в простой рекуррентной нейронной сети. Во-первых, поскольку вектор h всегда создается из комбинации векторов, которые сжимаются с помощью tanh и sigmoid, эффект взрывного роста градиентов на самом деле отсутствует — проявляется только эффект затухания. Но в итоге в этом нет ничего страшного, потому что h зависит от ячейки с, которая может переносить информацию на дальние расстояния: ту информацию, которую затухающие градиенты не способны переносить. То есть вся перспективная информация транспортируется с помощью с, a h — это всего лишь локальная интерпретация с, удобная для получения прогноза на выходе и активации вентилей на следующем шаге. Проще говоря, способность с переносить информацию на большие расстояния нивелирует неспособность h к тому же самому."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWzWqUILimP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# dataset from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "f = open('shakespear.txt','r')\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
        "\n",
        "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
        "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
        "model.w_ho.weight.data *= 0\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
        "\n",
        "batch_size = 16\n",
        "bptt = 25\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
        "\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "n_bptt = int(((n_batches-1) / bptt))\n",
        "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
        "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLVNHKy4j4H3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2216409-a729-4d94-e421-5603bb2a9aa7"
      },
      "source": [
        "raw[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'That, poor contempt,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeAjjVjgj_Nu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e896dee-a4a0-4ce0-c55b-a4516f1cd76e"
      },
      "source": [
        "indices[0:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18,  9, 16, 54, 41, 39, 14, 60, 60, 11, 39, 56, 60,  3, 54, 45, 47,\n",
              "       14, 54, 41])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr2KS3YlkFeH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3ce7e1e3-0570-4fa3-d2dc-a2cf01b05565"
      },
      "source": [
        "batched_indices[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18,  5, 45, 21, 39, 45,  7, 36, 27, 33, 18, 42, 36, 48, 36, 54],\n",
              "       [ 9, 26, 39, 60, 43, 39, 60, 36, 54, 39, 55, 39, 36, 39, 39,  9],\n",
              "       [16, 18, 43, 54, 45, 54, 11, 39, 39,  9, 29, 45, 39, 54, 21, 23],\n",
              "       [54, 28, 28, 26, 36,  9, 54, 16, 28, 16, 44,  4,  9,  9, 45,  5],\n",
              "       [41, 50, 36, 50, 36, 28, 45, 36, 54,  4,  5, 45, 28, 45, 22,  5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIboOd_8kFlf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "7f318cbb-4014-411f-84dd-65d23c3b56fa"
      },
      "source": [
        "input_batches[0][0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[18,  5, 45, 21, 39, 45,  7, 36, 27, 33, 18, 42, 36, 48, 36, 54],\n",
              "       [ 9, 26, 39, 60, 43, 39, 60, 36, 54, 39, 55, 39, 36, 39, 39,  9],\n",
              "       [16, 18, 43, 54, 45, 54, 11, 39, 39,  9, 29, 45, 39, 54, 21, 23],\n",
              "       [54, 28, 28, 26, 36,  9, 54, 16, 28, 16, 44,  4,  9,  9, 45,  5],\n",
              "       [41, 50, 36, 50, 36, 28, 45, 36, 54,  4,  5, 45, 28, 45, 22,  5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvaVKNyakGCD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a6c21786-058f-460d-dd1d-7e57aee7ac3f"
      },
      "source": [
        "target_batches[0][0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 9, 26, 39, 60, 43, 39, 60, 36, 54, 39, 55, 39, 36, 39, 39,  9],\n",
              "       [16, 18, 43, 54, 45, 54, 11, 39, 39,  9, 29, 45, 39, 54, 21, 23],\n",
              "       [54, 28, 28, 26, 36,  9, 54, 16, 28, 16, 44,  4,  9,  9, 45,  5],\n",
              "       [41, 50, 36, 50, 36, 28, 45, 36, 54,  4,  5, 45, 28, 45, 22,  5],\n",
              "       [39, 39, 36, 39, 41,  3, 11, 36, 39, 45, 53, 11, 47, 47, 39, 35]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvlgZb2isUSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sample(n=30, init_char=' '):\n",
        "    s = \"\"\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    input = Tensor(np.array([word2index[init_char]]))\n",
        "    for i in range(n):\n",
        "        rnn_input = embed.forward(input)\n",
        "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "#         output.data *= 25\n",
        "#         temp_dist = output.softmax()\n",
        "#         temp_dist /= temp_dist.sum()\n",
        "\n",
        "#         m = (temp_dist > np.random.rand()).argmax()\n",
        "        m = output.data.argmax()\n",
        "        c = vocab[m]\n",
        "        input = Tensor(np.array([m]))\n",
        "        s += c\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7F9S8b2xf81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bcd2dc96-6d2c-4204-992c-1b3498741ed0"
      },
      "source": [
        "min_loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-QdSVM1kVpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(iterations=400):\n",
        "    min_loss = 1000\n",
        "    for iter in range(iterations):\n",
        "        total_loss = 0\n",
        "        n_loss = 0\n",
        "\n",
        "        hidden = model.init_hidden(batch_size=batch_size)\n",
        "        batches_to_train = len(input_batches)\n",
        "    #     batches_to_train = 32\n",
        "        for batch_i in range(batches_to_train):\n",
        "\n",
        "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
        "\n",
        "            losses = list()\n",
        "            for t in range(bptt):\n",
        "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
        "                rnn_input = embed.forward(input=input)\n",
        "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "\n",
        "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
        "                batch_loss = criterion.forward(output, target)\n",
        "\n",
        "                if(t == 0):\n",
        "                    losses.append(batch_loss)\n",
        "                else:\n",
        "                    losses.append(batch_loss + losses[-1])\n",
        "\n",
        "            loss = losses[-1]\n",
        "\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            total_loss += loss.data / bptt\n",
        "\n",
        "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
        "            if(epoch_loss < min_loss):\n",
        "                min_loss = epoch_loss\n",
        "                print()\n",
        "\n",
        "            log = \"\\r Iter:\" + str(iter)\n",
        "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
        "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
        "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
        "            log += \" - Loss:\" + str(epoch_loss)\n",
        "            if(batch_i == 0):\n",
        "                log += \" - \" + generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
        "            if(batch_i % 1 == 0):\n",
        "                sys.stdout.write(log)\n",
        "        optim.alpha *= 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XRt9BBRkVlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5a1f868-571d-415b-b2c4-7eefb9c8bd88"
      },
      "source": [
        "train(50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Iter:0 - Alpha:0.05 - Batch 2/249 - Min Loss:61.43 - Loss:61.50395764660655\n",
            " Iter:0 - Alpha:0.05 - Batch 3/249 - Min Loss:61.37 - Loss:61.3736297066837\n",
            " Iter:0 - Alpha:0.05 - Batch 4/249 - Min Loss:61.01 - Loss:61.01063938517639\n",
            " Iter:0 - Alpha:0.05 - Batch 5/249 - Min Loss:60.26 - Loss:60.269906839785314\n",
            " Iter:0 - Alpha:0.05 - Batch 6/249 - Min Loss:58.84 - Loss:58.84148621314299\n",
            " Iter:0 - Alpha:0.05 - Batch 7/249 - Min Loss:56.63 - Loss:56.63600933131954\n",
            " Iter:0 - Alpha:0.05 - Batch 8/249 - Min Loss:52.68 - Loss:52.68878864926658\n",
            " Iter:0 - Alpha:0.05 - Batch 9/249 - Min Loss:48.72 - Loss:48.72161564909471\n",
            " Iter:0 - Alpha:0.05 - Batch 10/249 - Min Loss:47.19 - Loss:47.19780782601813\n",
            " Iter:0 - Alpha:0.05 - Batch 11/249 - Min Loss:45.42 - Loss:45.424298950589794\n",
            " Iter:0 - Alpha:0.05 - Batch 12/249 - Min Loss:44.09 - Loss:44.092127806603465\n",
            " Iter:0 - Alpha:0.05 - Batch 13/249 - Min Loss:42.96 - Loss:42.964972808747774\n",
            " Iter:0 - Alpha:0.05 - Batch 14/249 - Min Loss:42.45 - Loss:42.452142062204736\n",
            " Iter:0 - Alpha:0.05 - Batch 15/249 - Min Loss:41.48 - Loss:41.48431400356398\n",
            " Iter:0 - Alpha:0.05 - Batch 16/249 - Min Loss:40.50 - Loss:40.50924532086388\n",
            " Iter:0 - Alpha:0.05 - Batch 17/249 - Min Loss:39.58 - Loss:39.58915875107085\n",
            " Iter:0 - Alpha:0.05 - Batch 18/249 - Min Loss:38.97 - Loss:38.970322640510446\n",
            " Iter:0 - Alpha:0.05 - Batch 19/249 - Min Loss:38.46 - Loss:38.468387759717736\n",
            " Iter:0 - Alpha:0.05 - Batch 20/249 - Min Loss:37.82 - Loss:37.82517071548089\n",
            " Iter:0 - Alpha:0.05 - Batch 24/249 - Min Loss:37.10 - Loss:37.17720974581725\n",
            " Iter:0 - Alpha:0.05 - Batch 25/249 - Min Loss:36.65 - Loss:36.655511668790254\n",
            " Iter:0 - Alpha:0.05 - Batch 26/249 - Min Loss:35.97 - Loss:35.97926856959905\n",
            " Iter:0 - Alpha:0.05 - Batch 27/249 - Min Loss:35.61 - Loss:35.614751188740904\n",
            " Iter:0 - Alpha:0.05 - Batch 28/249 - Min Loss:35.12 - Loss:35.12068600176885\n",
            " Iter:0 - Alpha:0.05 - Batch 29/249 - Min Loss:34.75 - Loss:34.756401858629374\n",
            " Iter:0 - Alpha:0.05 - Batch 30/249 - Min Loss:34.38 - Loss:34.38217397224482\n",
            " Iter:0 - Alpha:0.05 - Batch 31/249 - Min Loss:34.15 - Loss:34.15687092232486\n",
            " Iter:0 - Alpha:0.05 - Batch 32/249 - Min Loss:33.83 - Loss:33.83917491964368\n",
            " Iter:0 - Alpha:0.05 - Batch 33/249 - Min Loss:33.54 - Loss:33.54578750229636\n",
            " Iter:0 - Alpha:0.05 - Batch 34/249 - Min Loss:33.23 - Loss:33.23917319647738\n",
            " Iter:0 - Alpha:0.05 - Batch 35/249 - Min Loss:33.11 - Loss:33.11306188189393\n",
            " Iter:0 - Alpha:0.05 - Batch 36/249 - Min Loss:32.95 - Loss:32.9565190178945\n",
            " Iter:0 - Alpha:0.05 - Batch 37/249 - Min Loss:32.70 - Loss:32.703952406226485\n",
            " Iter:0 - Alpha:0.05 - Batch 38/249 - Min Loss:32.48 - Loss:32.487131347122826\n",
            " Iter:0 - Alpha:0.05 - Batch 39/249 - Min Loss:32.38 - Loss:32.3892572555698\n",
            " Iter:0 - Alpha:0.05 - Batch 40/249 - Min Loss:32.10 - Loss:32.10469888218284\n",
            " Iter:0 - Alpha:0.05 - Batch 41/249 - Min Loss:31.99 - Loss:31.995434414796595\n",
            " Iter:0 - Alpha:0.05 - Batch 42/249 - Min Loss:31.85 - Loss:31.850105585470008\n",
            " Iter:0 - Alpha:0.05 - Batch 43/249 - Min Loss:31.58 - Loss:31.581437320201267\n",
            " Iter:0 - Alpha:0.05 - Batch 49/249 - Min Loss:31.53 - Loss:31.576156241993214\n",
            " Iter:0 - Alpha:0.05 - Batch 50/249 - Min Loss:31.43 - Loss:31.43284857089971\n",
            " Iter:0 - Alpha:0.05 - Batch 51/249 - Min Loss:31.16 - Loss:31.166888288313938\n",
            " Iter:0 - Alpha:0.05 - Batch 52/249 - Min Loss:30.99 - Loss:30.998691778663854\n",
            " Iter:0 - Alpha:0.05 - Batch 53/249 - Min Loss:30.88 - Loss:30.883227710344947\n",
            " Iter:0 - Alpha:0.05 - Batch 54/249 - Min Loss:30.72 - Loss:30.724984991881747\n",
            " Iter:0 - Alpha:0.05 - Batch 55/249 - Min Loss:30.54 - Loss:30.540801487156518\n",
            " Iter:0 - Alpha:0.05 - Batch 56/249 - Min Loss:30.38 - Loss:30.38167548230328\n",
            " Iter:0 - Alpha:0.05 - Batch 57/249 - Min Loss:30.18 - Loss:30.18738075887357\n",
            " Iter:0 - Alpha:0.05 - Batch 58/249 - Min Loss:30.04 - Loss:30.04260221212051\n",
            " Iter:0 - Alpha:0.05 - Batch 59/249 - Min Loss:29.85 - Loss:29.859718838210533\n",
            " Iter:0 - Alpha:0.05 - Batch 60/249 - Min Loss:29.69 - Loss:29.697643690052878\n",
            " Iter:0 - Alpha:0.05 - Batch 61/249 - Min Loss:29.48 - Loss:29.487077121367076\n",
            " Iter:0 - Alpha:0.05 - Batch 62/249 - Min Loss:29.31 - Loss:29.317661985452627\n",
            " Iter:0 - Alpha:0.05 - Batch 63/249 - Min Loss:29.10 - Loss:29.102116794601788\n",
            " Iter:0 - Alpha:0.05 - Batch 64/249 - Min Loss:28.92 - Loss:28.92519301625896\n",
            " Iter:0 - Alpha:0.05 - Batch 65/249 - Min Loss:28.85 - Loss:28.854738760526967\n",
            " Iter:0 - Alpha:0.05 - Batch 66/249 - Min Loss:28.79 - Loss:28.796422913997525\n",
            " Iter:0 - Alpha:0.05 - Batch 67/249 - Min Loss:28.67 - Loss:28.673580536912016\n",
            " Iter:0 - Alpha:0.05 - Batch 68/249 - Min Loss:28.46 - Loss:28.469273688775967\n",
            " Iter:0 - Alpha:0.05 - Batch 69/249 - Min Loss:28.37 - Loss:28.37635597186601\n",
            " Iter:0 - Alpha:0.05 - Batch 70/249 - Min Loss:28.23 - Loss:28.234997819051564\n",
            " Iter:0 - Alpha:0.05 - Batch 71/249 - Min Loss:28.15 - Loss:28.157024162645815\n",
            " Iter:0 - Alpha:0.05 - Batch 72/249 - Min Loss:28.13 - Loss:28.1319118788434\n",
            " Iter:0 - Alpha:0.05 - Batch 73/249 - Min Loss:28.04 - Loss:28.046231689775265\n",
            " Iter:0 - Alpha:0.05 - Batch 74/249 - Min Loss:27.88 - Loss:27.882903140862176\n",
            " Iter:0 - Alpha:0.05 - Batch 75/249 - Min Loss:27.77 - Loss:27.771234207402514\n",
            " Iter:0 - Alpha:0.05 - Batch 76/249 - Min Loss:27.68 - Loss:27.6842590186696\n",
            " Iter:0 - Alpha:0.05 - Batch 77/249 - Min Loss:27.56 - Loss:27.561213154015462\n",
            " Iter:0 - Alpha:0.05 - Batch 78/249 - Min Loss:27.48 - Loss:27.48425001241434\n",
            " Iter:0 - Alpha:0.05 - Batch 79/249 - Min Loss:27.37 - Loss:27.376863085238828\n",
            " Iter:0 - Alpha:0.05 - Batch 80/249 - Min Loss:27.28 - Loss:27.28032735750375\n",
            " Iter:0 - Alpha:0.05 - Batch 81/249 - Min Loss:27.13 - Loss:27.13725100917658\n",
            " Iter:0 - Alpha:0.05 - Batch 82/249 - Min Loss:27.05 - Loss:27.057924472547636\n",
            " Iter:0 - Alpha:0.05 - Batch 83/249 - Min Loss:26.97 - Loss:26.97271846785529\n",
            " Iter:0 - Alpha:0.05 - Batch 86/249 - Min Loss:26.95 - Loss:26.976848946260414\n",
            " Iter:0 - Alpha:0.05 - Batch 87/249 - Min Loss:26.82 - Loss:26.82112088732075\n",
            " Iter:0 - Alpha:0.05 - Batch 88/249 - Min Loss:26.68 - Loss:26.68746822202644\n",
            " Iter:0 - Alpha:0.05 - Batch 89/249 - Min Loss:26.55 - Loss:26.55226667584506\n",
            " Iter:0 - Alpha:0.05 - Batch 90/249 - Min Loss:26.42 - Loss:26.429429564061913\n",
            " Iter:0 - Alpha:0.05 - Batch 91/249 - Min Loss:26.31 - Loss:26.31839626948311\n",
            " Iter:0 - Alpha:0.05 - Batch 92/249 - Min Loss:26.26 - Loss:26.26133673151034\n",
            " Iter:0 - Alpha:0.05 - Batch 93/249 - Min Loss:26.11 - Loss:26.114108576293738\n",
            " Iter:0 - Alpha:0.05 - Batch 94/249 - Min Loss:26.01 - Loss:26.011274538504733\n",
            " Iter:0 - Alpha:0.05 - Batch 95/249 - Min Loss:25.90 - Loss:25.90251528427718\n",
            " Iter:0 - Alpha:0.05 - Batch 96/249 - Min Loss:25.86 - Loss:25.868165988529636\n",
            " Iter:0 - Alpha:0.05 - Batch 97/249 - Min Loss:25.76 - Loss:25.765770677240965\n",
            " Iter:0 - Alpha:0.05 - Batch 98/249 - Min Loss:25.67 - Loss:25.670767174436097\n",
            " Iter:0 - Alpha:0.05 - Batch 99/249 - Min Loss:25.55 - Loss:25.552115616958837\n",
            " Iter:0 - Alpha:0.05 - Batch 100/249 - Min Loss:25.44 - Loss:25.445590523399726\n",
            " Iter:0 - Alpha:0.05 - Batch 101/249 - Min Loss:25.35 - Loss:25.352512459504045\n",
            " Iter:0 - Alpha:0.05 - Batch 102/249 - Min Loss:25.25 - Loss:25.252260308440345\n",
            " Iter:0 - Alpha:0.05 - Batch 103/249 - Min Loss:25.15 - Loss:25.152126768055638\n",
            " Iter:0 - Alpha:0.05 - Batch 104/249 - Min Loss:25.07 - Loss:25.07294712482244\n",
            " Iter:0 - Alpha:0.05 - Batch 105/249 - Min Loss:25.00 - Loss:25.002886129427857\n",
            " Iter:0 - Alpha:0.05 - Batch 106/249 - Min Loss:24.90 - Loss:24.904950544680208\n",
            " Iter:0 - Alpha:0.05 - Batch 107/249 - Min Loss:24.84 - Loss:24.84187443178292\n",
            " Iter:0 - Alpha:0.05 - Batch 108/249 - Min Loss:24.74 - Loss:24.74036287688322\n",
            " Iter:0 - Alpha:0.05 - Batch 109/249 - Min Loss:24.65 - Loss:24.653809294419354\n",
            " Iter:0 - Alpha:0.05 - Batch 110/249 - Min Loss:24.58 - Loss:24.582381589921074\n",
            " Iter:0 - Alpha:0.05 - Batch 111/249 - Min Loss:24.51 - Loss:24.515719488701468\n",
            " Iter:0 - Alpha:0.05 - Batch 112/249 - Min Loss:24.42 - Loss:24.428327845932554\n",
            " Iter:0 - Alpha:0.05 - Batch 113/249 - Min Loss:24.33 - Loss:24.333631880547454\n",
            " Iter:0 - Alpha:0.05 - Batch 114/249 - Min Loss:24.25 - Loss:24.258208905856208\n",
            " Iter:0 - Alpha:0.05 - Batch 115/249 - Min Loss:24.18 - Loss:24.18890226927859\n",
            " Iter:0 - Alpha:0.05 - Batch 116/249 - Min Loss:24.10 - Loss:24.106717546364337\n",
            " Iter:0 - Alpha:0.05 - Batch 117/249 - Min Loss:24.03 - Loss:24.039355420129446\n",
            " Iter:0 - Alpha:0.05 - Batch 118/249 - Min Loss:23.95 - Loss:23.952450872660858\n",
            " Iter:0 - Alpha:0.05 - Batch 119/249 - Min Loss:23.89 - Loss:23.898339419247936\n",
            " Iter:0 - Alpha:0.05 - Batch 120/249 - Min Loss:23.81 - Loss:23.814963305322117\n",
            " Iter:0 - Alpha:0.05 - Batch 121/249 - Min Loss:23.73 - Loss:23.73800473347888\n",
            " Iter:0 - Alpha:0.05 - Batch 122/249 - Min Loss:23.65 - Loss:23.650641227474683\n",
            " Iter:0 - Alpha:0.05 - Batch 123/249 - Min Loss:23.56 - Loss:23.562265506316788\n",
            " Iter:0 - Alpha:0.05 - Batch 124/249 - Min Loss:23.47 - Loss:23.478154661511002\n",
            " Iter:0 - Alpha:0.05 - Batch 125/249 - Min Loss:23.40 - Loss:23.409890165161556\n",
            " Iter:0 - Alpha:0.05 - Batch 126/249 - Min Loss:23.35 - Loss:23.352717977923994\n",
            " Iter:0 - Alpha:0.05 - Batch 127/249 - Min Loss:23.28 - Loss:23.281045965142276\n",
            " Iter:0 - Alpha:0.05 - Batch 128/249 - Min Loss:23.18 - Loss:23.186050334910192\n",
            " Iter:0 - Alpha:0.05 - Batch 129/249 - Min Loss:23.10 - Loss:23.103853989914143\n",
            " Iter:0 - Alpha:0.05 - Batch 130/249 - Min Loss:23.03 - Loss:23.03991921826136\n",
            " Iter:0 - Alpha:0.05 - Batch 131/249 - Min Loss:22.95 - Loss:22.955259208032043\n",
            " Iter:0 - Alpha:0.05 - Batch 132/249 - Min Loss:22.86 - Loss:22.867627377169626\n",
            " Iter:0 - Alpha:0.05 - Batch 133/249 - Min Loss:22.79 - Loss:22.790002743983234\n",
            " Iter:0 - Alpha:0.05 - Batch 134/249 - Min Loss:22.70 - Loss:22.708487090816703\n",
            " Iter:0 - Alpha:0.05 - Batch 136/249 - Min Loss:22.66 - Loss:22.67627254572151\n",
            " Iter:0 - Alpha:0.05 - Batch 137/249 - Min Loss:22.61 - Loss:22.611195755245177\n",
            " Iter:0 - Alpha:0.05 - Batch 138/249 - Min Loss:22.52 - Loss:22.523522151558268\n",
            " Iter:0 - Alpha:0.05 - Batch 140/249 - Min Loss:22.48 - Loss:22.501337391835772\n",
            " Iter:0 - Alpha:0.05 - Batch 141/249 - Min Loss:22.43 - Loss:22.431873187816255\n",
            " Iter:0 - Alpha:0.05 - Batch 142/249 - Min Loss:22.35 - Loss:22.351415003391324\n",
            " Iter:0 - Alpha:0.05 - Batch 143/249 - Min Loss:22.27 - Loss:22.277429461185825\n",
            " Iter:0 - Alpha:0.05 - Batch 144/249 - Min Loss:22.20 - Loss:22.20658740549601\n",
            " Iter:0 - Alpha:0.05 - Batch 146/249 - Min Loss:22.16 - Loss:22.196475495357483\n",
            " Iter:0 - Alpha:0.05 - Batch 147/249 - Min Loss:22.15 - Loss:22.151875688836512\n",
            " Iter:0 - Alpha:0.05 - Batch 148/249 - Min Loss:22.08 - Loss:22.087513119417693\n",
            " Iter:0 - Alpha:0.05 - Batch 149/249 - Min Loss:22.01 - Loss:22.017354420293348\n",
            " Iter:0 - Alpha:0.05 - Batch 150/249 - Min Loss:21.95 - Loss:21.95603542151571\n",
            " Iter:0 - Alpha:0.05 - Batch 151/249 - Min Loss:21.91 - Loss:21.917924326487388\n",
            " Iter:0 - Alpha:0.05 - Batch 152/249 - Min Loss:21.86 - Loss:21.866971333194055\n",
            " Iter:0 - Alpha:0.05 - Batch 153/249 - Min Loss:21.79 - Loss:21.792296839504843\n",
            " Iter:0 - Alpha:0.05 - Batch 154/249 - Min Loss:21.73 - Loss:21.73876423922069\n",
            " Iter:0 - Alpha:0.05 - Batch 155/249 - Min Loss:21.70 - Loss:21.70364584869217\n",
            " Iter:0 - Alpha:0.05 - Batch 156/249 - Min Loss:21.68 - Loss:21.68383628986473\n",
            " Iter:0 - Alpha:0.05 - Batch 157/249 - Min Loss:21.64 - Loss:21.640796707208718\n",
            " Iter:0 - Alpha:0.05 - Batch 158/249 - Min Loss:21.58 - Loss:21.588008599342384\n",
            " Iter:0 - Alpha:0.05 - Batch 159/249 - Min Loss:21.55 - Loss:21.552047716363198\n",
            " Iter:0 - Alpha:0.05 - Batch 160/249 - Min Loss:21.49 - Loss:21.496984007194133\n",
            " Iter:0 - Alpha:0.05 - Batch 161/249 - Min Loss:21.43 - Loss:21.43670693165273\n",
            " Iter:0 - Alpha:0.05 - Batch 162/249 - Min Loss:21.38 - Loss:21.385289127808722\n",
            " Iter:0 - Alpha:0.05 - Batch 163/249 - Min Loss:21.33 - Loss:21.331477331316115\n",
            " Iter:0 - Alpha:0.05 - Batch 164/249 - Min Loss:21.28 - Loss:21.286210318233948\n",
            " Iter:0 - Alpha:0.05 - Batch 165/249 - Min Loss:21.24 - Loss:21.242626293013938\n",
            " Iter:0 - Alpha:0.05 - Batch 166/249 - Min Loss:21.19 - Loss:21.195390598378687\n",
            " Iter:0 - Alpha:0.05 - Batch 167/249 - Min Loss:21.16 - Loss:21.165702904501373\n",
            " Iter:0 - Alpha:0.05 - Batch 168/249 - Min Loss:21.12 - Loss:21.12843732570778\n",
            " Iter:0 - Alpha:0.05 - Batch 169/249 - Min Loss:21.07 - Loss:21.076418095942092\n",
            " Iter:0 - Alpha:0.05 - Batch 170/249 - Min Loss:21.03 - Loss:21.037454755797235\n",
            " Iter:0 - Alpha:0.05 - Batch 171/249 - Min Loss:21.00 - Loss:21.004372648394206\n",
            " Iter:0 - Alpha:0.05 - Batch 172/249 - Min Loss:20.97 - Loss:20.978386049829684\n",
            " Iter:0 - Alpha:0.05 - Batch 173/249 - Min Loss:20.93 - Loss:20.93893895081425\n",
            " Iter:0 - Alpha:0.05 - Batch 174/249 - Min Loss:20.90 - Loss:20.90216704631186\n",
            " Iter:0 - Alpha:0.05 - Batch 175/249 - Min Loss:20.87 - Loss:20.870200636923865\n",
            " Iter:0 - Alpha:0.05 - Batch 176/249 - Min Loss:20.83 - Loss:20.831110964429943\n",
            " Iter:0 - Alpha:0.05 - Batch 177/249 - Min Loss:20.78 - Loss:20.789089300955563\n",
            " Iter:0 - Alpha:0.05 - Batch 178/249 - Min Loss:20.74 - Loss:20.742713231017284\n",
            " Iter:0 - Alpha:0.05 - Batch 179/249 - Min Loss:20.72 - Loss:20.723006706736662\n",
            " Iter:0 - Alpha:0.05 - Batch 180/249 - Min Loss:20.68 - Loss:20.68102095158839\n",
            " Iter:0 - Alpha:0.05 - Batch 181/249 - Min Loss:20.63 - Loss:20.638060861719183\n",
            " Iter:0 - Alpha:0.05 - Batch 182/249 - Min Loss:20.59 - Loss:20.59470399109266\n",
            " Iter:0 - Alpha:0.05 - Batch 183/249 - Min Loss:20.54 - Loss:20.54611537628786\n",
            " Iter:0 - Alpha:0.05 - Batch 184/249 - Min Loss:20.51 - Loss:20.51241932893738\n",
            " Iter:0 - Alpha:0.05 - Batch 185/249 - Min Loss:20.47 - Loss:20.478051582486437\n",
            " Iter:0 - Alpha:0.05 - Batch 186/249 - Min Loss:20.46 - Loss:20.463373830017265\n",
            " Iter:0 - Alpha:0.05 - Batch 187/249 - Min Loss:20.43 - Loss:20.43241322879335\n",
            " Iter:0 - Alpha:0.05 - Batch 188/249 - Min Loss:20.41 - Loss:20.415489639674075\n",
            " Iter:0 - Alpha:0.05 - Batch 189/249 - Min Loss:20.38 - Loss:20.382415841757297\n",
            " Iter:0 - Alpha:0.05 - Batch 190/249 - Min Loss:20.35 - Loss:20.35636952438826\n",
            " Iter:0 - Alpha:0.05 - Batch 191/249 - Min Loss:20.32 - Loss:20.323261107001173\n",
            " Iter:0 - Alpha:0.05 - Batch 192/249 - Min Loss:20.29 - Loss:20.29493752201367\n",
            " Iter:0 - Alpha:0.05 - Batch 193/249 - Min Loss:20.23 - Loss:20.231706936402226\n",
            " Iter:0 - Alpha:0.05 - Batch 194/249 - Min Loss:20.21 - Loss:20.21432965474384\n",
            " Iter:0 - Alpha:0.05 - Batch 195/249 - Min Loss:20.21 - Loss:20.210144050121418\n",
            " Iter:0 - Alpha:0.05 - Batch 196/249 - Min Loss:20.16 - Loss:20.168569648346853\n",
            " Iter:0 - Alpha:0.05 - Batch 197/249 - Min Loss:20.14 - Loss:20.142011333499262\n",
            " Iter:0 - Alpha:0.05 - Batch 198/249 - Min Loss:20.12 - Loss:20.125140346198748\n",
            " Iter:0 - Alpha:0.05 - Batch 199/249 - Min Loss:20.08 - Loss:20.087380807123058\n",
            " Iter:0 - Alpha:0.05 - Batch 200/249 - Min Loss:20.04 - Loss:20.04318704221147\n",
            " Iter:0 - Alpha:0.05 - Batch 201/249 - Min Loss:20.01 - Loss:20.016876269715105\n",
            " Iter:0 - Alpha:0.05 - Batch 202/249 - Min Loss:19.99 - Loss:19.993609763489637\n",
            " Iter:0 - Alpha:0.05 - Batch 203/249 - Min Loss:19.95 - Loss:19.95958895363022\n",
            " Iter:0 - Alpha:0.05 - Batch 204/249 - Min Loss:19.92 - Loss:19.926210041225872\n",
            " Iter:0 - Alpha:0.05 - Batch 205/249 - Min Loss:19.89 - Loss:19.897226008405745\n",
            " Iter:0 - Alpha:0.05 - Batch 206/249 - Min Loss:19.85 - Loss:19.858399448651905\n",
            " Iter:0 - Alpha:0.05 - Batch 207/249 - Min Loss:19.82 - Loss:19.825156772682394\n",
            " Iter:0 - Alpha:0.05 - Batch 208/249 - Min Loss:19.79 - Loss:19.791423468702167\n",
            " Iter:0 - Alpha:0.05 - Batch 209/249 - Min Loss:19.74 - Loss:19.748426133139297\n",
            " Iter:0 - Alpha:0.05 - Batch 210/249 - Min Loss:19.70 - Loss:19.70684081157403\n",
            " Iter:0 - Alpha:0.05 - Batch 211/249 - Min Loss:19.65 - Loss:19.656149451479525\n",
            " Iter:0 - Alpha:0.05 - Batch 212/249 - Min Loss:19.61 - Loss:19.615069526880045\n",
            " Iter:0 - Alpha:0.05 - Batch 213/249 - Min Loss:19.58 - Loss:19.582103310840218\n",
            " Iter:0 - Alpha:0.05 - Batch 214/249 - Min Loss:19.55 - Loss:19.551917829443145\n",
            " Iter:0 - Alpha:0.05 - Batch 215/249 - Min Loss:19.51 - Loss:19.510461166431092\n",
            " Iter:0 - Alpha:0.05 - Batch 216/249 - Min Loss:19.47 - Loss:19.475011337024338\n",
            " Iter:0 - Alpha:0.05 - Batch 217/249 - Min Loss:19.45 - Loss:19.451335316570166\n",
            " Iter:0 - Alpha:0.05 - Batch 218/249 - Min Loss:19.41 - Loss:19.41142036973254\n",
            " Iter:0 - Alpha:0.05 - Batch 219/249 - Min Loss:19.36 - Loss:19.36146522484553\n",
            " Iter:0 - Alpha:0.05 - Batch 220/249 - Min Loss:19.33 - Loss:19.33861295418467\n",
            " Iter:0 - Alpha:0.05 - Batch 221/249 - Min Loss:19.33 - Loss:19.332413479101064\n",
            " Iter:0 - Alpha:0.05 - Batch 222/249 - Min Loss:19.30 - Loss:19.30646915323804\n",
            " Iter:0 - Alpha:0.05 - Batch 223/249 - Min Loss:19.27 - Loss:19.27674800443457\n",
            " Iter:0 - Alpha:0.05 - Batch 224/249 - Min Loss:19.25 - Loss:19.251562163392435\n",
            " Iter:0 - Alpha:0.05 - Batch 225/249 - Min Loss:19.22 - Loss:19.222437561099124\n",
            " Iter:0 - Alpha:0.05 - Batch 226/249 - Min Loss:19.18 - Loss:19.18836632229978\n",
            " Iter:0 - Alpha:0.05 - Batch 227/249 - Min Loss:19.16 - Loss:19.162084970086052\n",
            " Iter:0 - Alpha:0.05 - Batch 228/249 - Min Loss:19.14 - Loss:19.149911547882425\n",
            " Iter:0 - Alpha:0.05 - Batch 229/249 - Min Loss:19.12 - Loss:19.1219394343792\n",
            " Iter:0 - Alpha:0.05 - Batch 230/249 - Min Loss:19.09 - Loss:19.096148635990602\n",
            " Iter:0 - Alpha:0.05 - Batch 231/249 - Min Loss:19.08 - Loss:19.081140852678754\n",
            " Iter:0 - Alpha:0.05 - Batch 232/249 - Min Loss:19.05 - Loss:19.053406614162057\n",
            " Iter:0 - Alpha:0.05 - Batch 233/249 - Min Loss:19.03 - Loss:19.03693768081677\n",
            " Iter:0 - Alpha:0.05 - Batch 234/249 - Min Loss:19.00 - Loss:19.009419641222927\n",
            " Iter:0 - Alpha:0.05 - Batch 235/249 - Min Loss:18.97 - Loss:18.97683561933004\n",
            " Iter:0 - Alpha:0.05 - Batch 236/249 - Min Loss:18.94 - Loss:18.940971848831524\n",
            " Iter:0 - Alpha:0.05 - Batch 237/249 - Min Loss:18.91 - Loss:18.9180874535922\n",
            " Iter:0 - Alpha:0.05 - Batch 238/249 - Min Loss:18.90 - Loss:18.901568132487206\n",
            " Iter:0 - Alpha:0.05 - Batch 239/249 - Min Loss:18.88 - Loss:18.885853616967232\n",
            " Iter:0 - Alpha:0.05 - Batch 240/249 - Min Loss:18.87 - Loss:18.87138521302166\n",
            " Iter:0 - Alpha:0.05 - Batch 241/249 - Min Loss:18.83 - Loss:18.835558989911696\n",
            " Iter:0 - Alpha:0.05 - Batch 242/249 - Min Loss:18.80 - Loss:18.807088870947275\n",
            " Iter:0 - Alpha:0.05 - Batch 244/249 - Min Loss:18.77 - Loss:18.783188527854637\n",
            " Iter:0 - Alpha:0.05 - Batch 245/249 - Min Loss:18.76 - Loss:18.764469185299056\n",
            " Iter:0 - Alpha:0.05 - Batch 246/249 - Min Loss:18.74 - Loss:18.74507008499201\n",
            " Iter:0 - Alpha:0.05 - Batch 247/249 - Min Loss:18.72 - Loss:18.72202504887896\n",
            " Iter:0 - Alpha:0.05 - Batch 248/249 - Min Loss:18.69 - Loss:18.695901177432972\n",
            " Iter:0 - Alpha:0.05 - Batch 249/249 - Min Loss:18.67 - Loss:18.672448412749706\n",
            " Iter:1 - Alpha:0.049 - Batch 1/249 - Min Loss:13.02 - Loss:13.025888771112934 - her the the the the the the the the the the the the the the the the th\n",
            " Iter:1 - Alpha:0.049 - Batch 3/249 - Min Loss:12.86 - Loss:12.911852216643139\n",
            " Iter:1 - Alpha:0.049 - Batch 4/249 - Min Loss:12.77 - Loss:12.779643901134486\n",
            " Iter:2 - Alpha:0.049 - Batch 4/249 - Min Loss:12.69 - Loss:12.861306794576533\n",
            " Iter:2 - Alpha:0.049 - Batch 100/249 - Min Loss:12.54 - Loss:12.553837414241563\n",
            " Iter:2 - Alpha:0.049 - Batch 202/249 - Min Loss:12.54 - Loss:12.54634211352977\n",
            " Iter:2 - Alpha:0.049 - Batch 203/249 - Min Loss:12.54 - Loss:12.545861505157209\n",
            " Iter:2 - Alpha:0.049 - Batch 204/249 - Min Loss:12.53 - Loss:12.539544475109949\n",
            " Iter:2 - Alpha:0.049 - Batch 205/249 - Min Loss:12.53 - Loss:12.538158626236882\n",
            " Iter:2 - Alpha:0.049 - Batch 206/249 - Min Loss:12.53 - Loss:12.531453875874227\n",
            " Iter:2 - Alpha:0.049 - Batch 207/249 - Min Loss:12.52 - Loss:12.522510778738871\n",
            " Iter:2 - Alpha:0.049 - Batch 208/249 - Min Loss:12.51 - Loss:12.5158004289451\n",
            " Iter:2 - Alpha:0.049 - Batch 209/249 - Min Loss:12.50 - Loss:12.50658976071825\n",
            " Iter:2 - Alpha:0.049 - Batch 210/249 - Min Loss:12.49 - Loss:12.497212397307786\n",
            " Iter:2 - Alpha:0.049 - Batch 211/249 - Min Loss:12.48 - Loss:12.4801470360593\n",
            " Iter:2 - Alpha:0.049 - Batch 212/249 - Min Loss:12.47 - Loss:12.472410142769563\n",
            " Iter:2 - Alpha:0.049 - Batch 213/249 - Min Loss:12.46 - Loss:12.466821255993375\n",
            " Iter:2 - Alpha:0.049 - Batch 214/249 - Min Loss:12.46 - Loss:12.46250302693015\n",
            " Iter:2 - Alpha:0.049 - Batch 215/249 - Min Loss:12.45 - Loss:12.450562994682569\n",
            " Iter:2 - Alpha:0.049 - Batch 217/249 - Min Loss:12.44 - Loss:12.44712729858359\n",
            " Iter:2 - Alpha:0.049 - Batch 218/249 - Min Loss:12.44 - Loss:12.440302120344606\n",
            " Iter:2 - Alpha:0.049 - Batch 219/249 - Min Loss:12.42 - Loss:12.425904198404831\n",
            " Iter:2 - Alpha:0.049 - Batch 249/249 - Min Loss:12.42 - Loss:12.473358614379077\n",
            " Iter:3 - Alpha:0.048 - Batch 1/249 - Min Loss:11.96 - Loss:11.96816271151561 - hend theres, and theres, and theres, and theres, and theres, and there\n",
            " Iter:3 - Alpha:0.048 - Batch 214/249 - Min Loss:11.94 - Loss:11.946587600476539\n",
            " Iter:3 - Alpha:0.048 - Batch 215/249 - Min Loss:11.93 - Loss:11.933923047280956\n",
            " Iter:3 - Alpha:0.048 - Batch 217/249 - Min Loss:11.92 - Loss:11.926656502973673\n",
            " Iter:3 - Alpha:0.048 - Batch 218/249 - Min Loss:11.91 - Loss:11.917809154684457\n",
            " Iter:3 - Alpha:0.048 - Batch 223/249 - Min Loss:11.90 - Loss:11.907968459614233\n",
            " Iter:3 - Alpha:0.048 - Batch 224/249 - Min Loss:11.90 - Loss:11.902355400459102\n",
            " Iter:3 - Alpha:0.048 - Batch 225/249 - Min Loss:11.89 - Loss:11.898399616347808\n",
            " Iter:3 - Alpha:0.048 - Batch 236/249 - Min Loss:11.89 - Loss:11.896736737502133\n",
            " Iter:4 - Alpha:0.048 - Batch 4/249 - Min Loss:11.89 - Loss:11.93749047561051\n",
            " Iter:4 - Alpha:0.048 - Batch 25/249 - Min Loss:11.67 - Loss:11.710709419035581\n",
            " Iter:4 - Alpha:0.048 - Batch 31/249 - Min Loss:11.60 - Loss:11.655038473490318\n",
            " Iter:4 - Alpha:0.048 - Batch 33/249 - Min Loss:11.60 - Loss:11.622026031347389\n",
            " Iter:4 - Alpha:0.048 - Batch 34/249 - Min Loss:11.58 - Loss:11.584410536097456\n",
            " Iter:4 - Alpha:0.048 - Batch 55/249 - Min Loss:11.56 - Loss:11.59348565034329\n",
            " Iter:4 - Alpha:0.048 - Batch 56/249 - Min Loss:11.55 - Loss:11.553452091216283\n",
            " Iter:4 - Alpha:0.048 - Batch 57/249 - Min Loss:11.52 - Loss:11.523624213684682\n",
            " Iter:4 - Alpha:0.048 - Batch 100/249 - Min Loss:11.51 - Loss:11.52019683227539\n",
            " Iter:4 - Alpha:0.048 - Batch 101/249 - Min Loss:11.50 - Loss:11.509577065372184\n",
            " Iter:4 - Alpha:0.048 - Batch 102/249 - Min Loss:11.49 - Loss:11.491519216923768\n",
            " Iter:4 - Alpha:0.048 - Batch 103/249 - Min Loss:11.48 - Loss:11.484576051174493\n",
            " Iter:4 - Alpha:0.048 - Batch 104/249 - Min Loss:11.48 - Loss:11.482251821619048\n",
            " Iter:4 - Alpha:0.048 - Batch 105/249 - Min Loss:11.47 - Loss:11.477400526998704\n",
            " Iter:4 - Alpha:0.048 - Batch 107/249 - Min Loss:11.46 - Loss:11.46564343952615\n",
            " Iter:4 - Alpha:0.048 - Batch 129/249 - Min Loss:11.45 - Loss:11.460378138592388\n",
            " Iter:4 - Alpha:0.048 - Batch 130/249 - Min Loss:11.45 - Loss:11.45460877668019\n",
            " Iter:4 - Alpha:0.048 - Batch 131/249 - Min Loss:11.43 - Loss:11.436848495211077\n",
            " Iter:4 - Alpha:0.048 - Batch 135/249 - Min Loss:11.42 - Loss:11.433507242920555\n",
            " Iter:4 - Alpha:0.048 - Batch 136/249 - Min Loss:11.41 - Loss:11.418981297647917\n",
            " Iter:4 - Alpha:0.048 - Batch 137/249 - Min Loss:11.41 - Loss:11.416048267430309\n",
            " Iter:4 - Alpha:0.048 - Batch 138/249 - Min Loss:11.39 - Loss:11.398926954900745\n",
            " Iter:4 - Alpha:0.048 - Batch 143/249 - Min Loss:11.39 - Loss:11.397617546563055\n",
            " Iter:4 - Alpha:0.048 - Batch 152/249 - Min Loss:11.39 - Loss:11.396587922054346\n",
            " Iter:4 - Alpha:0.048 - Batch 153/249 - Min Loss:11.38 - Loss:11.386413672865634\n",
            " Iter:4 - Alpha:0.048 - Batch 213/249 - Min Loss:11.38 - Loss:11.382570894809966\n",
            " Iter:4 - Alpha:0.048 - Batch 214/249 - Min Loss:11.37 - Loss:11.379202437332493\n",
            " Iter:4 - Alpha:0.048 - Batch 215/249 - Min Loss:11.36 - Loss:11.368017535797224\n",
            " Iter:4 - Alpha:0.048 - Batch 217/249 - Min Loss:11.35 - Loss:11.36308130222091\n",
            " Iter:4 - Alpha:0.048 - Batch 218/249 - Min Loss:11.35 - Loss:11.355348390014566\n",
            " Iter:4 - Alpha:0.048 - Batch 222/249 - Min Loss:11.34 - Loss:11.345817880695664\n",
            " Iter:4 - Alpha:0.048 - Batch 223/249 - Min Loss:11.34 - Loss:11.340187511215117\n",
            " Iter:4 - Alpha:0.048 - Batch 224/249 - Min Loss:11.33 - Loss:11.334937275117644\n",
            " Iter:4 - Alpha:0.048 - Batch 225/249 - Min Loss:11.33 - Loss:11.331100336921256\n",
            " Iter:4 - Alpha:0.048 - Batch 226/249 - Min Loss:11.32 - Loss:11.328961747495915\n",
            " Iter:4 - Alpha:0.048 - Batch 227/249 - Min Loss:11.32 - Loss:11.324704336305567\n",
            " Iter:4 - Alpha:0.048 - Batch 234/249 - Min Loss:11.32 - Loss:11.3277883008984\n",
            " Iter:4 - Alpha:0.048 - Batch 235/249 - Min Loss:11.32 - Loss:11.320809933553308\n",
            " Iter:4 - Alpha:0.048 - Batch 236/249 - Min Loss:11.31 - Loss:11.310224997032277\n",
            " Iter:4 - Alpha:0.048 - Batch 237/249 - Min Loss:11.30 - Loss:11.301638755334423\n",
            " Iter:4 - Alpha:0.048 - Batch 240/249 - Min Loss:11.29 - Loss:11.30339324171569\n",
            " Iter:4 - Alpha:0.048 - Batch 242/249 - Min Loss:11.29 - Loss:11.296226493349897\n",
            " Iter:5 - Alpha:0.047 - Batch 34/249 - Min Loss:11.29 - Loss:11.295798727310434\n",
            " Iter:5 - Alpha:0.047 - Batch 37/249 - Min Loss:11.28 - Loss:11.313966493696062\n",
            " Iter:5 - Alpha:0.047 - Batch 55/249 - Min Loss:11.28 - Loss:11.305744614606017\n",
            " Iter:5 - Alpha:0.047 - Batch 56/249 - Min Loss:11.26 - Loss:11.269090320237261\n",
            " Iter:5 - Alpha:0.047 - Batch 89/249 - Min Loss:11.24 - Loss:11.246903466540104\n",
            " Iter:5 - Alpha:0.047 - Batch 90/249 - Min Loss:11.23 - Loss:11.23040034411463\n",
            " Iter:5 - Alpha:0.047 - Batch 92/249 - Min Loss:11.21 - Loss:11.217792719376169\n",
            " Iter:5 - Alpha:0.047 - Batch 93/249 - Min Loss:11.18 - Loss:11.187492874212705\n",
            " Iter:5 - Alpha:0.047 - Batch 94/249 - Min Loss:11.17 - Loss:11.170589772552834\n",
            " Iter:5 - Alpha:0.047 - Batch 96/249 - Min Loss:11.15 - Loss:11.163111277550282\n",
            " Iter:5 - Alpha:0.047 - Batch 97/249 - Min Loss:11.15 - Loss:11.152788357987951\n",
            " Iter:5 - Alpha:0.047 - Batch 100/249 - Min Loss:11.14 - Loss:11.145558327649022\n",
            " Iter:5 - Alpha:0.047 - Batch 101/249 - Min Loss:11.13 - Loss:11.136507909341137\n",
            " Iter:5 - Alpha:0.047 - Batch 102/249 - Min Loss:11.11 - Loss:11.118644315584339\n",
            " Iter:5 - Alpha:0.047 - Batch 103/249 - Min Loss:11.11 - Loss:11.11005050903458\n",
            " Iter:5 - Alpha:0.047 - Batch 104/249 - Min Loss:11.10 - Loss:11.107330553053169\n",
            " Iter:5 - Alpha:0.047 - Batch 105/249 - Min Loss:11.10 - Loss:11.1055347009597\n",
            " Iter:5 - Alpha:0.047 - Batch 130/249 - Min Loss:11.09 - Loss:11.094912966915382\n",
            " Iter:5 - Alpha:0.047 - Batch 131/249 - Min Loss:11.07 - Loss:11.077304628994057\n",
            " Iter:5 - Alpha:0.047 - Batch 133/249 - Min Loss:11.06 - Loss:11.064413958151633\n",
            " Iter:5 - Alpha:0.047 - Batch 135/249 - Min Loss:11.05 - Loss:11.060112309362838\n",
            " Iter:5 - Alpha:0.047 - Batch 137/249 - Min Loss:11.04 - Loss:11.049502596095175\n",
            " Iter:5 - Alpha:0.047 - Batch 210/249 - Min Loss:11.03 - Loss:11.051644872804914\n",
            " Iter:5 - Alpha:0.047 - Batch 211/249 - Min Loss:11.03 - Loss:11.032906195670478\n",
            " Iter:5 - Alpha:0.047 - Batch 212/249 - Min Loss:11.02 - Loss:11.026525921686591\n",
            " Iter:5 - Alpha:0.047 - Batch 213/249 - Min Loss:11.01 - Loss:11.01957383583561\n",
            " Iter:5 - Alpha:0.047 - Batch 214/249 - Min Loss:11.01 - Loss:11.016464027923316\n",
            " Iter:5 - Alpha:0.047 - Batch 215/249 - Min Loss:11.00 - Loss:11.005740796702076\n",
            " Iter:5 - Alpha:0.047 - Batch 217/249 - Min Loss:10.99 - Loss:11.00009338291413\n",
            " Iter:5 - Alpha:0.047 - Batch 218/249 - Min Loss:10.99 - Loss:10.993414595406108\n",
            " Iter:5 - Alpha:0.047 - Batch 221/249 - Min Loss:10.98 - Loss:10.987128388397407\n",
            " Iter:5 - Alpha:0.047 - Batch 222/249 - Min Loss:10.98 - Loss:10.981680386549076\n",
            " Iter:5 - Alpha:0.047 - Batch 223/249 - Min Loss:10.97 - Loss:10.976160001229232\n",
            " Iter:5 - Alpha:0.047 - Batch 224/249 - Min Loss:10.97 - Loss:10.97123466134001\n",
            " Iter:5 - Alpha:0.047 - Batch 225/249 - Min Loss:10.96 - Loss:10.967957731498261\n",
            " Iter:5 - Alpha:0.047 - Batch 226/249 - Min Loss:10.96 - Loss:10.966287548690959\n",
            " Iter:5 - Alpha:0.047 - Batch 227/249 - Min Loss:10.95 - Loss:10.959771906206303\n",
            " Iter:5 - Alpha:0.047 - Batch 228/249 - Min Loss:10.95 - Loss:10.956842427154987\n",
            " Iter:5 - Alpha:0.047 - Batch 230/249 - Min Loss:10.95 - Loss:10.957712924458365\n",
            " Iter:5 - Alpha:0.047 - Batch 233/249 - Min Loss:10.95 - Loss:10.959567302341233\n",
            " Iter:5 - Alpha:0.047 - Batch 234/249 - Min Loss:10.95 - Loss:10.951658953912649\n",
            " Iter:5 - Alpha:0.047 - Batch 235/249 - Min Loss:10.94 - Loss:10.943100387617525\n",
            " Iter:5 - Alpha:0.047 - Batch 236/249 - Min Loss:10.93 - Loss:10.933564626378628\n",
            " Iter:5 - Alpha:0.047 - Batch 237/249 - Min Loss:10.92 - Loss:10.926709504993909\n",
            " Iter:6 - Alpha:0.047 - Batch 25/249 - Min Loss:10.92 - Loss:10.999370163894985\n",
            " Iter:6 - Alpha:0.047 - Batch 26/249 - Min Loss:10.88 - Loss:10.881101028873612\n",
            " Iter:6 - Alpha:0.047 - Batch 37/249 - Min Loss:10.87 - Loss:10.890244017637709\n",
            " Iter:6 - Alpha:0.047 - Batch 52/249 - Min Loss:10.86 - Loss:10.871406055131486\n",
            " Iter:6 - Alpha:0.047 - Batch 53/249 - Min Loss:10.85 - Loss:10.850734227239277\n",
            " Iter:6 - Alpha:0.047 - Batch 54/249 - Min Loss:10.84 - Loss:10.844342750653146\n",
            " Iter:6 - Alpha:0.047 - Batch 55/249 - Min Loss:10.82 - Loss:10.82964928153141\n",
            " Iter:6 - Alpha:0.047 - Batch 56/249 - Min Loss:10.80 - Loss:10.806095326510157\n",
            " Iter:6 - Alpha:0.047 - Batch 100/249 - Min Loss:10.78 - Loss:10.794403402442638\n",
            " Iter:6 - Alpha:0.047 - Batch 101/249 - Min Loss:10.78 - Loss:10.782537072245798\n",
            " Iter:6 - Alpha:0.047 - Batch 102/249 - Min Loss:10.76 - Loss:10.76771739344167\n",
            " Iter:6 - Alpha:0.047 - Batch 103/249 - Min Loss:10.76 - Loss:10.761370722904894\n",
            " Iter:6 - Alpha:0.047 - Batch 105/249 - Min Loss:10.75 - Loss:10.75871675762743\n",
            " Iter:6 - Alpha:0.047 - Batch 137/249 - Min Loss:10.74 - Loss:10.754060964622065\n",
            " Iter:6 - Alpha:0.047 - Batch 153/249 - Min Loss:10.74 - Loss:10.74831765925302\n",
            " Iter:6 - Alpha:0.047 - Batch 208/249 - Min Loss:10.74 - Loss:10.74544211428577\n",
            " Iter:6 - Alpha:0.047 - Batch 209/249 - Min Loss:10.73 - Loss:10.738573646258345\n",
            " Iter:6 - Alpha:0.047 - Batch 210/249 - Min Loss:10.73 - Loss:10.731406122926327\n",
            " Iter:6 - Alpha:0.047 - Batch 211/249 - Min Loss:10.71 - Loss:10.712558738937219\n",
            " Iter:6 - Alpha:0.047 - Batch 212/249 - Min Loss:10.70 - Loss:10.706112678395735\n",
            " Iter:6 - Alpha:0.047 - Batch 213/249 - Min Loss:10.69 - Loss:10.69988882680964\n",
            " Iter:6 - Alpha:0.047 - Batch 214/249 - Min Loss:10.69 - Loss:10.697287899706241\n",
            " Iter:6 - Alpha:0.047 - Batch 215/249 - Min Loss:10.68 - Loss:10.687186208038334\n",
            " Iter:6 - Alpha:0.047 - Batch 216/249 - Min Loss:10.68 - Loss:10.680354021789618\n",
            " Iter:6 - Alpha:0.047 - Batch 217/249 - Min Loss:10.68 - Loss:10.680158837792247\n",
            " Iter:6 - Alpha:0.047 - Batch 218/249 - Min Loss:10.67 - Loss:10.673939824307038\n",
            " Iter:6 - Alpha:0.047 - Batch 219/249 - Min Loss:10.66 - Loss:10.66218354004966\n",
            " Iter:6 - Alpha:0.047 - Batch 221/249 - Min Loss:10.66 - Loss:10.665572687123563\n",
            " Iter:6 - Alpha:0.047 - Batch 222/249 - Min Loss:10.66 - Loss:10.660573817774315\n",
            " Iter:6 - Alpha:0.047 - Batch 223/249 - Min Loss:10.65 - Loss:10.655508118863366\n",
            " Iter:6 - Alpha:0.047 - Batch 224/249 - Min Loss:10.64 - Loss:10.649569776815541\n",
            " Iter:6 - Alpha:0.047 - Batch 225/249 - Min Loss:10.64 - Loss:10.644235759358205\n",
            " Iter:6 - Alpha:0.047 - Batch 226/249 - Min Loss:10.64 - Loss:10.641910683697617\n",
            " Iter:6 - Alpha:0.047 - Batch 227/249 - Min Loss:10.63 - Loss:10.63576652566022\n",
            " Iter:6 - Alpha:0.047 - Batch 228/249 - Min Loss:10.63 - Loss:10.633937947917275\n",
            " Iter:6 - Alpha:0.047 - Batch 230/249 - Min Loss:10.63 - Loss:10.632851510791031\n",
            " Iter:6 - Alpha:0.047 - Batch 233/249 - Min Loss:10.63 - Loss:10.634243531165238\n",
            " Iter:6 - Alpha:0.047 - Batch 234/249 - Min Loss:10.62 - Loss:10.625880982982418\n",
            " Iter:6 - Alpha:0.047 - Batch 235/249 - Min Loss:10.61 - Loss:10.618171866373872\n",
            " Iter:6 - Alpha:0.047 - Batch 236/249 - Min Loss:10.61 - Loss:10.614117383481885\n",
            " Iter:7 - Alpha:0.046 - Batch 4/249 - Min Loss:10.61 - Loss:10.639774523877929\n",
            " Iter:7 - Alpha:0.046 - Batch 25/249 - Min Loss:10.56 - Loss:10.63215443709742\n",
            " Iter:7 - Alpha:0.046 - Batch 50/249 - Min Loss:10.52 - Loss:10.54478921940455\n",
            " Iter:7 - Alpha:0.046 - Batch 51/249 - Min Loss:10.50 - Loss:10.506901952815788\n",
            " Iter:7 - Alpha:0.046 - Batch 52/249 - Min Loss:10.50 - Loss:10.505238917069232\n",
            " Iter:7 - Alpha:0.046 - Batch 53/249 - Min Loss:10.47 - Loss:10.479046803999461\n",
            " Iter:7 - Alpha:0.046 - Batch 54/249 - Min Loss:10.46 - Loss:10.465003558444366\n",
            " Iter:7 - Alpha:0.046 - Batch 55/249 - Min Loss:10.45 - Loss:10.451869444117891\n",
            " Iter:7 - Alpha:0.046 - Batch 56/249 - Min Loss:10.42 - Loss:10.424349791185957\n",
            " Iter:7 - Alpha:0.046 - Batch 217/249 - Min Loss:10.40 - Loss:10.405030121295288\n",
            " Iter:7 - Alpha:0.046 - Batch 218/249 - Min Loss:10.39 - Loss:10.399731309757378\n",
            " Iter:7 - Alpha:0.046 - Batch 221/249 - Min Loss:10.38 - Loss:10.392509114339108\n",
            " Iter:7 - Alpha:0.046 - Batch 222/249 - Min Loss:10.38 - Loss:10.388150791460712\n",
            " Iter:7 - Alpha:0.046 - Batch 223/249 - Min Loss:10.38 - Loss:10.382787251615675\n",
            " Iter:7 - Alpha:0.046 - Batch 224/249 - Min Loss:10.37 - Loss:10.376638158547895\n",
            " Iter:7 - Alpha:0.046 - Batch 225/249 - Min Loss:10.37 - Loss:10.370642910690153\n",
            " Iter:7 - Alpha:0.046 - Batch 226/249 - Min Loss:10.36 - Loss:10.368273265433587\n",
            " Iter:7 - Alpha:0.046 - Batch 233/249 - Min Loss:10.36 - Loss:10.37270574846897\n",
            " Iter:7 - Alpha:0.046 - Batch 234/249 - Min Loss:10.36 - Loss:10.36354535916619\n",
            " Iter:7 - Alpha:0.046 - Batch 235/249 - Min Loss:10.35 - Loss:10.356103717854003\n",
            " Iter:7 - Alpha:0.046 - Batch 236/249 - Min Loss:10.35 - Loss:10.350516150005124\n",
            " Iter:8 - Alpha:0.046 - Batch 4/249 - Min Loss:10.34 - Loss:10.347544134461526\n",
            " Iter:8 - Alpha:0.046 - Batch 51/249 - Min Loss:10.32 - Loss:10.325147612812179\n",
            " Iter:8 - Alpha:0.046 - Batch 52/249 - Min Loss:10.32 - Loss:10.32380047698891\n",
            " Iter:8 - Alpha:0.046 - Batch 53/249 - Min Loss:10.30 - Loss:10.303471771315444\n",
            " Iter:8 - Alpha:0.046 - Batch 54/249 - Min Loss:10.29 - Loss:10.297303536340769\n",
            " Iter:8 - Alpha:0.046 - Batch 55/249 - Min Loss:10.28 - Loss:10.288745015178314\n",
            " Iter:8 - Alpha:0.046 - Batch 56/249 - Min Loss:10.26 - Loss:10.265505163471389\n",
            " Iter:8 - Alpha:0.046 - Batch 57/249 - Min Loss:10.25 - Loss:10.250160915438341\n",
            " Iter:8 - Alpha:0.046 - Batch 218/249 - Min Loss:10.25 - Loss:10.254641723145166\n",
            " Iter:8 - Alpha:0.046 - Batch 222/249 - Min Loss:10.24 - Loss:10.2463827490572\n",
            " Iter:8 - Alpha:0.046 - Batch 223/249 - Min Loss:10.24 - Loss:10.241216706418147\n",
            " Iter:8 - Alpha:0.046 - Batch 224/249 - Min Loss:10.23 - Loss:10.23550285074425\n",
            " Iter:8 - Alpha:0.046 - Batch 225/249 - Min Loss:10.23 - Loss:10.230025864233134\n",
            " Iter:8 - Alpha:0.046 - Batch 226/249 - Min Loss:10.22 - Loss:10.227572703077831\n",
            " Iter:8 - Alpha:0.046 - Batch 235/249 - Min Loss:10.22 - Loss:10.225017595042756\n",
            " Iter:8 - Alpha:0.046 - Batch 236/249 - Min Loss:10.21 - Loss:10.219474230425707\n",
            " Iter:9 - Alpha:0.045 - Batch 222/249 - Min Loss:10.21 - Loss:10.216623388711389\n",
            " Iter:9 - Alpha:0.045 - Batch 223/249 - Min Loss:10.21 - Loss:10.211296117199229\n",
            " Iter:9 - Alpha:0.045 - Batch 224/249 - Min Loss:10.20 - Loss:10.205694702954577\n",
            " Iter:9 - Alpha:0.045 - Batch 225/249 - Min Loss:10.20 - Loss:10.200093044743422\n",
            " Iter:9 - Alpha:0.045 - Batch 226/249 - Min Loss:10.19 - Loss:10.196741582361025\n",
            " Iter:9 - Alpha:0.045 - Batch 227/249 - Min Loss:10.19 - Loss:10.192933571655582\n",
            " Iter:9 - Alpha:0.045 - Batch 228/249 - Min Loss:10.19 - Loss:10.192528030836765\n",
            " Iter:9 - Alpha:0.045 - Batch 234/249 - Min Loss:10.19 - Loss:10.192964726592036\n",
            " Iter:9 - Alpha:0.045 - Batch 235/249 - Min Loss:10.18 - Loss:10.187445845296695\n",
            " Iter:9 - Alpha:0.045 - Batch 236/249 - Min Loss:10.18 - Loss:10.180675158123698\n",
            " Iter:9 - Alpha:0.045 - Batch 242/249 - Min Loss:10.17 - Loss:10.177516547278913\n",
            " Iter:10 - Alpha:0.045 - Batch 222/249 - Min Loss:10.17 - Loss:10.179094741453659\n",
            " Iter:10 - Alpha:0.045 - Batch 223/249 - Min Loss:10.17 - Loss:10.173215975108741\n",
            " Iter:10 - Alpha:0.045 - Batch 224/249 - Min Loss:10.16 - Loss:10.167508286346278\n",
            " Iter:10 - Alpha:0.045 - Batch 225/249 - Min Loss:10.16 - Loss:10.162385579680368\n",
            " Iter:10 - Alpha:0.045 - Batch 226/249 - Min Loss:10.15 - Loss:10.158477329327077\n",
            " Iter:10 - Alpha:0.045 - Batch 227/249 - Min Loss:10.15 - Loss:10.153018415828498\n",
            " Iter:10 - Alpha:0.045 - Batch 228/249 - Min Loss:10.15 - Loss:10.151151478978312\n",
            " Iter:10 - Alpha:0.045 - Batch 230/249 - Min Loss:10.14 - Loss:10.150457298592459\n",
            " Iter:10 - Alpha:0.045 - Batch 231/249 - Min Loss:10.14 - Loss:10.148094034580863\n",
            " Iter:10 - Alpha:0.045 - Batch 233/249 - Min Loss:10.14 - Loss:10.148934084314103\n",
            " Iter:10 - Alpha:0.045 - Batch 234/249 - Min Loss:10.14 - Loss:10.141599020614\n",
            " Iter:10 - Alpha:0.045 - Batch 235/249 - Min Loss:10.13 - Loss:10.135198660553108\n",
            " Iter:10 - Alpha:0.045 - Batch 236/249 - Min Loss:10.12 - Loss:10.12735270272132\n",
            " Iter:10 - Alpha:0.045 - Batch 237/249 - Min Loss:10.12 - Loss:10.122565487650919\n",
            " Iter:10 - Alpha:0.045 - Batch 240/249 - Min Loss:10.12 - Loss:10.12688068912441\n",
            " Iter:10 - Alpha:0.045 - Batch 241/249 - Min Loss:10.11 - Loss:10.119216913051648\n",
            " Iter:10 - Alpha:0.045 - Batch 242/249 - Min Loss:10.11 - Loss:10.115675096574979\n",
            " Iter:11 - Alpha:0.044 - Batch 2/249 - Min Loss:10.11 - Loss:10.142954264410974\n",
            " Iter:11 - Alpha:0.044 - Batch 236/249 - Min Loss:10.10 - Loss:10.11079866037127\n",
            " Iter:11 - Alpha:0.044 - Batch 237/249 - Min Loss:10.10 - Loss:10.108865376242138\n",
            " Iter:11 - Alpha:0.044 - Batch 240/249 - Min Loss:10.10 - Loss:10.114187214565092\n",
            " Iter:11 - Alpha:0.044 - Batch 241/249 - Min Loss:10.10 - Loss:10.106862696987486\n",
            " Iter:11 - Alpha:0.044 - Batch 242/249 - Min Loss:10.10 - Loss:10.105272047074063\n",
            " Iter:14 - Alpha:0.043 - Batch 21/249 - Min Loss:10.10 - Loss:10.744918723048924"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-a9e27de9283a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-3550573a855b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(iterations)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cross_entropy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_output\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"index_select\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0mones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mones\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreation_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sub\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad, grad_origin)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mc0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b5d1e5ac4017>\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m                           \u001b[0mcreators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                           creation_op=\"mm\")\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Uv10Pds-SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9bb8ca89-fb20-4a03-9a16-9b2007f5aaf5"
      },
      "source": [
        "print(generate_sample(n=500, init_char='\\n'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I avo to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkYHsezN2NjT",
        "colab_type": "text"
      },
      "source": [
        "## Задание 3\n",
        "\n",
        "Предложите свои варианты решения проблемы исчезающего градиента в RNN\n",
        "\n",
        "Deep Residual Learning (ResNets)\n",
        "\n",
        "Когда более глубокая сеть начинает сворачиваться, возникает проблема: с увеличением глубины сети точность сначала увеличивается, а затем быстро ухудшается. Снижение точности обучения показывает, что не все сети легко оптимизировать.\n",
        "\n",
        "\n",
        "Соединения быстрого доступа (shortcut connections) пропускают один или несколько слоев и выполняют сопоставление идентификаторов. Их выходы добавляются к выходам stacked layers. Используя ResNet, можно решить множество проблем, таких как:\n",
        "\n",
        "1. ResNet относительно легко оптимизировать: «простые» сети (которые просто складывают слои) показывают большую ошибку обучения, когда глубина увеличивается.\n",
        "2. ResNet позволяет относительно легко увеличить точность благодаря увеличению глубины, чего с другими сетями добиться сложнее.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8oWTAwN7Nub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}